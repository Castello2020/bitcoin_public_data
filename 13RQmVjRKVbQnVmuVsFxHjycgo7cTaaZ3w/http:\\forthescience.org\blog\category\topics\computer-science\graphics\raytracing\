http://forthescience.org/blog/category/topics/computer-science/graphics/raytracing/
HTTP/1.1 200 OK
Date: Tue, 22 Jul 2014 11:20:03 GMT
Server: Apache
X-Pingback: http://forthescience.org/blog/xmlrpc.php
Vary: Accept-Encoding
Content-Encoding: gzip
Content-Length: 19189
Connection: close
Content-Type: text/html; charset=UTF-8

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
<title> &raquo; Raytracing ForTheScience.org</title>
<link rel="pingback" href="http://forthescience.org/blog/xmlrpc.php" />
<link rel="shortcut icon" href="http://forthescience.org/favicon.png">

<style type="text/css" media="screen">

@import "http://forthescience.org/blog/wp-content/themes/garland-revisited/style.php";
</style>
<link type="text/css" rel="stylesheet" href="http://forthescience.org/blog/wp-content/plugins/easy-table-creator/css/easy_table_creator.css" />
<link type="text/css" rel="stylesheet" href="http://forthescience.org/blog/wp-content/plugins/easy-table-creator/css/tablesorter/style.css" />

<style type="text/css" media="screen">@import url(http://forthescience.org/blog/?garland_css=1);
</style><link rel="alternate" type="application/rss+xml" title="ForTheScience.org &raquo; Feed" href="http://forthescience.org/blog/feed/" />
<link rel="alternate" type="application/rss+xml" title="ForTheScience.org &raquo; Comments Feed" href="http://forthescience.org/blog/comments/feed/" />
<link rel="alternate" type="application/rss+xml" title="ForTheScience.org &raquo; Raytracing Category Feed" href="http://forthescience.org/blog/category/topics/computer-science/graphics/raytracing/feed/" />
<link rel='stylesheet' id='openid-css'  href='http://forthescience.org/blog/wp-content/plugins/openid/f/openid.css?ver=519' type='text/css' media='all' />
<script type='text/javascript' src='http://forthescience.org/blog/wp-includes/js/jquery/jquery.js?ver=1.10.2'></script>
<script type='text/javascript' src='http://forthescience.org/blog/wp-includes/js/jquery/jquery-migrate.min.js?ver=1.2.1'></script>
<script type='text/javascript' src='http://forthescience.org/blog/wp-content/plugins/easy-table-creator/js/easy_table_creator.js?ver=0.1'></script>
<script type='text/javascript' src='http://forthescience.org/blog/wp-content/plugins/easy-table-creator/js/jquery.tablesorter.min.js?ver=0.1'></script>
<script type='text/javascript' src='http://forthescience.org/blog/wp-content/plugins/google-analyticator/external-tracking.min.js?ver=6.4.5'></script>
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://forthescience.org/blog/xmlrpc.php?rsd" />
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://forthescience.org/blog/wp-includes/wlwmanifest.xml" /> 
<meta name="generator" content="WordPress 3.8.3" />
<style type="text/css" id="syntaxhighlighteranchor"></style>
<!-- Google Analytics Tracking by Google Analyticator 6.4.5: http://www.videousermanuals.com/google-analyticator/ -->
<script type="text/javascript">
	var analyticsFileTypes = [''];
	var analyticsEventTracking = 'enabled';
</script>
<script type="text/javascript">
	var _gaq = _gaq || [];
	_gaq.push(['_setAccount', 'UA-13239309-1']);
        _gaq.push(['_addDevId', 'i9k95']); // Google Analyticator App ID with Google 
        
	_gaq.push(['_trackPageview']);

	(function() {
		var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
		                ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
		                var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
	})();
</script>
</head>
<body class="archive category category-raytracing category-187 sidebars">
<div id="navigation"></div>
<div id="wrapper">
<div id="container" class="clear-block">
<div id="header">
<div id="logo-floater">
<h1>
<a href="http://forthescience.org/blog/">ForTheScience.org</a> <span style="font-style: italic; font-size: 60%;">A blog about science and programming</span>
</h1>
</div>
<!-- <ul class="links primary-links"> -->
<ul id="dropmenu">
<li class="page_item page-item-2291"><a href="http://gaia.forthescience.org/blog">Gaia blog</a></li>
<li class="page_item page-item-2"><a href="http://forthescience.org/blog/about/">About me</a></li>
<li class="page_item page-item-106"><a href="http://forthescience.org/blog/software/">Software</a></li>
<li class="page_item page-item-34"><a href="http://forthescience.org/blog/my-articles/">My Articles</a></li>
<li class="page_item page-item-5"><a href="http://forthescience.org/blog/bookshelf/">My Bookshelf</a></li>
</ul>                
</div> <!-- /header -->
<div id="sidebar-left" class="sidebar">
</div>
<div id="center"><div id="squeeze"><div class="right-corner"><div class="left-corner">
<!-- begin content -->
<div class="node">
<div class="post-1809 post type-post status-publish format-standard hentry category-python category-raytracing">
<h2><a href="http://forthescience.org/blog/2013/09/05/a-raytracer-in-python-%e2%80%93-part-6-cameras/" rel="bookmark">A raytracer in python â€“ part 6: cameras</a></h2>
<span class="submitted">September 5, 2013 &#8212; Stefano Borini </span>
<div class="content">
<p>In the <a href="https://github.com/stefanoborini/python-raytrace/commit/c5fc3a3c97c46d75d378f6e8c0fa4b53a0efa7b2">latest commit for the raytracer, I added cameras</a>. The design changed so that now the responsible for rendering is the camera object. Actual cameras are specializations of an abstract class BaseCamera, which holds common information about positioning. The BaseCamera is then specialized into two concrete classes:</p>
<ol>
<li>PinholeCamera is a camera where rays are shot as diverging from a single point, called the eye_point. This allows perspective, which was not present previously as the rays were emerging from the ViewPlane pixels.</li>
<li>LensCamera is a camera that simulates depth of field, that is, focus/out of focus. Contrary to the PinholeCamera, where everything is in focus, LensCamera allows different focusing. Objects that happen to be on the &#8220;focal plane&#8221; are in focus, while objects that are outside (either closer or farther from the camera) present less defined details proper of an out-of-focus object. To perform this effect, we need the random sampling on a disk implemented in the previous post.</li>
</ol>
<p>The following picture shows how LensCamera performs. A set of hemispheres are deployed along a line. The camera is above them, slightly angled and with a roll angle appreciable from the horizon. In all three cases, the orange central sphere is focused, as the focus plane has been set to fall on the sphere&#8217;s position. Note how other objects are in focus for a Pinhole camera (left picture) which has no depth of field by construction, and become more out of focus as the lens size increases (1.0 in the center picture, 5.0 in the right one)</p>
<div id="attachment_1811" style="width: 610px" class="wp-caption aligncenter"><a href="http://forthescience.org/blog/wp-content/uploads/2011/08/lens.png"><img class="size-full wp-image-1811" title="lens" src="http://forthescience.org/blog/wp-content/uploads/2011/08/lens.png" alt="Focusing" width="600" height="200" /></a><p class="wp-caption-text">From left to right, PinholeCamera, LensCamera with lens size 1.0, LensCamera with lens size 5.0</p></div>
<p>Other cameras may technically be possible: the book goes further in deploying fisheye and stereoscopic cameras, but I am not interested in them. I think the pinhole and lens camera are flexible enough for quality renderings and my desire to learn.</p>
<p>One important feature of the Camera system is that it requires the definition of local coordinates on the camera itself. The three vectors defining this set of coordinates, called u, v, w in the book, are obtained by building an orthonormal basis using the cross product between the observation vector (the vector between the &#8220;eye&#8221; of the camera and the look_at point) and an &#8220;up&#8221; vector, our default being in the same direction as the y axis. Doing the cross product of these two vectors (observation and up) produces the third remaining vector of the orthogonal basis centered on the camera. However, if the camera looks straight up, or straight down, the cross product is zero and we obtain a singularity, losing one degree of freedom (a condition also known as <a href="http://en.wikipedia.org/wiki/Gimbal_lock">gimbal lock</a>). The book proposes to detect this condition and treat it accordingly, by either overriding the specification and setting the vectors to an arbitrary, well defined alternative, or by &#8220;juggling&#8221; the up vector out of alignment so that the third vector is still defined. I decided for the third option, ignore the problem, as I am not going to use gimbal locked configurations for now, but it&#8217;s definitely a problem to add to the todo list.</p>
<p>With this post, I take a temporary break from the raytracing business. I may add optical effects such as reflections, refractions, materials, lights, but the point is that the amount of rays that must be propagated for these effects to show tends to be very high. I want to venture into CUDA, and therefore I will switch my attention to CUDA programming from now on, integrate it with the raytracing later on, then go back to light effects at a later stage. I will implement light effects first in python, then use CUDA to achieve the same results. My aim is to have fun, test CUDA/C/Python integration, compare performances, and provide a fully python raytracer with optional C/CUDA high-performance code to achieve the same task. For CUDA tinkering, I will switch back to my old friend, the mandelbrot set.</p>
</div>
<div class="meta">
Posted in <a href="http://forthescience.org/blog/category/topics/computer-science/languages/python/" title="View all posts in Python" rel="category tag">Python</a>, <a href="http://forthescience.org/blog/category/topics/computer-science/graphics/raytracing/" title="View all posts in Raytracing" rel="category tag">Raytracing</a>.  <span>Comments Off</span> &#187;
</div>
</div>
<div class="post-1771 post type-post status-publish format-standard hentry category-python category-raytracing">
<h2><a href="http://forthescience.org/blog/2013/01/05/a-raytracer-in-python-%e2%80%93-part-5-non-planar-samplers/" rel="bookmark">A raytracer in python â€“ part 5: non-planar samplers</a></h2>
<span class="submitted">January 5, 2013 &#8212; Stefano Borini </span>
<div class="content">
<p>In this post we are going to describe and implement non-planar samplers. In the previous <a href="http://forthescience.org/blog/2011/11/30/a-raytracer-in-python-%e2%80%93-part-3-samplers/">post about samplers</a>, we implemented and characterized different planar samplers to make antialiasing possible. The characteristic of these samplers was to produce regular or random points on a plane with x,y between zero and one. To implement effects such as simulation of lenses behavior, reflections and so on, we also need to be able to shoot rays according toÂ  geometrical patterns other than the plane. More specifically, we need to be able to map points on a disk (to simulate lenses) or on a hemisphere (to simulate other optical effects such as reflections), while at the same time preserving the good characteristics of the random distributions outlined in the planar case.</p>
<p>To achieve this, the samplers now implement two new methods, BaseSampler.map_samples_to_disk() and BaseSampler.map_sampler_to_hemisphere(). They are in charge of remapping the planar distribution to a disk or to a hemisphere, but with a couple of twists: in the disk remap, the points in the range [0:1] must be remapped to a full circle from [-1:1] in both axes, so to cover the circle completely while preserving the distribution. This is done through a formulation called Shirley&#8217;s concentric maps.</p>
<p><a href="http://forthescience.org/blog/wp-content/uploads/2011/07/disk.png"><img class="aligncenter size-full wp-image-1772" title="disk" src="http://forthescience.org/blog/wp-content/uploads/2011/07/disk.png" alt="" width="500" height="300" /></a>In the hemisphere remapping, we also want to introduce a variation in the density so that it changes with the cosine of the polar angle from the top of the hemisphere. In other words, we want an adjustable parameter <em>e</em> to focus the point density closer to the top of the hemisphere.</p>
<p><a href="http://forthescience.org/blog/wp-content/uploads/2011/07/hemisphere.png"><img class="aligncenter size-full wp-image-1773" title="hemisphere" src="http://forthescience.org/blog/wp-content/uploads/2011/07/hemisphere.png" alt="" width="450" height="280" /></a>We will need the characteristics of this distributions later on, when we will have to implement reflections and other optical effects. As you can see from the above plot, higher values of the parameter <em>e</em> produce a higher concentration of the points close to the top of the hemisphere. On the other hand, a low <em>e</em> parameter tend to produce a more uniform distribution over the full hemisphere.</p>
<p>To obtain the points, the sampler object has now three methods to request an iterator. We are no longer iterating on the object itself, because we need to provide three different iteration strategies. Methods BaseSampler.diskiter(), BaseSampler.hemisphereiter() and BaseSampler.squareiter(), each returning a generator over the proper set of points. Note that the hemisphere point generator returns 3D points, differently from the other two returning 2D points.</p>
<p><a href="https://github.com/stefanoborini/python-raytrace/commit/363cdc7d59f7a132efcadab617e3c9a9373ed5dc">You can find the code for this post at github</a>.</p>
</div>
<div class="meta">
Posted in <a href="http://forthescience.org/blog/category/topics/computer-science/languages/python/" title="View all posts in Python" rel="category tag">Python</a>, <a href="http://forthescience.org/blog/category/topics/computer-science/graphics/raytracing/" title="View all posts in Raytracing" rel="category tag">Raytracing</a>.  <span>Comments Off</span> &#187;
</div>
</div>
<div class="post-1756 post type-post status-publish format-standard hentry category-python category-raytracing">
<h2><a href="http://forthescience.org/blog/2012/05/04/a-raytracer-in-python-%e2%80%93-part-4-profiling/" rel="bookmark">A raytracer in python â€“ part 4: profiling</a></h2>
<span class="submitted">May 4, 2012 &#8212; Stefano Borini </span>
<div class="content">
<p>After having finally obtained a raytracer which produces antialiasing, it is now time to take a look at performance. We already saw some numbers in the last post. Rendering a 200&#215;200 image with 16 samples per pixels (a grand total of 640.000 rays) takes definitely too much. I want to perform some profiling with python, find the hotspots in the code, and eventually devise a strategy to optimize them.</p>
<h2>General profiling with cProfile</h2>
<p>To perform basic profiling, I used the cProfile program provided in the standard library. It appears that the longest processing time is in the hit() function</p>
<pre>$ python -m cProfile -s time test3.py
 Ordered by: internal time

 ncallsÂ  tottimeÂ  percallÂ  cumtimeÂ  percall filename:lineno(function)
 2560000Â  85.329Â Â Â  0.000Â  108.486Â Â Â  0.000 Sphere.py:12(hit)
 1960020Â  30.157Â Â Â  0.000Â Â  30.157Â Â Â  0.000 {numpy.core.multiarray.array}
 7680000Â  23.115Â Â Â  0.000Â Â  23.115Â Â Â  0.000 {numpy.core._dotblas.dot}
 1Â Â       19.589Â Â  19.589Â  195.476Â  195.476 World.py:25(render)
 2560000Â Â  7.968Â Â Â  0.000Â  116.454Â Â Â  0.000 World.py:62(f)
 640000Â Â Â  6.710Â Â Â  0.000Â  133.563Â Â Â  0.000 World.py:61(hit_bare_bones_object)
 640025Â Â Â  4.438Â Â Â  0.000Â  120.902Â Â Â  0.000 {map}
 640000Â Â Â  3.347Â Â Â  0.000Â  136.910Â Â Â  0.000 Tracer.py:5(trace_ray)
 640000Â Â Â  3.009Â Â Â  0.000Â Â Â  3.009Â Â Â  0.000 {numpy.core.multiarray.zeros}
 640000Â Â Â  2.596Â Â Â  0.000Â Â Â  2.613Â Â Â  0.000 {sorted}
 640000Â Â Â  2.502Â Â Â  0.000Â Â Â  3.347Â Â Â  0.000 {filter}
 640000Â Â Â  1.835Â Â Â  0.000Â Â  16.784Â Â Â  0.000 Ray.py:4(__init__)</pre>
<p>This does not surprise me, as the main computation a raytracer performs is to test each ray for intersection on the objects in the scene, in this case multiple Sphere objects.</p>
<h2>Profiling line by line for hot spots</h2>
<p>Understood that most of the time is spent into hit(), I wanted to perform line-by-line profiling. This is not possible with the standard python cProfile module, therefore I searched and found an alternative, <a href="http://packages.python.org/line_profiler/">line_profiler</a>:</p>
<pre>$ easy_install-2.7 --prefix=$HOME line_profiler
$ kernprof.py -l test3.py
Wrote profile results to test3.py.lprof
$ python -m line_profiler test3.py.lprof</pre>
<p>Before running the commands above, I added the @profile decorator to the method I am interested in. This decorator is added by line_profiler to the __builtin__ module, so no explicit import statement is needed.</p>
<pre>class Sphere(object):
    &lt;...&gt;
   Â @profile
    def hit(self, ray):
         &lt;...&gt;</pre>
<p>The results of this profiling are</p>
<pre>Line # HitsÂ  TimeÂ    Per Hit  % Time Line Contents
==============================================================
12Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  @profile
13Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  def hit(self, ray):
14 2560000Â  27956358Â  10.9Â Â Â Â  19.2Â Â Â Â  temp = ray.origin - self.center
15 2560000Â  17944912Â Â  7.0Â Â Â Â  12.3Â Â Â Â  a = numpy.dot(ray.direction, ray.direction)
16 2560000Â  24132737Â Â  9.4Â Â Â Â  16.5Â Â Â Â  b = 2.0 * numpy.dot(temp, ray.direction)
17 2560000Â  37113811Â  14.5Â Â Â Â  25.4Â Â Â Â  c = numpy.dot(temp, temp) \
                                              - self.radius * self.radius
18 2560000Â  20808930Â Â  8.1Â Â Â Â  14.3Â Â Â Â  disc = b * b - 4.0 * a * c
19
20 2560000  10963318Â Â  4.3Â Â Â Â Â  7.5Â Â Â Â  if (disc &lt; 0.0):
21 2539908Â Â  5403624Â Â  2.1Â Â Â Â Â  3.7Â Â Â Â Â Â Â Â  return None
22Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  else:
23Â Â  20092Â Â Â Â  75076Â Â  3.7Â Â Â Â Â  0.1Â Â Â Â Â Â Â Â  e = math.sqrt(disc)
24Â Â  20092Â Â Â  104950Â Â  5.2Â Â Â Â Â  0.1Â Â Â Â Â Â Â Â  denom = 2.0 * a
25Â Â  20092Â Â Â  115956Â Â  5.8Â Â Â Â Â  0.1Â Â Â Â Â Â Â Â  t = (-b - e) / denom
26Â Â  20092Â Â Â Â  83382Â Â  4.2Â Â Â Â Â  0.1Â Â Â Â Â Â Â Â  if (t &gt; 1.0e-7):
27Â Â  20092Â Â Â  525272Â  26.1Â Â Â Â Â  0.4Â Â Â Â Â Â Â Â Â Â Â  normal = (temp + t * ray.direction)\
                                                          Â / self.radius
28Â Â  20092Â Â Â  333879Â  16.6Â Â Â Â Â  0.2Â Â Â Â Â Â Â Â Â Â Â  hit_point = ray.origin + t * \
                                                              ray.direction
29Â Â  20092    299494Â  14.9Â Â Â Â Â  0.2Â Â Â Â Â Â Â Â Â Â Â  return ShadeRecord.ShadeRecord(
                                                         normal=normal,
                                                         hit_point=hit_point,
                                                         parameter=t,
                                                         color=self.color)</pre>
<p>Therefore, it appears that most of the time is spent in this chunk of code:</p>
<pre>temp = ray.origin - self.center
a = numpy.dot(ray.direction, ray.direction)
b = 2.0 * numpy.dot(temp, ray.direction)
c = numpy.dot(temp, temp) - self.radius * self.radius
disc = b * b - 4.0 * a * c</pre>
<p>We cannot really optimize much. We could precompute self.radius * self.radius, but it does not really have an impact. Something we can observe is the huge amount of routine calls. Is the routine call overhead relevant ? Maybe: <a href="http://wiki.python.org/moin/PythonSpeed/PerformanceTips#Data_Aggregation">Python has a relevant call overhead</a>, but a very simple program like this</p>
<pre>def main():
    def f():
        return 0
    a=0
    for i in xrange(2560000):
        if f():
            a = a+1

    print a

main()</pre>
<p>is going to take 0.6 seconds, not small, but definitely not as huge as the numbers we see. Why is that ? And why is the raytracer so slow for the same task ? I think the bottleneck is somewhere else.</p>
<h2>Finding the problem</h2>
<p>I decided to profile World.render() to understand what&#8217;s going on: this is the routine in charge of going through the pixels, shooting the rays, then delegating the task of finding intersections to Tracer.trace_ray, which in turns re-delegates the task to World.hit_bare_bone_object. I don&#8217;t really like this design, but I stick to the book as much as possible, mostly because I don&#8217;t know how things will become later on.</p>
<p>The profiling showed two hot spots in World.render(), in the inner loop:</p>
<pre>Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================

    41    640000     18786192     29.4     29.2  ray = Ray.Ray(origin = origin,
                                                              Â direction = (0.0,0.0,-1.0))
    42
    43    640000     22414265     35.0     34.9  color += numpy.array(tracer.trace_ray(ray))</pre>
<p>Why is it so slow to perform these two operations? It turns out that <a href="http://stackoverflow.com/questions/6559463/why-is-numpy-array-so-slow">numpy is incredibly slow at creating arrays</a>. This may indeed be the reason why it&#8217;s so slow to instantiate a Ray object (two numpy.arrays), to add the color (another instantiation) and to perform operations in the Sphere.hit slow lines. At this point I&#8217;m not sure I can trust numpy.array, and I decide to remove it completely replacing arrays with tuples. The result is pleasing</p>
<pre>$ time python test3.py
real	0m31.215s
user	0m29.923s
sys	0m2.355s</pre>
<p>This is an important point: tuples are much faster than small arrays. numpy seems to be optimized for large datasets and performs poorly when handling small ones. This includes not only the creation of the arrays, but also any operation in numpy that may create numpy arrays as a consequence, such as calling numpy.dot on two tuples instead of a trivial implementation such as</p>
<pre>def dot(a,b):
    return a[0]*b[0]+a[1]*b[1]+a[2]*b[2]</pre>
<p>in fact, if I use numpy.dot on tuples in Sphere.hit():</p>
<pre> a = numpy.dot(ray.direction, ray.direction)
 b = 2.0 * numpy.dot(temp, ray.direction)
 c = numpy.dot(temp, temp) - self.radius * self.radius</pre>
<p>the total running time goes from 31 seconds to a staggering 316 seconds (5 minutes). My guess is that they are converted to numpy.arrays internally, followed by the actual vector-vector operation.</p>
<p>I call myself happy with a runtime of 30 seconds for now, and plan to optimize further when more complex operations are performed. You can find the <a href="https://github.com/stefanoborini/python-raytrace/commit/2cb6e2b31bc75a21a121ed9c7a46b1a3113fcab0">version for this post at github</a>.</p>
</div>
<div class="meta">
Posted in <a href="http://forthescience.org/blog/category/topics/computer-science/languages/python/" title="View all posts in Python" rel="category tag">Python</a>, <a href="http://forthescience.org/blog/category/topics/computer-science/graphics/raytracing/" title="View all posts in Raytracing" rel="category tag">Raytracing</a>.  <span>Comments Off</span> &#187;
</div>
</div>
<div class="post-1738 post type-post status-publish format-standard hentry category-python category-raytracing">
<h2><a href="http://forthescience.org/blog/2012/02/05/a-raytracer-in-python-%e2%80%93-part-3-samplers/" rel="bookmark">A raytracer in python â€“ part 3: samplers</a></h2>
<span class="submitted">February 5, 2012 &#8212; Stefano Borini </span>
<div class="content">
<p>In the previous post, we explored a very basic way of plotting images: shooting a ray from the center of every pixel, and plot the color of the object we hit. The result is a rather flat, very jagged image</p>
<p><a href="http://forthescience.org/blog/wp-content/uploads/2011/06/no-antialiasing.png"><img class="aligncenter size-full wp-image-1739" title="no antialiasing" src="http://forthescience.org/blog/wp-content/uploads/2011/06/no-antialiasing.png" alt="" width="365" height="205" /></a></p>
<p>Border jagging arises from the fact that we are sampling with a discrete grid (our ViewPlane) an object that is smooth due to its functional expression (our spheres). The sampling is by nature approximated, and when mapped to pixels it produces either the color of the object, or the color of the background.</p>
<p>We would like to remove the jagged behavior. To achieve this, we could increase the resolution. That would make the jaggies less appreciable thanks to a higher number of pixels. An alternative is to keep the lower resolution, and shoot many rays per each pixels instead of only one (a technique called <a href="http://en.wikipedia.org/wiki/Supersampling">supersampling</a>). To compute the final color, weighting is performed according to the number of rays that impact vs. the total number of rays sent from that pixel. This technique is known as <a href="http://en.wikipedia.org/wiki/Anti-aliasing">anti-aliasing</a>.</p>
<p><a href="http://forthescience.org/blog/wp-content/uploads/2011/06/antialiasing_explained.png"><img class="aligncenter size-full wp-image-1740" title="antialiasing explained" src="http://forthescience.org/blog/wp-content/uploads/2011/06/antialiasing_explained.png" alt="" width="477" height="227" /></a>The figure details visually what said above: the real description of  the sphere is smooth (left hand figure). Shooting one ray per pixel, and  coloring the pixel according to hit/no-hit, produces either a fully  colored pixel, or a fully background pixel (center). With supersampling,  we shoot a higher number of rays, and per each pixel we perform  weighting of the color.</p>
<p>As a result, the jaggies in the sphere are replaced with a smoother, more pleasant transition</p>
<p><a href="http://forthescience.org/blog/wp-content/uploads/2011/06/antialiasing.png"><img class="aligncenter size-full wp-image-1741" title="antialiasing" src="http://forthescience.org/blog/wp-content/uploads/2011/06/antialiasing.png" alt="" width="407" height="204" /></a></p>
<h2>Choice of Samplers</h2>
<p>The pattern used for the supersampled rays is important. The module samplers in the python raytracer implements three common strategies.</p>
<h3>Regular sampling</h3>
<p>It is the most intuitive and easy, and the one used above: a regular grid of rays. It is easy to implement but it can introduce unpleasant artifacts for more complex situations. Plotting the position of the rays in the pixel will produce the following layout (for a 8&#215;8 supersample grid)</p>
<p><a href="http://forthescience.org/blog/wp-content/uploads/2011/06/regular.png"><img class="aligncenter size-full wp-image-1742" title="regular" src="http://forthescience.org/blog/wp-content/uploads/2011/06/regular.png" alt="" width="510" height="532" /></a></p>
<p>As we see, the layout is regular on the grid of subcells (painted yellow and white for better visualization) that define the pixel. On the vertical and horizontal distributions (plotted on the grey bars above and on the left) we also see a regular distribution, but its regularity may introduce artifacts in some cases.</p>
<h3>Random sampling</h3>
<p>Random sampling positions the rays at random within the pixel. This may sound appealing, but it may instead end up as suboptimal: it can produce random clumping, in particular for a small number of samples. This will unbalance the weighting leading to an incorrect evaluation. Plotting one distribution one may obtain</p>
<p><a href="http://forthescience.org/blog/wp-content/uploads/2011/06/random-sampling.png"><img class="aligncenter size-full wp-image-1743" title="random-sampling" src="http://forthescience.org/blog/wp-content/uploads/2011/06/random-sampling.png" alt="" width="510" height="532" /></a></p>
<p>Note the uneven distribution of the points, leaving large parts not sampled and other parts oversampled. In addition, the vertical and horizontal distribution tend to be uneven.</p>
<h3>Jittered sampling</h3>
<p>Jittered sampling takes the best of both worlds: the regularity of the Regular sampler with a degree of randomness from the Random sampler. The idea is to select the center of each subcell and apply randomization, so that each subcell produces only one ray, but without the artifact inducing regularity proper of the Regular sampler.</p>
<p><a href="http://forthescience.org/blog/wp-content/uploads/2011/06/jittered.png"><img class="aligncenter size-full wp-image-1744" title="jittered" src="http://forthescience.org/blog/wp-content/uploads/2011/06/jittered.png" alt="" width="510" height="532" /></a></p>
<h2>Computational cost impact</h2>
<p>Unfortunately, performing supersampling makes the creation of the image considerably slower. In the following table you can see the timings (in seconds) for Jittered and Regular sampling, compared against the case with no sampling. The size of the image rendered is 200&#215;200 pixels.</p>
<table border="0" width="458" height="137" align="center">
<tbody>
<tr>
<td style="text-align: center;">Sample size/sets</td>
<td style="text-align: center;">No sampling</td>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">Jittered</td>
</tr>
<tr>
<td style="text-align: center;">1/1</td>
<td style="text-align: center;">23</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td style="text-align: center;">4/1</td>
<td></td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">160</td>
</tr>
<tr>
<td style="text-align: center;">9/1</td>
<td></td>
<td style="text-align: center;">158</td>
<td style="text-align: center;">152</td>
<td style="text-align: center;">271</td>
</tr>
<tr>
<td style="text-align: center;">16/1</td>
<td></td>
<td style="text-align: center;">229</td>
<td style="text-align: center;">235</td>
<td style="text-align: center;">276</td>
</tr>
<tr>
<td style="text-align: center;">16/2</td>
<td></td>
<td style="text-align: center;">223</td>
<td style="text-align: center;">247</td>
<td style="text-align: center;">267</td>
</tr>
<tr>
<td style="text-align: center;">16/4</td>
<td></td>
<td style="text-align: center;">240</td>
<td style="text-align: center;">277</td>
<td style="text-align: center;">260</td>
</tr>
</tbody>
</table>
<p>As we can see, supersampling introduces a considerably higher computational effort. We also see that having multiple sets (a requirement to prevent the same set of subsamples to be reused for adjacent pixels, something that again would introduce artifacts) does not really change the timings. According to the current implementation, I expect this to be verified. On the other hand, I don&#8217;t expect timings for Regular and Jittered to be so different, since the creation of the values is performed once and for all at startup. This is worth investigating while looking for performance improvement. In the next post I will perform profiling of the python code and check possible strategies to reduce the timings, eventually revising the current design.</p>
<h2>Current implementation</h2>
<p>The current implementation of python-raytrace <a href="https://github.com/stefanoborini/python-raytrace/commit/00de858590b76929d216bfe0d53605ddcbde8548">can be found at github</a>. In this release, I added the samplers. Samplers are derived classes of the BaseSampler class, and are hosted in the samplers module. Derived Samplers must reimplement _generate_samples. Points are stored because we want to be able to select sets at random as well as replay the same points. Samplers also reimplements the __iter__() method as a generator of (x,y) tuples, with x and y being in the interval [0.0, 1.0). Once initialized, the Sampler can therefore be iterated over with a simple</p>
<pre>for subpixel in sampler:
    # use subpixel</pre>
<p>The World class can now be configured with different Samplers for the antialiasing. The default Sampler is a Regular 1 subpixel Sampler, which is the original one-ray-per-pixel sampling.</p>
</div>
<div class="meta">
Posted in <a href="http://forthescience.org/blog/category/topics/computer-science/languages/python/" title="View all posts in Python" rel="category tag">Python</a>, <a href="http://forthescience.org/blog/category/topics/computer-science/graphics/raytracing/" title="View all posts in Raytracing" rel="category tag">Raytracing</a>.  <span>Comments Off</span> &#187;
</div>
</div>
<div class="post-1692 post type-post status-publish format-standard hentry category-python category-raytracing">
<h2><a href="http://forthescience.org/blog/2011/11/05/a-raytracer-in-python-part-2-rendering-multiple-objects/" rel="bookmark">A raytracer in python &#8211; part 2: rendering multiple objects</a></h2>
<span class="submitted">November 5, 2011 &#8212; Stefano Borini </span>
<div class="content">
<p>A quick addition needed to the raytracer is providing freedom to add more objects to the rendering scene. In Part 1, the design was such that only one object, a sphere, could be drawn. The new code allows much more flexibility. I added a Plane object, introduced assignment of colors to the objects, divided the source into multiple files, and fixed a bug relative to rendering direction. Let&#8217;s see more specifically.</p>
<p>The class World is the main interface to user programming of the raytracer. This small program creates and renders a scene containing four spheres, a white one in the origin, the others along the axes, each with different colors.</p>
<pre>import raytrace
from raytrace import objects

w=raytrace.World()
w.add_object(objects.Sphere(center=(0.0,0.0,0.0),
                            radius=10.0,
                            color=(1.0,1.0,1.0)
                           )
            )
w.add_object(objects.Sphere(center=(50.0,0.0,0.0),
                            radius=10.0,
                            color=(1.0,0.0,0.0)
                           )
            )
w.add_object(objects.Sphere(center=(0.0,50.0,0.0),
                            radius=10.0,
                            color=(0.0,1.0,0.0)
                           )
            )
w.add_object(objects.Sphere(center=(0.0,0.0,50.0),
                            radius=10.0,
                            color=(0.0,0.0,1.0)
                            )
            )
w.render()</pre>
<p>The resulting image is the following</p>
<p><a href="http://forthescience.org/blog/wp-content/uploads/2011/05/render1.png"><img class="aligncenter size-full wp-image-1694" title="render" src="http://forthescience.org/blog/wp-content/uploads/2011/05/render1.png" alt="" width="500" height="200" /></a></p>
<p>Clearly, the white sphere is not visible, as it&#8217;s hidden by the blue sphere. This test allowed me to discover a problem with orientation: the green sphere was on the wrong side. I therefore had to analyze a bit the orientation and the different coordinate systems into play here</p>
<ol>
<li>The measure of the screen is given in pixels. We are used to this when it comes to screen size, for example. An image which is 320&#215;200 means that it&#8217;s 320 pixels wide (horizontal resolution) and 200 pixel high (vertical resolution). Don&#8217;t fall into &#8220;thinking matrix&#8221;, where NxM means N rows x M columns. It is the exact opposite.</li>
<li>The geometry uses the cartesian system, which has the origin in the center of the picture. The x axis is oriented towards the right, the y axis towards the top, and the z axis towards the observer. This is not different from a traditional cartesian layout: for the z=0 plane, the top left pixel correspond to a (-,+) coordinate, the bottom right to a (+,-) coordinate. Points closer to the observer have positive z, honoring the <a href="http://en.wikipedia.org/wiki/Right-hand_rule">right hand system</a>. The camera in the above picture is at z=+100.0</li>
<li>The raytracer uses pixel coordinates for the viewplane. Pixel 0,0 is at the bottom left. Changing the first index moves horizontally (along the row) from left to right. Changing the second index moves vertically (along the column) from bottom to top. As a consequence, the point at the bottom right is (hres-1,0), and at the top left is (o, vres-1). Note that this is equivalent to a cartesian (x,y) system with origin on the bottom left corner. Not a surprise, since there is a direct mapping between the pixels and the rays&#8217; origins.</li>
<li>Finally, the pixel image indexing of pygame and PIL. For them, pixel 0,0 is top left. Like the case above, incrementing the first index also moves along the row from left to right. However, incrementing the second index moves vertically from top to bottom, which is the opposite of the raytracing index. Bottom left is (0, vres-1) and the bottom right is (hres-1, vres-1).</li>
</ol>
<p>A remapping is therefore needed from the pixel coordinate of the  rendering and the pixel coordinate of the display (e.g. pygame). The  transformation is trivial, of course, but it must be kept into account, otherwise pics will be flipped horizontally.</p>
<p>Another interesting fact is that, according to &#8220;Ray Tracing from the ground up&#8221; it&#8217;s useful to perform the raytracing operation starting from the bottom and working our way up, rendering pixels from left to right. According to them this is for coding symmetry and future convenience, so we stick to it.</p>
<h2>Finding the foremost object</h2>
<p>In the book, finding the foremost object is made more complex by the language used, C++. In python, you can use functional programming style to obtain the same in a very concise statement. The idea is to cast a ray, then go through all objects in the world to find the intersection point (if any) between the ray and the object. If more than one object is hit, the one with the intersection point closer to the observer commands the pixel color, since it&#8217;s in front.</p>
<p>I achieve this with the following code</p>
<pre>def hit_bare_bones_object(self,ray):
   def f(o):
     shadeRec = o.hit(ray)
     if shadeRec:
       return (shadeRec.parameter, o)
     else:
       return None

   try:
     foremost=sorted( \
                filter(lambda x: x is not None, \
                  map(f, self.objects)
                ), key=lambda x: x[0]
              )[0][1]
   except IndexError:
     return None

   return foremost</pre>
<p>What does this code do ? I defined a simple internal function f which accepts an object, performs the hit and returns a tuple containing the hit point position (as a parameter of the ray, so it&#8217;s a single number, not a xyz coordinate) and the object.</p>
<p>Now, I use this function to map all the objects defined in the world. I will obtain a list with one entry per each object, either a None (not hit) or a 2-tuple containing the parameter and the hit object. I filter out the None entries, leaving only the 2-tuples and then sort according to their first element. The 2-tuple with the lowest parameter is now at index 0, and the [1] element of this tuple is the foremost object. At any time, the list may be empty (such as if you don&#8217;t have any object,Â  or no object is hit. In that case, a IndexError will be raised and that will indicate that the ray hit nothing. I may rework on this function later on, but for this second round, it suits my needs.</p>
<p>It&#8217;s now time to move on to samplers. Given that the code is growing in size, <a href="https://github.com/stefanoborini/python-raytrace">I created a git repository you can clone from</a>. The <a href="https://github.com/stefanoborini/python-raytrace/tree/74521b39d6ebba01b7446b7353c9a7868407513b">release of this post is available here</a>. The code is under BSD license.</p>
</div>
<div class="meta">
Posted in <a href="http://forthescience.org/blog/category/topics/computer-science/languages/python/" title="View all posts in Python" rel="category tag">Python</a>, <a href="http://forthescience.org/blog/category/topics/computer-science/graphics/raytracing/" title="View all posts in Raytracing" rel="category tag">Raytracing</a>.  <span>Comments Off</span> &#187;
</div>
</div>
<div class="post-1681 post type-post status-publish format-standard hentry category-books category-python category-raytracing">
<h2><a href="http://forthescience.org/blog/2011/09/05/a-raytracer-in-python-part-1-basic-functionality/" rel="bookmark">A raytracer in python &#8211; part 1: basic functionality</a></h2>
<span class="submitted">September 5, 2011 &#8212; Stefano Borini </span>
<div class="content">
<p>Some time ago I visited Sydney, and I made a tragic mistake: I entered the University bookshop. Why a mistake, you say? I am book maniac. As soon as I enter a book shop (live or on web) I end up spending up to a thousands euro every time. This time, it was not the credit card, but the limit of 20 kg on my luggage to put a limit on what I could buy. Needless to say I got really good stuff, in particular this 761 pages of pure awesomeness: <a href="http://www.amazon.com/Ray-Tracing-Ground-Kevin-Suffern/dp/1568812728">&#8220;Ray tracing from the ground up&#8221;</a>. This book teaches you how to write a raytracer, step by step from a first basic skeleton to incredibly complex optic effects. It also provides code, available for download under GPL. I haven&#8217;t downloaded it, but according to the book it&#8217;s in C++. Since I want to keep myself fresh with python, I will write my own version in this language.</p>
<p>The first objection I may hear is performance. Native python is much slower than C++. I agree, and I do it on purpose. My objective is in fact not only to write a raytracer for fun (and the world is full of <a href="http://en.wikipedia.org/wiki/List_of_ray_tracing_software">already awesome raytracers</a>), but also to perform some infrequent python exercises: interfacing with C, parallelization and, hopefully, some OpenCL programming. This is my hope, at least. I will try to spend some time on it and see how far I can get.</p>
<h2>What is raytracing and how does it work ?</h2>
<p>Raytracing is a technique to produce a photorealistic image. It works by projecting rays from the observer to the scene, and coloring pixels on a viewplane for every ray that intersects an object.</p>
<p><a href="http://en.wikipedia.org/wiki/File:Ray_trace_diagram.svg"><img class="aligncenter" title="raytracing" src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Ray_trace_diagram.svg/500px-Ray_trace_diagram.svg.png" alt="" width="500" height="333" /></a></p>
<p>This mechanism resembles how vision works, although in the opposite direction. Light rays from a lamp hit objects and their reflection happens to scatter around. Some of these reflections will enter our eyes and allow us to see the world around us. Doing the opposite, tracing from the observer, is clearly more efficient as we don&#8217;t care about the rays not hitting the observer (at least in its basic implementation), only those who do.</p>
<p>Performing raytracing (or to be more accurate, for now just its basic form <a href="http://en.wikipedia.org/wiki/Ray_casting">raycasting</a>)Â  involves the following steps:</p>
<ol>
<li>define a geometric object in space, like for example a sphere</li>
<li>define a view panel made of pixels</li>
<li>shoot one straight line (ray) from the center of each pixel</li>
<li>if the ray intersects the object, mark the pixel colored, otherwise mark it with the background color</li>
</ol>
<p>That&#8217;s it. Basically, it&#8217;s an exercise in geometry: finding intersections between lines and 3D objects in 3D space. This first program, <a href="http://forthescience.org/blog/wp-content/uploads/2011/05/ray.py_.txt">ray.py</a>, does exactly that. You will need to install the Python Imaging Library, pygame and numpy. The result is intriguing:</p>
<div id="attachment_1682" style="width: 330px" class="wp-caption aligncenter"><a href="http://forthescience.org/blog/wp-content/uploads/2011/05/render.png"><img class="size-full wp-image-1682" title="render" src="http://forthescience.org/blog/wp-content/uploads/2011/05/render.png" alt="" width="320" height="200" /></a><p class="wp-caption-text">A rendered sphere</p></div>
<p>Ok, I have a very loose definition of &#8220;intriguing&#8221;, but it&#8217;s a start.</p>
</div>
<div class="meta">
Posted in <a href="http://forthescience.org/blog/category/resources/books/" title="View all posts in Books" rel="category tag">Books</a>, <a href="http://forthescience.org/blog/category/topics/computer-science/languages/python/" title="View all posts in Python" rel="category tag">Python</a>, <a href="http://forthescience.org/blog/category/topics/computer-science/graphics/raytracing/" title="View all posts in Raytracing" rel="category tag">Raytracing</a>.  <span>Comments Off</span> &#187;
</div>
</div>
<div class="nextprev">
<div class="alignleft"> &laquo; <a href="http://forthescience.org/blog/2011/08/20/pac-mecium-and-other-games/" rel="prev">Pac-mecium and other games</a></div>
<div class="alignright"> <a href="http://forthescience.org/blog/2011/09/10/switching-to-a-once-a-month-schedule/" rel="next">Switching to a once-a-month schedule</a> &raquo;</div>
</div>
</div><!-- end content -->    
<span class="clear"></span>
<div id="footer">
Valid <a href="http://validator.w3.org/check?uri=referer" title="Valid XHTML Strict 1.0">XHTML Strict 1.0</a> &bull; Theme: Garland-revisited by <a href="http://www.pross.org.uk">Pross</a><br />
</div>
</div></div></div></div> <!-- /.left-corner, /.right-corner, /#squeeze, /#center -->
<div id="sidebar-right" class="sidebar">
<ul class="menu">
<li id="text-3" class="widget widget_text"><h2 class="widgettitle">Found something useful?</h2>
			<div class="textwidget"><div>
Tip me via <img style="
width:42px; height:42px; vertical-align:middle" src="http://forthescience.org/blog/wp-content/uploads/2014/02/bitcoin.png"/>
<span style="font-size: 12pt"> <a href="bitcoin:13RQmVjRKVbQnVmuVsFxHjycgo7cTaaZ3w">BitCoin</a>!</span><br> <span style="font-size: 9pt">More info...</span> 
</div>
</div>
		</li>
<li id="categories-516756312" class="widget widget_categories"><h2 class="widgettitle">Categories</h2>
<select name='cat' id='cat' class='postform' >
	<option value='-1'>Select Category</option>
	<option class="level-0" value="7">Administrative</option>
	<option class="level-0" value="113">Amministrative</option>
	<option class="level-0" value="105">Argomenti</option>
	<option class="level-1" value="114">&nbsp;&nbsp;&nbsp;Biologia</option>
	<option class="level-2" value="116">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bioetica</option>
	<option class="level-2" value="136">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Evoluzione</option>
	<option class="level-1" value="115">&nbsp;&nbsp;&nbsp;Chimica</option>
	<option class="level-1" value="104">&nbsp;&nbsp;&nbsp;Informatica</option>
	<option class="level-2" value="159">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Grafica</option>
	<option class="level-2" value="178">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hardware @it</option>
	<option class="level-2" value="118">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Linguaggi</option>
	<option class="level-3" value="128">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bash @it</option>
	<option class="level-3" value="137">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C/C++ @it</option>
	<option class="level-3" value="162">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JavaScript @it</option>
	<option class="level-3" value="164">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PostScript @it</option>
	<option class="level-3" value="119">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Python @it</option>
	<option class="level-2" value="166">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sistemi Operativi</option>
	<option class="level-3" value="177">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MacOSX @it</option>
	<option class="level-0" value="100">Non categorizzate</option>
	<option class="level-0" value="66">Opinion</option>
	<option class="level-0" value="52">Personal</option>
	<option class="level-0" value="12">Resources</option>
	<option class="level-1" value="83">&nbsp;&nbsp;&nbsp;Articles</option>
	<option class="level-1" value="13">&nbsp;&nbsp;&nbsp;Books</option>
	<option class="level-1" value="19">&nbsp;&nbsp;&nbsp;Courses</option>
	<option class="level-1" value="32">&nbsp;&nbsp;&nbsp;Meetings</option>
	<option class="level-1" value="126">&nbsp;&nbsp;&nbsp;Movies</option>
	<option class="level-1" value="55">&nbsp;&nbsp;&nbsp;Software</option>
	<option class="level-1" value="57">&nbsp;&nbsp;&nbsp;TV</option>
	<option class="level-1" value="14">&nbsp;&nbsp;&nbsp;Websites</option>
	<option class="level-0" value="140">Risorse</option>
	<option class="level-1" value="143">&nbsp;&nbsp;&nbsp;Corsi</option>
	<option class="level-1" value="146">&nbsp;&nbsp;&nbsp;Software @it</option>
	<option class="level-0" value="3">Topics</option>
	<option class="level-1" value="226">&nbsp;&nbsp;&nbsp;Academia</option>
	<option class="level-1" value="91">&nbsp;&nbsp;&nbsp;Art</option>
	<option class="level-1" value="44">&nbsp;&nbsp;&nbsp;Authoring</option>
	<option class="level-1" value="18">&nbsp;&nbsp;&nbsp;Bioinformatics</option>
	<option class="level-1" value="4">&nbsp;&nbsp;&nbsp;Biology</option>
	<option class="level-2" value="45">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bioethics</option>
	<option class="level-2" value="21">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Evolution</option>
	<option class="level-2" value="197">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;genetics</option>
	<option class="level-1" value="75">&nbsp;&nbsp;&nbsp;Charity</option>
	<option class="level-1" value="38">&nbsp;&nbsp;&nbsp;Chemistry</option>
	<option class="level-2" value="73">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Computational Chemistry</option>
	<option class="level-2" value="102">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Natural compounds chemistry</option>
	<option class="level-1" value="5">&nbsp;&nbsp;&nbsp;Computer Science</option>
	<option class="level-2" value="16">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Databases</option>
	<option class="level-3" value="34">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MySQL</option>
	<option class="level-2" value="53">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Design</option>
	<option class="level-2" value="22">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Graphics</option>
	<option class="level-3" value="187" selected="selected">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Raytracing</option>
	<option class="level-2" value="174">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hardware</option>
	<option class="level-2" value="9">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Languages</option>
	<option class="level-3" value="17">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Bash</option>
	<option class="level-3" value="23">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C/C++</option>
	<option class="level-3" value="54">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fortran</option>
	<option class="level-3" value="56">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gnuplot</option>
	<option class="level-3" value="112">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JavaScript</option>
	<option class="level-3" value="25">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Perl</option>
	<option class="level-3" value="24">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;PostScript</option>
	<option class="level-3" value="10">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Python</option>
	<option class="level-3" value="33">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;R</option>
	<option class="level-2" value="26">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Operating Systems</option>
	<option class="level-3" value="29">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Linux</option>
	<option class="level-4" value="224">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ubuntu</option>
	<option class="level-3" value="31">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;MacOSX</option>
	<option class="level-3" value="58">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The Cloud</option>
	<option class="level-3" value="30">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Windows</option>
	<option class="level-2" value="15">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Refactoring</option>
	<option class="level-2" value="49">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Security</option>
	<option class="level-2" value="110">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Semantics</option>
	<option class="level-2" value="35">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Technologies</option>
	<option class="level-3" value="69">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Backups</option>
	<option class="level-3" value="72">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Content Management</option>
	<option class="level-3" value="107">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Digital Rights Management</option>
	<option class="level-3" value="59">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Email</option>
	<option class="level-3" value="60">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Google Wave</option>
	<option class="level-3" value="111">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HTML</option>
	<option class="level-3" value="68">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;iPod touch</option>
	<option class="level-3" value="109">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Microformats</option>
	<option class="level-3" value="51">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Package Management</option>
	<option class="level-3" value="80">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;QR-Codes</option>
	<option class="level-3" value="225">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Qt</option>
	<option class="level-3" value="71">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RDF</option>
	<option class="level-3" value="41">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Unicode</option>
	<option class="level-3" value="50">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Version Control</option>
	<option class="level-4" value="223">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;git</option>
	<option class="level-3" value="67">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Web frameworks</option>
	<option class="level-3" value="36">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;XML</option>
	<option class="level-2" value="40">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Testing</option>
	<option class="level-1" value="78">&nbsp;&nbsp;&nbsp;Devices</option>
	<option class="level-2" value="79">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;iPad</option>
	<option class="level-2" value="172">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;iPhone</option>
	<option class="level-1" value="106">&nbsp;&nbsp;&nbsp;Dissemination</option>
	<option class="level-1" value="175">&nbsp;&nbsp;&nbsp;Environment</option>
	<option class="level-1" value="74">&nbsp;&nbsp;&nbsp;Food</option>
	<option class="level-1" value="133">&nbsp;&nbsp;&nbsp;Gardening</option>
	<option class="level-1" value="84">&nbsp;&nbsp;&nbsp;Geology</option>
	<option class="level-2" value="85">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Earthquakes</option>
	<option class="level-1" value="63">&nbsp;&nbsp;&nbsp;Law</option>
	<option class="level-2" value="64">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Software Licensing</option>
	<option class="level-1" value="6">&nbsp;&nbsp;&nbsp;Mathematics</option>
	<option class="level-2" value="120">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Fractals</option>
	<option class="level-2" value="173">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Linear Algebra</option>
	<option class="level-2" value="127">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Probability</option>
	<option class="level-1" value="101">&nbsp;&nbsp;&nbsp;Medicine</option>
	<option class="level-1" value="121">&nbsp;&nbsp;&nbsp;Photography</option>
	<option class="level-1" value="39">&nbsp;&nbsp;&nbsp;Physics</option>
	<option class="level-2" value="93">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Large Hadron Collider</option>
	<option class="level-2" value="97">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Magnetism</option>
	<option class="level-1" value="11">&nbsp;&nbsp;&nbsp;Project Management</option>
	<option class="level-2" value="132">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Risk Management</option>
	<option class="level-1" value="65">&nbsp;&nbsp;&nbsp;Psychology</option>
	<option class="level-1" value="20">&nbsp;&nbsp;&nbsp;Robotics</option>
	<option class="level-1" value="46">&nbsp;&nbsp;&nbsp;Space</option>
	<option class="level-2" value="47">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Astrobiology</option>
	<option class="level-2" value="42">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Astronomy</option>
	<option class="level-2" value="48">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Space Exploration</option>
	<option class="level-1" value="43">&nbsp;&nbsp;&nbsp;Statistics</option>
	<option class="level-1" value="219">&nbsp;&nbsp;&nbsp;Usability</option>
</select>

<script type='text/javascript'>
/* <![CDATA[ */
	var dropdown = document.getElementById("cat");
	function onCatChange() {
		if ( dropdown.options[dropdown.selectedIndex].value > 0 ) {
			location.href = "http://forthescience.org/blog/?cat="+dropdown.options[dropdown.selectedIndex].value;
		}
	}
	dropdown.onchange = onCatChange;
/* ]]> */
</script>

</li>
 </ul>
</div></div> <!-- /container -->
</div>
<!-- /layout -->
<script type="text/javascript">

    jQuery(document).ready(function(){
        jQuery(".easy-table-creator").tablesorter({widgets: ['zebra']});


    })



</script>
</body>
</html>
