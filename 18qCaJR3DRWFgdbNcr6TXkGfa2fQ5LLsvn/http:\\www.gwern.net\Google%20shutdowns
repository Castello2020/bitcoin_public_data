http://www.gwern.net/Google%20shutdowns
HTTP/1.1 200 OK
Server: cloudflare-nginx
Date: Tue, 22 Jul 2014 13:23:29 GMT
Content-Type: text/html; charset=utf-8
Connection: close
Set-Cookie: __cfduid=d634ab23ba28f336705ac513c02f9ef9c1406035409439; expires=Mon, 23-Dec-2019 23:50:00 GMT; path=/; domain=.gwern.net; HttpOnly
x-amz-id-2: 23z6dt050MBL5QD6O7cOwx62tuqqhXuwVnme5t8lC+MDGHk+Y+TAvMsGwP6ECgj2
x-amz-request-id: 04BA8137FD22F0F8
x-amz-meta-s3cmd-attrs: uid:1000/gname:gwern/uname:gwern/gid:1000/mode:33152/mtime:1405785741/atime:1405785738/ctime:1405785741
Cache-Control: max-age=604800, public
Last-Modified: Sat, 19 Jul 2014 16:09:53 GMT
CF-RAY: 14dff3fcfbaf0097-IAD
Content-Encoding: gzip

<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8"/>
<meta name="generator" content="hakyll"/>
<meta name="google-site-verification" content="BOhOQI1uMfsqu_DopVApovk1mJD5ZBLfan0s9go3phk"/>
<meta name="author" content="gwern"/>
<meta name="description" content="Analyzing predictors of Google abandoning products; predicting future shutdowns"/>
<meta name="dc.date.issued" content="28 Mar 2013"/>
<meta name="dcterms.modified" content="18 Jul 2014"/>
<title>Predicting Google closures</title>
<link rel="stylesheet" type="text/css" href="./static/css/default.css"/>
<link href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed"/>
<link rel="shortcut icon" type="image/x-icon" href="./static/img/favicon.ico"/>
</head>
<body>
 
<div class="indent_class1"></div>
<div id="main">
<div id="sidebar">
<div id="logo"><img alt="Logo: a Gothic/Fraktur blackletter capital G/ùï≤" height="36" src="./images/logo.png" width="32"/></div>
<div id="sidebar-links">
<p>
<a href="./index" title="index: categorized list of articles">Home</a>
<a href="./About" title="Site ideals, source, content, traffic, examples, license">Site</a>
<a href="./Links" title="Who am I online, what have I done, what am I like? Contact information; sites I use; things I've worked on">Me</a>
</p>
<hr/>
<div id="sidebar-news">
<p>
<a href="./Changelog" title="What's new or updated">New:</a>
<a href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed">RSS</a>
<a href="http://eepurl.com/Kc155" title="Monthly mailing list: signup form">MAIL</a>
</p>
<hr/>
</div>
<div id="cse-sitesearch">
<script>
            (function() {
            var cx = '009114923999563836576:dv0a4ndtmly';
            var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
            gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//www.google.com/cse/cse.js?cx=' + cx;
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
            })();
          </script>
<div style="width:0px;overflow:hidden;height:0px;">
<gcse:search></gcse:search>
</div>
<form id="searchbox_009114923999563836576:dv0a4ndtmly">
<input value="009114923999563836576:dv0a4ndtmly" name="cx" type="hidden"/>
<input value="FORID:11" name="cof" type="hidden"/>
<input id="q" style name="q" size="5" type="text" placeholder="search"/>
</form>
</div>
</div>
<hr/>
<div id="metadata">
<div id="abstract"><em>Analyzing predictors of Google abandoning products; predicting future shutdowns</em></div>
<br/>
<div id="tags"><i><a href="./tags/statistics">statistics</a>, <a href="./tags/archiving">archiving</a>, <a href="./tags/predictions">predictions</a></i></div>
<br/>
<div id="page-created">created:
<br/>
<i>28 Mar 2013</i></div>
<div id="last-modified">modified:
<br/>
<i>18 Jul 2014</i></div>
<br/>
<div id="version">status:
<br/>
<i>finished</i></div>
<br/>
<div id="epistemological-status"><a href="./About#belief-tags" title="Explanation of 'belief' metadata">belief:</a>
<br/>
<i>likely</i>
</div>
<hr/>
</div>
<div id="donations">
<div id="bitcoin-donation-address">
<a href="http://en.wikipedia.org/wiki/Bitcoin">‡∏ø</a>: 18qCaJR3DRWFgdbNcr6TXkGfa2fQ5LLsvn
</div>
<div id="paypal">
<form style="display: inline" action="https://www.paypal.com/cgi-bin/webscr" method="post" onClick="_gaq.push(['_trackEvent', 'Click', 'PayPalClicked', '']);">
<div class="form-type">
<input type="hidden" name="cmd" value="_s-xclick"/>
<input type="hidden" name="hosted_button_id" value="8GSLCWGCC6AF8"/>
<input type="image" src="http://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif" name="submit" alt="Help support my writings!"/>
</div>
</form>
</div>
<div id="Gittip">
<script data-gittip-username="gwern" data-gittip-widget="button" src="//gttp.co/v1.js"></script>
</div>
</div>
</div>
 
<div id="adsense">
<a href="http://41j.com/ads/ad.html"><img alt="Advertisement for 'HTerm, The Graphical Terminal'" src="http://41j.com/ads/ad.png" height="90" width="728"></a>
</div>
<div id="header">
<h1>Predicting Google closures</h1>
</div>
<div id="content">
<div id="TOC"><ul>
<li><a href="#a-glance-back">A glance back</a></li>
<li><a href="#data">Data</a><ul>
<li><a href="#sources">Sources</a><ul>
<li><a href="#dead-products">Dead products</a></li>
<li><a href="#live-products">Live products</a></li>
</ul></li>
<li><a href="#variables">Variables</a><ul>
<li><a href="#hits">Hits</a></li>
</ul></li>
<li><a href="#processing">Processing</a></li>
</ul></li>
<li><a href="#analysis">Analysis</a><ul>
<li><a href="#descriptive">Descriptive</a><ul>
<li><a href="#shutdowns-over-time">Shutdowns over time</a></li>
</ul></li>
<li><a href="#modeling">Modeling</a><ul>
<li><a href="#logistic-regression">Logistic regression</a></li>
<li><a href="#survival-curve">Survival curve</a></li>
<li><a href="#random-forests">Random forests</a></li>
</ul></li>
<li><a href="#predictions">Predictions</a></li>
</ul></li>
<li><a href="#followups">Followups</a></li>
<li><a href="#see-also">See also</a></li>
<li><a href="#external-links">External links</a></li>
<li><a href="#appendix">Appendix</a><ul>
<li><a href="#source-code">Source code</a></li>
<li><a href="#leakage">Leakage</a></li>
</ul></li>
</ul></div>
<p>Google has occasionally shutdown services I use, and not always with serious warning (many tech companies are like that - here one day and gone the next - though Google is one of the least-worst); this is frustrating and tedious. Naturally, we are preached at by apologists that Google owes us nothing and if it‚Äôs a problem then it‚Äôs all our fault and we should have prophesied the future better (and too bad about the ordinary people who may be screwed over or the unique history<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> or data casually destroyed). But how can we have any sort of rational expectation if we lack any data or ideas about how long Google will run anything or why or how it chooses to do what it does? So in the following essay, I collect data on <a href="#sources">350 Google products</a> and look for <a href="#variables">predictive variables</a>. I find some while <a href="#modeling">modeling shutdown patterns</a>, and <a href="#predictions">make some predictions</a> about future shutdowns. Hopefully the results are interesting, useful, or both.</p>
<section id="a-glance-back" class="level1">
<h1>A glance back</h1>
<blockquote>
<p>‚ÄòThis is something that literature has always been very keen on, that technology never gets around to acknowledging. The cold wind moaning through the empty stone box. When are you gonna own up to it? Where are the Dell PCs? This is Austin, Texas. Michael Dell is the biggest tech mogul in central Texas. Why is he not here? Why is he not at least not selling his wares? Where are the dedicated gaming consoles you used to love? Do you remember how important those were? I could spend all day here just reciting the names of the causalities in your line of work. It‚Äôs always the electronic frontier. Nobody ever goes back to look at the electronic forests that were cut down with chainsaws and tossed into the rivers. And then there‚Äôs this empty pretense that these innovations make the world ‚Äúbetter‚Äù‚Ä¶Like: ‚ÄúIf we‚Äôre not making the world better, then why are we doing this at all?‚Äù Now, I don‚Äôt want to claim that this attitude is hypocritical. Because when you say a thing like that at South By: ‚ÄúOh, we‚Äôre here to make the world better‚Äù - you haven‚Äôt even <em>reached</em> the level of hypocrisy. You‚Äôre stuck at the level of childish naivete.‚Äô ‚Äì<a href="http://en.wikipedia.org/wiki/Bruce%20Sterling" title="Wikipedia: Bruce Sterling">Bruce Sterling</a>, <a href="http://www.wired.com/beyond_the_beyond/2013/04/text-of-sxsw2013-closing-remarks-by-bruce-sterling/">‚ÄúText of SXSW2013 closing remarks‚Äù</a></p>
</blockquote>
<p>The shutdown of the popular service <a href="http://en.wikipedia.org/wiki/Google%20Reader" title="Wikipedia: Google Reader">Google Reader</a>, announced on <a href="http://googleblog.blogspot.com/2013/03/a-second-spring-of-cleaning.html">13 March 2013</a>, has brought home to many people that some products they rely on exist only at Google‚Äôs sufferance: it provides the products for reasons that are difficult for outsiders to divine, may have little commitment to a product<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a><a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>, may not include their users‚Äô best interests, may choose to withdraw the product at any time for any reason<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> (especially since most of the products are services<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> &amp; not <a href="http://en.wikipedia.org/wiki/Free%20and%20open-source%20software" title="Wikipedia: Free and open-source software">FLOSS</a> in any way, and may be too tightly coupled with the Google infrastructure<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> to be spun off or sold, so when the CEO turns against it &amp; <a href="http://www.buzzfeed.com/mattlynley/google-reader-died-because-no-one-would-run-it">no Googlers</a> are willing to waste their careers championing it‚Ä¶), and users have no voice<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> - <a href="http://en.wikipedia.org/wiki/Exit%2C%20Voice%2C%20and%20Loyalty" title="Wikipedia: Exit, Voice, and Loyalty">only exit</a> as an option.</p>
<p>In the case of Reader, while Reader destroyed the original RSS reader market, there still exist some usable alternatives; the main damage is a shrinkage in the RSS audience as inevitably many users choose not to invest in a new reader or give up, and an irreversible loss of Reader‚Äôs uniquely comprehensive RSS archives covering back to 2005. Although to be fair, I should mention two major points in favor of Google:</p>
<ol type="1">
<li>a reason I did and still do use Google services is that, with a few lapses like <a href="AB%20testing#max-width">Website Optimizer</a> aside, they are almost unique in enabling users to back up their data via the work of the <a href="http://en.wikipedia.org/wiki/Google%20Data%20Liberation%20Front" title="Wikipedia: Google Data Liberation Front">Google Data Liberation Front</a> and have been far more proactive than many companies in encouraging users to back up data from dead services - for example, in automatically copying Buzz users‚Äô data to their Google Drive.</li>
<li>Google‚Äôs practices of undercutting all market incumbents with free services <em>also</em> has very large benefits<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a>, so we shouldn‚Äôt focus just on <a href="http://bastiat.org/en/twisatwins.html" title="'That Which is Seen, and That Which is Not Seen', Frederic Bastiat (1850)">the seen</a>.</li>
</ol>
<p>But nevertheless, every shutdown still hurts its users to some degree, even if we - currently<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> - can rule out the most devastating possible shutdowns, like Gmail. It would be interesting to see if shutdowns are to some degree predictable, whether there are any obvious patterns, whether common claims about relevant factors can be confirmed, and what the results might suggest for the future.</p>
</section>
<section id="data" class="level1">
<h1>Data</h1>
<section id="sources" class="level2">
<h2>Sources</h2>
<section id="dead-products" class="level3">
<h3>Dead products</h3>
<blockquote>
<p>The summer grasses ‚Äì<br/>the sole remnants of many<br/>brave warriors‚Äô dreams.</p>
</blockquote>
<p>I begin with a list of services/APIs/programs that Google has shutdown or abandoned taken from the <em>Guardian</em> article <a href="http://www.guardian.co.uk/technology/2013/mar/22/google-keep-services-closed">‚ÄúGoogle Keep? It‚Äôll probably be with us until March 2017 - on average: The closure of Google Reader has got early adopters and developers worried that Google services or APIs they adopt will just get shut off. An analysis of 39 shuttered offerings says how long they get‚Äù</a> by Charles Arthur. Arthur‚Äôs list seemed relatively complete, but I‚Äôve added in &gt;300 items he missed based on the <a href="http://www.slate.com/articles/technology/map_of_the_week/2013/03/google_reader_joins_graveyard_of_dead_google_products.html">Slate graveyard</a>, Weber‚Äôs <a href="http://thenextweb.com/google/2011/10/17/google-fails/">‚ÄúGoogle Fails 36% Of The Time‚Äù</a><a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>, the <a href="http://en.wikipedia.org/wiki/Category%3AGoogle%20acquisitions" title="Wikipedia: Category:Google acquisitions">Wikipedia category</a>/<a href="http://en.wikipedia.org/wiki/List%20of%20mergers%20and%20acquisitions%20by%20Google" title="Wikipedia: List of mergers and acquisitions by Google">list</a> for Google acquisitions, the <a href="http://en.wikipedia.org/wiki/Category%3ADiscontinued%20Google%20services" title="Wikipedia: Category:Discontinued Google services">Wikipedia category</a>/<a href="http://en.wikipedia.org/wiki/List%20of%20Google%20products%23Discontinued%20products%20and%20services" title="Wikipedia: List of Google products#Discontinued products and services">list</a>, and finally the official <a href="https://www.google.com/about/company/history/">Google History</a>. (The additional shutdowns include many shutdowns predating 2010, suggesting that Arthur‚Äôs list was biased towards recent shutdowns.)</p>
<p>In a few cases, the start dates are well-informed guesses (eg. <a href="https://plus.google.com/u/0/103530621949492999968/posts/fqxuM2SBRQ5">Google Translate</a>) and dates of abandonment/shut-down are even harder to get due to the lack of attention paid to most (Joga Bonito) and so I infer the date from archived pages on the Internet Archive, news reports, blogs such as <a href="http://googlesystem.blogspot.com/">Google Operating System</a>, the dates of press releases, the shutdown of closely related services (eReader Play based on Reader), source code repositories (AngularJS) etc; some are listed as discontinued (Google Catalogs) but are still supported or were merged into other software (Spreadsheets, Docs, Writely, News Archive) or sold/given to third parties (Flu Shot Finder, App Inventor, Body) or active effort has ceased but the content remains and so I do not list those as dead; for cases of acquired software/services that were shutdown, I date the start from Google‚Äôs purchase.</p>
</section>
<section id="live-products" class="level3">
<h3>Live products</h3>
<blockquote>
<p>‚Äú‚Ä¶He often lying broad awake, and yet / Remaining from the body, and apart / In intellect and power and will, hath heard / Time flowing in the middle of the night, / And all things creeping to a day of doom.‚Äù<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></p>
</blockquote>
<p>A major criticism of Arthur‚Äôs post was that it was fundamentally using the wrong data: if you have a dataset of all Google products which have been shutdown, you can make statements like ‚Äúthe average dead Google product lived 1459 days‚Äù, but you can‚Äôt infer very much about a live product‚Äôs life expectancy - because you don‚Äôt know if it will join the dead products. If, for example, only 1% of products ever died, then 1459 days would lead to a massive underestimates of the average lifespan of all currently living products. With his data, you can only make inferences conditional on a product eventually dying, you cannot make an unconditional inference. Unfortunately, the unconditional question ‚Äúwill it die?‚Äù is the real question any Google user wants answered!</p>
<p>So drawing on the same sources, I have compiled a second list of <em>living</em> products; the ratio of living to dead gives a base rate for how likely a randomly selected Google product is to be canceled within the 1997-2013 window, and with the date of the founding of each living product, we can also do a simple right-censored <a href="http://en.wikipedia.org/wiki/survival%20analysis" title="Wikipedia: survival analysis">survival analysis</a> which will let us make better still predictions by extracting concrete results like mean time to shutdown. Some items are obviously dead in a meaningful sense since they have been closed to new users (Sync), lost major functionality (FeedBurner, Meebo), degraded severely due to neglect (eg. <a href="Google%20Alerts" title="Go to wiki page: Google%20Alerts">Google Alerts</a>), or just been completely neglected for a decade or more (Google Group‚Äôs Usenet archive) - but haven‚Äôt actually died or closed yet, so I list them as alive.</p>
</section>
</section>
<section id="variables" class="level2">
<h2>Variables</h2>
<blockquote>
<p>To my good friend<br/>Would I show, I thought,<br/>The plum blossoms,<br/>Now lost to sight<br/>Amid the falling snow.<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a></p>
</blockquote>
<p>Simply collecting the data is useful since it allows us to make some estimates like overall death-rates or median lifespan. But maybe we can do better than just base rates and find characteristics which let us crack open the Google black box a tiny bit. So finally, for all products, I have collected several covariates which I thought might help predict longevity:</p>
<ul>
<li><p><code>Hits</code>: the number of Google hits for a service</p>
<p>While number of Google hits is a very crude measure, at best, for underlying variables like ‚Äúpopularity‚Äù or ‚Äúnumber of users‚Äù or ‚Äúprofitability‚Äù, and clearly biased towards recently released products (there aren‚Äôt going to be as many hits for, say, ‚ÄúGoogle Answers‚Äù as there would have been if we had searched for it in 2002), it may add some insight.</p>
There do not seem to be any other free quality sources indicating either historical or contemporary traffic to a product URL/homepage which could be used in the analysis - services like Alexa or Google Ad Planner either are commercial, for domains only, or simply do not cover many of the URLs. (After I finished data collection, it was pointed out to me that while Google‚Äôs Ad Planner may not be useful, Google‚Äôs AdWords <em>does</em> yield a count of global searches for a particular query that month, which would have worked albeit it would only indicate current levels of interest and nothing about historical levels.)</li>
<li><p><code>Type</code>: a categorization into ‚Äúservice‚Äù/‚Äúprogram‚Äù/‚Äúthing‚Äù/‚Äúother‚Äù</p>
<ol type="1">
<li>A <em>service</em> is anything primarily accessed through a web browser or API or the Internet; so Gmail or a browser loading fonts from a Google server, but not a Gmail notification program one runs on one‚Äôs computer or a FLOSS font available for download &amp; distribution.</li>
<li>A <em>program</em> is anything which is an application, plugin, library, framework, or all of these combined; some are very small (Authenticator) and some are very large (Android). This does include programs which require Internet connections or Google APIs as well as programs for which the source code has not been released, so things in the program category are not immune to shutdown and may be useful only as long as Google supports them.</li>
<li><p>A <em>thing</em> is anything which is primarily a physical object. A cellphone running Android or a Chromebook would be an example.</p>
In retrospect, I probably should have excluded this category entirely: there‚Äôs no reason to expect cellphones to follow the same lifecycle as a service or program, it leads to even worse classification problems (when does an Android cellphone ‚Äòdie‚Äô? should one even be looking at individual cellphones or laptops rather than entire product lines?), there tend to be many iterations of a product and they‚Äôre all hard to research, etc.</li>
<li><em>Other</em> is the catch-all category for things which don‚Äôt quite seem to fit. Where does a Google think-tank, charity, conference, or venture capital fund fit in? They certainly aren‚Äôt software, but they don‚Äôt seem to be quite services either.</li>
</ol></li>
<li><p><code>Profit</code>: whether Google <em>directly</em> makes money off a product</p>
<p>This is a tricky one. Google excuses many of its products by saying that anything which increases Internet usage benefits Google and so by this logic, every single one of its services could potentially increase profit; but this is a little stretched, the truth very hard to judge by an outsider, and one would expect that products without direct monetization are more likely to be killed.</p>
Generally, I classify as for profit any Google product directly relating to producing/displaying advertising, paid subscriptions, fees, or purchases (AdWords, Gmail, Blogger, Search, shopping engines, surveys); but many do not seem to have any form of monetization related to them (Alerts, Office, Drive, Gears, Reader<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a>). Some services like Voice charge (for international calls) but the amounts are minor enough that one might wonder if classifying them as for profit is really right. While it might make sense to define every feature added to, say, Google Search (eg. Personalized Search, or Search History) as being ‚Äòfor profit‚Äô since Search lucratively displays ads, I have chosen to classify these secondary features as being not for profit.</li>
<li><p><code>FLOSS</code>: whether the source code was released or Google otherwise made it possible for third parties to continue the service or maintain the application.</p>
<blockquote>
<p>In the long run, the utility of all non-Free software approaches zero. All non-Free software is a dead end.<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a></p>
</blockquote>
Android, AngularJS, and Chrome are all examples of software products where Google losing interest would not be fatal; services spun off to third parties would also count. Many of the codebases rely on a proprietary Google API or service (especially the mobile applications), which means that this variable is not as meaningful and laudable as one might expect, so in the minority of cases where this variable is relevant, I code <code>Dead</code> &amp; <code>Ended</code> as related to whether &amp; when Google abandoned it, regardless of whether it was then picked up by third parties or not. (Example: App Inventor for Android is listed as dying in December 2011, though it was then half a year later handed over to MIT, who has supported it since.) It‚Äôs important to not naively believe that simply because source code is available, Google support doesn‚Äôt matter.</li>
<li><p><code>Acquisition</code>: whether it was related to a purchase of a company or licensing, or internally developed.</p>
<p>This is useful for investigating the so-called <a href="http://www.slate.com/articles/technology/technology/2008/08/the_google_black_hole.single.html" title="The Google Black Hole: Sergey and Larry just bought my company. Uh oh.">‚ÄúGoogle black hole‚Äù</a>: Google has bought many startups (DoubleClick, Dodgeball, Android, Picasa), or technologies/data licensed (SYSTRAN for Translate, Twitter data for Real-Time Search), but it‚Äôs claimed many stagnate &amp; wither (Jaiku, JotSpot, Dodgeball, <a href="http://www.businessinsider.com/google-zagat-story-2013-6" title="MISERY AT GOOGLE: You'd Never Expect NSFW Graffiti Like This On Google's Bathroom Walls">Zagat</a>). So we‚Äôll include this. If a closely related product is developed and released after purchase, like a mobile application, I do not class it as an acquisition; just products that were in existence when the company was purchased. I do not include products that Google dropped immediately on purchase (Apture, fflick, Sparrow, Reqwireless, PeakStream, Wavii) or where products based on them have not been released (BumpTop).</p></li>
</ul>
<section id="hits" class="level3">
<h3>Hits</h3>
<p>Ideally we would have Google hits from the day before a product was officially killed, but the past is, alas, no longer accessible to us, and we only have hits from searches I conducted 1-5 April 2013. There are three main problems with the Google hits metric:</p>
<ol type="1">
<li>the Web keeps growing, so 1 million hits in 2000 are not equivalent to 1 million hits in 2013</li>
<li>services which are not killed live longer and can rack up more hits</li>
<li>and the longer ago a product‚Äôs hits came into existence, the more likely the relevant hits may be to have disappeared themselves.</li>
</ol>
<p>We can partially compensate by looking at hits averaged by lifespan; 100k hits means much less for something that lived for a decade than 100k hits means for something that lived just 6 months. What about the growth objection? We can estimate the size of Google‚Äôs index at any period and interpret the current hits as a fraction of the index when the service died (example: suppose Answers has 1 million hits, died in 2006, and in 2006 the index held 1 billion URLs, then we‚Äôd turn our 1m hit figure into 1/1000 or 0.001); this gives us our ‚Äúdeflated hits‚Äù. We‚Äôll deflate the hits by first estimating the size of the index by fitting an exponential to the rare public reports and third-party estimates of the size of the Google index. The data points with the best linear fit:</p>
<figure>
<img alt="Estimating Google WWW index size over time" height="406" src="./images/google/www-index-model.png" width="1127"/><figcaption>Estimating Google WWW index size over time</figcaption>
</figure>
<p>It fits reasonably well. (A sigmoid might fit better, but maybe not, given the large disagreements towards the end.) With this we can then average over days as well, giving us 4 indices to use. We‚Äôll look closer at the hit variables later.</p>
</section>
</section>
<section id="processing" class="level2">
<h2>Processing</h2>
<p>If a product has not ended, the end-date is defined as 01 April 2013 (which is when I stopped compiling products); then the total lifetime is simply the end-date minus the start-date. The final CSV is available at <a href="./docs/2013-google.csv">docs/2013-google.csv</a>. (I welcome corrections from Googlers or Xooglers about any variables like launch or shutdown dates or products directly raising revenue.)</p>
</section>
</section>
<section id="analysis" class="level1">
<h1>Analysis</h1>
<blockquote>
<p>I spur my horse past ruins<br/>Ruins move a traveler‚Äôs heart<br/>the old parapets high and low<br/>the ancient graves great and small<br/>the shuddering shadow of a tumbleweed<br/>the steady sound of giant trees.<br/>But what I lament are the common bones<br/>unnamed in the records of Immortals.<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a></p>
</blockquote>
<section id="descriptive" class="level2">
<h2>Descriptive</h2>
<p>Loading up our hard-won data and looking at an R summary (for full source code reproducing all graphs and analyses below, see the <a href="#source-code">appendix</a>; I welcome statistical corrections or elaborations if accompanied by equally reproducible R source code), we can see we have a lot of data to look out</p>
<pre class="sourceCode R"><code class="sourceCode r">    Dead            Started               Ended                 Hits               Type
 Mode :logical   Min.   :<span class="dv">1997-09-15</span>   Min.   :<span class="dv">2005-03-16</span>   Min.   :<span class="fl">2.04e+03</span>   other  :<span class="st"> </span><span class="dv">14</span>
 <span class="ot">FALSE</span>:<span class="dv">227</span>       1st Qu.:<span class="dv">2006-06-09</span>   1st Qu.:<span class="dv">2012-04-27</span>   1st Qu.:<span class="fl">1.55e+05</span>   program:<span class="st"> </span><span class="dv">92</span>
 <span class="ot">TRUE</span> :<span class="dv">123</span>       Median :<span class="dv">2008-10-18</span>   Median :<span class="dv">2013-04-01</span>   Median :<span class="fl">6.50e+05</span>   service:<span class="dv">234</span>
                 Mean   :<span class="dv">2008-05-27</span>   Mean   :<span class="dv">2012-07-16</span>   Mean   :<span class="fl">5.23e+07</span>   thing  :<span class="st"> </span><span class="dv">10</span>
                 3rd Qu.:<span class="dv">2010-05-28</span>   3rd Qu.:<span class="dv">2013-04-01</span>   3rd Qu.:<span class="fl">4.16e+06</span>
                 Max.   :<span class="dv">2013-03-20</span>   Max.   :<span class="dv">2013-11-01</span>   Max.   :<span class="fl">3.86e+09</span>
   Profit          FLOSS         Acquisition       Social             Days         AvgHits
 Mode :logical   Mode :logical   Mode :logical   Mode :logical   Min.   :<span class="st">   </span><span class="dv">1</span>   Min.   :<span class="st">      </span><span class="dv">1</span>
 <span class="ot">FALSE</span>:<span class="dv">227</span>       <span class="ot">FALSE</span>:<span class="dv">300</span>       <span class="ot">FALSE</span>:<span class="dv">287</span>       <span class="ot">FALSE</span>:<span class="dv">305</span>       1st Qu.:<span class="st"> </span><span class="dv">746</span>   1st Qu.:<span class="st">    </span><span class="dv">104</span>
 <span class="ot">TRUE</span> :<span class="dv">123</span>       <span class="ot">TRUE</span> :<span class="dv">50</span>        <span class="ot">TRUE</span> :<span class="dv">63</span>        <span class="ot">TRUE</span> :<span class="dv">45</span>        Median :<span class="dv">1340</span>   Median :<span class="st">    </span><span class="dv">466</span>
                                                                 Mean   :<span class="dv">1511</span>   Mean   :<span class="st">  </span><span class="dv">29870</span>
                                                                 3rd Qu.:<span class="dv">2112</span>   3rd Qu.:<span class="st">   </span><span class="dv">2980</span>
                                                                 Max.   :<span class="dv">5677</span>   Max.   :<span class="dv">3611940</span>
  DeflatedHits    AvgDeflatedHits  EarlyGoogle      RelativeRisk    LinearPredictor
 Min.   :<span class="fl">0.0000</span>   Min.   :-<span class="fl">36.57</span>   Mode :logical   Min.   :<span class="st"> </span><span class="fl">0.021</span>   Min.   :-<span class="fl">3.848</span>
 1st Qu.:<span class="fl">0.0000</span>   1st Qu.:<span class="st"> </span>-<span class="fl">0.84</span>   <span class="ot">FALSE</span>:<span class="dv">317</span>       1st Qu.:<span class="st"> </span><span class="fl">0.597</span>   1st Qu.:-<span class="fl">0.517</span>
 Median :<span class="fl">0.0000</span>   Median :<span class="st"> </span>-<span class="fl">0.54</span>   <span class="ot">TRUE</span> :<span class="dv">33</span>        Median :<span class="st"> </span><span class="fl">1.262</span>   Median :<span class="st"> </span><span class="fl">0.233</span>
 Mean   :<span class="fl">0.0073</span>   Mean   :<span class="st"> </span>-<span class="fl">0.95</span>                   Mean   :<span class="st"> </span><span class="fl">1.578</span>   Mean   :<span class="st"> </span><span class="fl">0.000</span>
 3rd Qu.:<span class="fl">0.0001</span>   3rd Qu.:<span class="st"> </span>-<span class="fl">0.37</span>                   3rd Qu.:<span class="st"> </span><span class="fl">2.100</span>   3rd Qu.:<span class="st"> </span><span class="fl">0.742</span>
 Max.   :<span class="fl">0.7669</span>   Max.   :<span class="st">  </span><span class="fl">0.00</span>                   Max.   :<span class="fl">12.556</span>   Max.   :<span class="st"> </span><span class="fl">2.530</span>
 ExpectedEvents   FiveYearSurvival
 Min.   :<span class="fl">0.0008</span>   Min.   :<span class="fl">0.0002</span>
 1st Qu.:<span class="fl">0.1280</span>   1st Qu.:<span class="fl">0.1699</span>
 Median :<span class="fl">0.2408</span>   Median :<span class="fl">0.3417</span>
 Mean   :<span class="fl">0.3518</span>   Mean   :<span class="fl">0.3952</span>
 3rd Qu.:<span class="fl">0.4580</span>   3rd Qu.:<span class="fl">0.5839</span>
 Max.   :<span class="fl">2.0456</span>   Max.   :<span class="fl">1.3443</span></code></pre>
 
<section id="shutdowns-over-time" class="level3">
<h3>Shutdowns over time</h3>
<blockquote>
<p><code>Google Reader</code>: ‚ÄúWho is it in the blogs that calls on me? / I hear a tongue shriller than all the YouTubes / Cry ‚ÄòReader!‚Äô Speak, Reader is turn‚Äôd to hear.‚Äù</p>
<p><code>Dataset</code>: ‚ÄúBeware the ides of <a href="http://googleblog.blogspot.com/2013/03/a-second-spring-of-cleaning.html" title="'A second spring of cleaning', 13 March 2013">March</a>.‚Äù<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a></p>
</blockquote>
<p>An interesting aspect of the shutdowns is they are unevenly distributed by month as we can see with a chi-squared test (<em>p</em>=0.014) and graphically, with a major spike in September and then March/April<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a>:</p>
<figure>
<img alt="Shutdowns binned by month of year, revealing peaks in September, March, and April" height="361" src="./images/google/shutdownsbymonth.png" width="690"/><figcaption>Shutdowns binned by month of year, revealing peaks in September, March, and April</figcaption>
</figure>
<p>As befits a company which has grown enormously since 1997, we can see other imbalances over time: eg. Google launched very few products from 1997-2004, and many more from 2005 and on:</p>
<figure>
<img alt="Starts binned by year" height="443" src="./images/google/startsbyyear.png" width="642"/><figcaption>Starts binned by year</figcaption>
</figure>
<p>We can plot lifetime against shut-down to get a clearer picture:</p>
<figure>
<img alt="All products scatter-plotted date of opening vs lifespan" height="480" src="./images/google/openedvslifespan.png" width="720"/><figcaption>All products scatter-plotted date of opening vs lifespan</figcaption>
</figure>
<p>That clumpiness around 2009 is suspicious. To emphasize this bulge of shutdowns in late 2011-2012, we can plot the histogram of dead products by year and also a kernel density:</p>
<figure>
<img alt="Shutdown density binned by year" height="442" src="./images/google/shutdownsbyyear.png" width="676"/><figcaption>Shutdown density binned by year</figcaption>
</figure>
<figure>
<img alt="Equivalent kernel density (default bandwidth)" height="444" src="./images/google/shutdownsbyyear-kernel.png" width="398"/><figcaption>Equivalent kernel density (default bandwidth)</figcaption>
</figure>
<p>The kernel density brings out an aspect of shutdowns we might have missed before: there seems to be an absence of recent shut downs. There are 4 shut downs scheduled for 2013 but the last one is scheduled for November, suggesting that we have seen the last of the 2013 casualties and that any future shut downs may be for 2014.</p>
<p>What explains such graphs over time? The obvious candidate is the 4 April 2011 accession of Larry Page to CEO, replacing Eric Schmidt who had been hired to provide ‚Äúadult supervision‚Äù for pre-IPO Google. He respected Steve Jobs greatly (he and Brin suggested, before meeting Schmidt, that their CEO be Jobs). Isaacon‚Äôs <em>Steve Jobs</em> records that before his death, Jobs had strongly advised Page to ‚Äúfocus‚Äù, and asked ‚ÄúWhat are the five products you want to focus on?‚Äù, saying ‚ÄúGet rid of the rest, because they‚Äôre dragging you down.‚Äù And indeed, on <a href="https://plus.google.com/+LarryPage/posts/dRtqKJCbpZ7">14 July 2011</a> Page posted:</p>
<blockquote>
<p>‚Ä¶Greater focus has also been another big feature for me this quarter ‚Äì more wood behind fewer arrows. Last month, for example, we announced that we will be closing Google Health and Google PowerMeter. We‚Äôve also done substantial internal work simplifying and streamlining our product lines. While much of that work has not yet become visible externally, I am very happy with our progress here. Focus and prioritization are crucial given our amazing opportunities.</p>
</blockquote>
<p>While some have <a href="http://thenextweb.com/google/2013/01/12/larry-page-did-well-to-ignore-steve-jobs/" title="Larry Page ignored Steve Jobs's deathbed advice, and Google is doing great">tried to disagree</a>, it‚Äôs hard not to conclude that indeed, a wall of shutdowns followed in late 2011 and 2012. But this sound very much like a one-time purge: if one has a new focus on focus (if you‚Äôll pardon the expression), then you may not be starting up as many services as before.</p>
</section>
</section>
<section id="modeling" class="level2">
<h2>Modeling</h2>
<section id="logistic-regression" class="level3">
<h3>Logistic regression</h3>
<p>A first step in predicting when a product will be shutdown is predicting whether it will be shutdown. Since we‚Äôre predicting a binary outcome (a product living or dying), we can use the usual: an ordinary <a href="http://en.wikipedia.org/wiki/logistic%20regression" title="Wikipedia: logistic regression">logistic regression</a>. Our first look uses the main variables plus the total hits:</p>
<pre class="sourceCode R"><code class="sourceCode r">Coefficients:
<span class="st">                </span>Estimate Std. Error z value <span class="kw">Pr</span>(&gt;<span class="er">|</span>z|)
(Intercept)       <span class="fl">2.3968</span>     <span class="fl">1.0680</span>    <span class="fl">2.24</span>    <span class="fl">0.025</span>
Typeprogram       <span class="fl">0.9248</span>     <span class="fl">0.8181</span>    <span class="fl">1.13</span>    <span class="fl">0.258</span>
Typeservice       <span class="fl">1.2261</span>     <span class="fl">0.7894</span>    <span class="fl">1.55</span>    <span class="fl">0.120</span>
Typething         <span class="fl">0.8805</span>     <span class="fl">1.1617</span>    <span class="fl">0.76</span>    <span class="fl">0.448</span>
ProfitTRUE       -<span class="fl">0.3857</span>     <span class="fl">0.2952</span>   -<span class="fl">1.31</span>    <span class="fl">0.191</span>
FLOSSTRUE        -<span class="fl">0.1777</span>     <span class="fl">0.3791</span>   -<span class="fl">0.47</span>    <span class="fl">0.639</span>
AcquisitionTRUE   <span class="fl">0.4955</span>     <span class="fl">0.3434</span>    <span class="fl">1.44</span>    <span class="fl">0.149</span>
SocialTRUE        <span class="fl">0.7866</span>     <span class="fl">0.3888</span>    <span class="fl">2.02</span>    <span class="fl">0.043</span>
<span class="kw">log</span>(Hits)        -<span class="fl">0.3089</span>     <span class="fl">0.0567</span>   -<span class="fl">5.45</span>  <span class="fl">5.1e-08</span></code></pre>
<p>In <a href="http://en.wikipedia.org/wiki/log%20odds" title="Wikipedia: log odds">log odds</a>, &gt;0 increases the chance of an event (shutdown) and &lt;0 decreases it. So looking at the coefficients, we can venture some interpretations:</p>
<ul>
<li><p>Google has a past history of screwing up social and then killing them</p>
This is interesting for confirming the general belief that Google has handled badly its social properties in the past, but I‚Äôm not sure how useful this is for predicting the future: since Larry Page became obsessed with social in 2009, a we might expect anything to do with ‚Äúsocial‚Äù will now either be merged into Google+ or otherwise be kept on life support far longer than it would before</li>
<li><p>Google is deprecating software products in favor of web services</p>
This should have been obvious to anyone watching - a lot of Google‚Äôs efforts with Firefox and then Chromium was for improving web browsers as a platform for delivering applications. As efforts like HTML5 mature, there is less incentive for Google to release and support standalone software.</li>
<li><p>But apparently not its FLOSS software</p>
This seems due to a number of its software releases being picked up by third-parties (Wave, Etherpad, Refine), designed to be integrated into existing communities (Summer of Code projects), or apparently serving a <em>strategic</em> role (Android, Chromium, Dart, Go, Closure Tools, VP Codecs) in which we could summarize as ‚Äòbuilding up a browser replacement for operating systems‚Äô. (Why? <a href="http://www.joelonsoftware.com/articles/StrategyLetterV.html">‚ÄúCommoditize your complements.‚Äù</a>)</li>
<li><p>things which charge or show advertising are more likely to survive</p>
Also obvious, but it‚Äôs good to have confirmation (if nothing else, it partially validates the data).</li>
<li><p>Popularity as measured by Google hits seems to matter</p>
<p>Likewise obvious‚Ä¶ or is it?</p></li>
</ul>
<section id="use-of-hits-data" class="level4">
<h4>Use of hits data</h4>
<p>Is our popularity metric - or any of the 4 - trustworthy? All this data has been collected after the fact, sometimes many years; what if the data have been contaminated by the fact that something shutdown? For example, by a burst of publicity about an obscure service shutting down? (Ironically, this page is contributing to the inflation of hits for any dead service mentioned.) Are we just seeing <a href="http://www.cs.umb.edu/~ding/history/470_670_fall_2011/papers/cs670_Tran_PreferredPaper_LeakingInDataMining.pdf" title="'Leakage in Data Mining: Formulation, Detection, and Avoidance', Kaufman et al 2011">information ‚Äúleakage‚Äù</a>? Leakage can be subtle, as I <a href="#leakage">learned for myself</a> doing this analysis.</p>
<p>Investigating further, hits by themselves do matter:</p>
<pre class="sourceCode R"><code class="sourceCode r">            Estimate Std. Error z value <span class="kw">Pr</span>(&gt;<span class="er">|</span>z|)
(Intercept)   <span class="fl">3.4052</span>     <span class="fl">0.7302</span>    <span class="fl">4.66</span>  <span class="fl">3.1e-06</span>
<span class="kw">log</span>(Hits)    -<span class="fl">0.3000</span>     <span class="fl">0.0549</span>   -<span class="fl">5.46</span>  <span class="fl">4.7e-08</span></code></pre>
<p>Average hits (hits over the product‚Äôs lifetime) turns out to be even more important:</p>
<pre class="sourceCode R"><code class="sourceCode r">             Estimate Std. Error z value <span class="kw">Pr</span>(&gt;<span class="er">|</span>z|)
(Intercept)    -<span class="fl">2.297</span>      <span class="fl">1.586</span>   -<span class="fl">1.45</span>    <span class="fl">0.147</span>
<span class="kw">log</span>(Hits)       <span class="fl">0.511</span>      <span class="fl">0.209</span>    <span class="fl">2.44</span>    <span class="fl">0.015</span>
<span class="kw">log</span>(AvgHits)   -<span class="fl">0.852</span>      <span class="fl">0.217</span>   -<span class="fl">3.93</span>  <span class="fl">8.3e-05</span></code></pre>
<p>This is more than a little strange; the higher the average hits, the less likely to be killed makes perfect sense but then, surely the higher the hits, the less likely as well? But no. The mystery deepens as we bring in the third hit metric we developed:</p>
<pre class="sourceCode R"><code class="sourceCode r">                  Estimate Std. Error z value <span class="kw">Pr</span>(&gt;<span class="er">|</span>z|)
(Intercept)        -<span class="fl">21.589</span>     <span class="fl">11.955</span>   -<span class="fl">1.81</span>   <span class="fl">0.0709</span>
<span class="kw">log</span>(Hits)            <span class="fl">2.054</span>      <span class="fl">0.980</span>    <span class="fl">2.10</span>   <span class="fl">0.0362</span>
<span class="kw">log</span>(AvgHits)        -<span class="fl">1.921</span>      <span class="fl">0.708</span>   -<span class="fl">2.71</span>   <span class="fl">0.0067</span>
<span class="kw">log</span>(DeflatedHits)   -<span class="fl">0.456</span>      <span class="fl">0.277</span>   -<span class="fl">1.64</span>   <span class="fl">0.1001</span></code></pre>
<p>And sure enough, if we run all 4 hit variables, 3 of them turn out to be statistically-significant and large:</p>
<pre class="sourceCode R"><code class="sourceCode r">                  Estimate Std. Error z value <span class="kw">Pr</span>(&gt;<span class="er">|</span>z|)
(Intercept)       -<span class="fl">24.6898</span>    <span class="fl">12.4696</span>   -<span class="fl">1.98</span>   <span class="fl">0.0477</span>
<span class="kw">log</span>(Hits)           <span class="fl">2.2908</span>     <span class="fl">1.0203</span>    <span class="fl">2.25</span>   <span class="fl">0.0248</span>
<span class="kw">log</span>(AvgHits)       -<span class="fl">2.0943</span>     <span class="fl">0.7405</span>   -<span class="fl">2.83</span>   <span class="fl">0.0047</span>
<span class="kw">log</span>(DeflatedHits)  -<span class="fl">0.5383</span>     <span class="fl">0.2914</span>   -<span class="fl">1.85</span>   <span class="fl">0.0647</span>
AvgDeflatedHits    -<span class="fl">0.0651</span>     <span class="fl">0.0605</span>   -<span class="fl">1.08</span>   <span class="fl">0.2819</span></code></pre>
<p>It‚Äôs not that the hit variables are somehow summarizing or proxying for the others, because if we toss in all the non-hits predictors and penalize parameters based on adding complexity without increasing fit, we still wind up with the 3 hit variables:</p>
<pre class="sourceCode R"><code class="sourceCode r">                  Estimate Std. Error z value <span class="kw">Pr</span>(&gt;<span class="er">|</span>z|)
(Intercept)        -<span class="fl">23.341</span>     <span class="fl">12.034</span>   -<span class="fl">1.94</span>   <span class="fl">0.0524</span>
AcquisitionTRUE      <span class="fl">0.631</span>      <span class="fl">0.350</span>    <span class="fl">1.80</span>   <span class="fl">0.0712</span>
SocialTRUE           <span class="fl">0.907</span>      <span class="fl">0.394</span>    <span class="fl">2.30</span>   <span class="fl">0.0213</span>
<span class="kw">log</span>(Hits)            <span class="fl">2.204</span>      <span class="fl">0.985</span>    <span class="fl">2.24</span>   <span class="fl">0.0252</span>
<span class="kw">log</span>(AvgHits)        -<span class="fl">2.068</span>      <span class="fl">0.713</span>   -<span class="fl">2.90</span>   <span class="fl">0.0037</span>
<span class="kw">log</span>(DeflatedHits)   -<span class="fl">0.492</span>      <span class="fl">0.280</span>   -<span class="fl">1.75</span>   <span class="fl">0.0793</span>
...
AIC:<span class="st"> </span><span class="fl">396.9</span></code></pre>
<p>Most of the predictors were removed as not helping a lot, 3 of the 4 hit variables survived (but not the both averaged &amp; deflated hits, suggesting it wasn‚Äôt adding much in combination), and we see two of the better predictors from earlier survived: whether something was an acquisition and whether it was social.</p>
<p>The original hits variable has the wrong sign, as expected of data leakage; now the average and deflated hits have the predicted sign (the higher the hit count, the lower the risk of death), but this doesn‚Äôt put to rest my concerns: the average hits has the right sign, yes, but now the effect size seems way too high - we reject the hits with a log-odds of +2.1 as obviously contaminated and a correlation almost 4 times larger than one of the known-good correlations (being an acquisition), but the average hits is -2 &amp; almost as big a log odds! The only variable which seems trustworthy is the deflated hits: it has the right sign and is a more plausible 5x smaller. I‚Äôll use just the deflated hits variable (although I will keep in mind that I‚Äôm still not sure it is free from data leakage).</p>
</section>
</section>
<section id="survival-curve" class="level3">
<h3>Survival curve</h3>
<p>The logistic regression helped winnow down the variables but is limited to the binary outcome of shutdown or not; it can‚Äôt use the potentially very important variable of how many days a product has survived for the obvious reason that <em>of course</em> mortality will increase with time! (‚ÄúBut this long run is a misleading guide to current affairs. In the long run we are all dead.‚Äù)</p>
<p>For looking at survival over time, <a href="http://en.wikipedia.org/wiki/survival%20analysis" title="Wikipedia: survival analysis">survival analysis</a> might be a useful elaboration. Not being previously familiar with the area, I drew on Wikipedia, <a href="http://socserv.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Cox-Regression.pdf">Fox &amp; Weisberg‚Äôs appendix</a>, <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1065034/" title="Statistics review 12: Survival analysis">Bewick et al 2004</a>, <a href="https://web.archive.org/web/20130402204254/http://www.ms.uky.edu/~mai/Rsurv.pdf" title="Use Software R to do Survival Analysis and Simulation. A tutorial">Zhou‚Äôs tutorial</a>, and Hosmer &amp; Lemeshow‚Äôs <a href="http://www.amazon.com/Applied-Survival-Analysis-Regression-Probability/dp/0471754994/?tag=gwernnet-20"><em>Applied Survival Analysis</em></a> for the following results using the <code>survival</code> library (see also <a href="http://cran.r-project.org/web/views/Survival.html">CRAN Task View: Survival Analysis</a>). Any errors are mine.</p>
<p>The initial characterization gives us an optimistic median of 2824 days (note that this is higher than Arthur‚Äôs mean of 1459 days because it addressed the conditionality issue discussed earlier by including products which were never canceled, and I made a stronger effort to collect pre-2009 products), but the lower bound is not tight and too little of the sample has died to get an upper bound:</p>
<pre class="sourceCode R"><code class="sourceCode r">records   n.max n.start  events  median <span class="fl">0.</span>95LCL <span class="fl">0.</span>95UCL
    <span class="dv">350</span>     <span class="dv">350</span>     <span class="dv">350</span>     <span class="dv">123</span>    <span class="dv">2824</span>    <span class="dv">2095</span>      <span class="ot">NA</span></code></pre>
<p>Our overall <a href="http://en.wikipedia.org/wiki/Kaplan-Meier%20estimator" title="Wikipedia: Kaplan-Meier estimator">Kaplan-Meier</a> <a href="http://en.wikipedia.org/wiki/survivorship%20curve" title="Wikipedia: survivorship curve">survivorship curve</a> looks a bit interesting:</p>
<figure>
<img alt="Shutdown cumulative probability as a function of time" height="402" src="./images/google/overall-survivorship-curve.png" width="604"/><figcaption>Shutdown cumulative probability as a function of time</figcaption>
</figure>
<p>If there were constant mortality of products at each day after their launch, we would expect a ‚Äútype II‚Äù curve where it looks like a straight line, and if the hazard <a href="http://lesswrong.com/lw/5qm/living_forever_is_hard_or_the_gompertz_curve/" title="Living Forever is Hard, or, The Gompertz Curve">increased with age like with humans</a> we would see a ‚Äútype I‚Äù graph in which the curve nose-dives; but in fact it looks like there‚Äôs a sort of ‚Äúleveling off‚Äù of deaths, suggesting a ‚Äútype III‚Äù curve; per Wikipedia:</p>
<blockquote>
<p>‚Ä¶the greatest mortality is experienced early on in life, with relatively low rates of death for those surviving this bottleneck. This type of curve is characteristic of species that produce a large number of offspring (see <a href="http://en.wikipedia.org/wiki/r%2FK%20selection%20theory" title="Wikipedia: r/K selection theory">r/K selection theory</a>).</p>
</blockquote>
<p>Very nifty: the survivorship curve is consistent with tech industry or startup philosophies of doing lots of things, iterating fast, and throwing things at the wall to see what sticks. (More pleasingly, it suggests that my dataset is not biased against the inclusion of short-lived products: if I had been failing to find a lot of short-lived products, then we would expect to see the true survivorship curve distorted into something of a type II or type I curve and not a type III curve where a lot of products are early deaths; so if there were a data collection bias against short-lived products, then the true survivorship curve must be even more extremely type III.)</p>
<p>However, it looks like the mortality only starts decreasing around 2000 days, so any product that far out must have been founded around or before 2005, which is when we previously noted that Google started pumping out a lot of products and may also have changed its shutdown-related behaviors; this could violate a basic assumption of Kaplan-Meier, that the underlying survival function isn‚Äôt itself changing over time.</p>
<p>Our next step is to fit a Cox <a href="http://en.wikipedia.org/wiki/proportional%20hazards%20model" title="Wikipedia: proportional hazards model">proportional hazards model</a> to our covariates:</p>
<pre class="sourceCode R"><code class="sourceCode r">...
  n=<span class="st"> </span><span class="dv">350</span>, number of events=<span class="st"> </span><span class="dv">123</span>

                    coef <span class="kw">exp</span>(coef) <span class="kw">se</span>(coef)     z <span class="kw">Pr</span>(&gt;<span class="er">|</span>z|)
AcquisitionTRUE    <span class="fl">0.130</span>     <span class="fl">1.139</span>    <span class="fl">0.257</span>  <span class="fl">0.51</span>    <span class="fl">0.613</span>
FLOSSTRUE          <span class="fl">0.141</span>     <span class="fl">1.151</span>    <span class="fl">0.293</span>  <span class="fl">0.48</span>    <span class="fl">0.630</span>
ProfitTRUE        -<span class="fl">0.180</span>     <span class="fl">0.836</span>    <span class="fl">0.231</span> -<span class="fl">0.78</span>    <span class="fl">0.438</span>
SocialTRUE         <span class="fl">0.664</span>     <span class="fl">1.943</span>    <span class="fl">0.262</span>  <span class="fl">2.53</span>    <span class="fl">0.011</span>
Typeprogram        <span class="fl">0.957</span>     <span class="fl">2.603</span>    <span class="fl">0.747</span>  <span class="fl">1.28</span>    <span class="fl">0.200</span>
Typeservice        <span class="fl">1.291</span>     <span class="fl">3.638</span>    <span class="fl">0.725</span>  <span class="fl">1.78</span>    <span class="fl">0.075</span>
Typething          <span class="fl">1.682</span>     <span class="fl">5.378</span>    <span class="fl">1.023</span>  <span class="fl">1.64</span>    <span class="fl">0.100</span>
<span class="kw">log</span>(DeflatedHits) -<span class="fl">0.288</span>     <span class="fl">0.749</span>    <span class="fl">0.036</span> -<span class="fl">8.01</span>  <span class="fl">1.2e-15</span>

                  <span class="kw">exp</span>(coef) <span class="kw">exp</span>(-coef) lower .<span class="dv">95</span> upper .<span class="dv">95</span>
AcquisitionTRUE       <span class="fl">1.139</span>      <span class="fl">0.878</span>     <span class="fl">0.688</span>     <span class="fl">1.884</span>
FLOSSTRUE             <span class="fl">1.151</span>      <span class="fl">0.868</span>     <span class="fl">0.648</span>     <span class="fl">2.045</span>
ProfitTRUE            <span class="fl">0.836</span>      <span class="fl">1.197</span>     <span class="fl">0.531</span>     <span class="fl">1.315</span>
SocialTRUE            <span class="fl">1.943</span>      <span class="fl">0.515</span>     <span class="fl">1.163</span>     <span class="fl">3.247</span>
Typeprogram           <span class="fl">2.603</span>      <span class="fl">0.384</span>     <span class="fl">0.602</span>    <span class="fl">11.247</span>
Typeservice           <span class="fl">3.637</span>      <span class="fl">0.275</span>     <span class="fl">0.878</span>    <span class="fl">15.064</span>
Typething             <span class="fl">5.377</span>      <span class="fl">0.186</span>     <span class="fl">0.724</span>    <span class="fl">39.955</span>
<span class="kw">log</span>(DeflatedHits)     <span class="fl">0.749</span>      <span class="fl">1.334</span>     <span class="fl">0.698</span>     <span class="fl">0.804</span>

Concordance=<span class="st"> </span><span class="fl">0.726</span>  (<span class="dt">se =</span> <span class="fl">0.028</span> )
Rsquare=<span class="st"> </span><span class="fl">0.227</span>   (max <span class="dt">possible=</span> <span class="fl">0.974</span> )
Likelihood ratio test=<span class="st"> </span><span class="fl">90.1</span>  on <span class="dv">8</span> df,   p=<span class="fl">4.44e-16</span>
Wald test            =<span class="st"> </span><span class="fl">79.5</span>  on <span class="dv">8</span> df,   p=<span class="fl">6.22e-14</span>
<span class="kw">Score</span> (logrank) test =<span class="st"> </span><span class="fl">83.5</span>  on <span class="dv">8</span> df,   p=<span class="fl">9.77e-15</span></code></pre>
<p>And then we can also test whether any of the covariates are suspicious; in general they seem to be fine:</p>
<pre class="sourceCode R"><code class="sourceCode r">                      rho  chisq     p
AcquisitionTRUE   -<span class="fl">0.0252</span> <span class="fl">0.0805</span> <span class="fl">0.777</span>
FLOSSTRUE          <span class="fl">0.0168</span> <span class="fl">0.0370</span> <span class="fl">0.848</span>
ProfitTRUE        -<span class="fl">0.0694</span> <span class="fl">0.6290</span> <span class="fl">0.428</span>
SocialTRUE         <span class="fl">0.0279</span> <span class="fl">0.0882</span> <span class="fl">0.767</span>
Typeprogram        <span class="fl">0.0857</span> <span class="fl">0.9429</span> <span class="fl">0.332</span>
Typeservice        <span class="fl">0.0936</span> <span class="fl">1.1433</span> <span class="fl">0.285</span>
Typething          <span class="fl">0.0613</span> <span class="fl">0.4697</span> <span class="fl">0.493</span>
<span class="kw">log</span>(DeflatedHits) -<span class="fl">0.0450</span> <span class="fl">0.2610</span> <span class="fl">0.609</span>
GLOBAL                 <span class="ot">NA</span> <span class="fl">2.5358</span> <span class="fl">0.960</span></code></pre>
<p>My suspicion lingers, though, so I threw in another covariate (<code>EarlyGoogle</code>): whether a product was released before or after 2005. Does this add predictive value above and over simply knowing that a product is really old, and does the regression still pass the proportional assumption check? Apparently yes to both:</p>
<pre class="sourceCode R"><code class="sourceCode r">                     coef <span class="kw">exp</span>(coef) <span class="kw">se</span>(coef)     z <span class="kw">Pr</span>(&gt;<span class="er">|</span>z|)
AcquisitionTRUE    <span class="fl">0.1674</span>    <span class="fl">1.1823</span>   <span class="fl">0.2553</span>  <span class="fl">0.66</span>    <span class="fl">0.512</span>
FLOSSTRUE          <span class="fl">0.1034</span>    <span class="fl">1.1090</span>   <span class="fl">0.2922</span>  <span class="fl">0.35</span>    <span class="fl">0.723</span>
ProfitTRUE        -<span class="fl">0.1949</span>    <span class="fl">0.8230</span>   <span class="fl">0.2318</span> -<span class="fl">0.84</span>    <span class="fl">0.401</span>
SocialTRUE         <span class="fl">0.6541</span>    <span class="fl">1.9233</span>   <span class="fl">0.2601</span>  <span class="fl">2.51</span>    <span class="fl">0.012</span>
Typeprogram        <span class="fl">0.8195</span>    <span class="fl">2.2694</span>   <span class="fl">0.7472</span>  <span class="fl">1.10</span>    <span class="fl">0.273</span>
Typeservice        <span class="fl">1.1619</span>    <span class="fl">3.1960</span>   <span class="fl">0.7262</span>  <span class="fl">1.60</span>    <span class="fl">0.110</span>
Typething          <span class="fl">1.6200</span>    <span class="fl">5.0529</span>   <span class="fl">1.0234</span>  <span class="fl">1.58</span>    <span class="fl">0.113</span>
<span class="kw">log</span>(DeflatedHits) -<span class="fl">0.2645</span>    <span class="fl">0.7676</span>   <span class="fl">0.0375</span> -<span class="fl">7.06</span>  <span class="fl">1.7e-12</span>
EarlyGoogleTRUE   -<span class="fl">1.0061</span>    <span class="fl">0.3656</span>   <span class="fl">0.5279</span> -<span class="fl">1.91</span>    <span class="fl">0.057</span>
...
Concordance=<span class="st"> </span><span class="fl">0.728</span>  (<span class="dt">se =</span> <span class="fl">0.028</span> )
Rsquare=<span class="st"> </span><span class="fl">0.237</span>   (max <span class="dt">possible=</span> <span class="fl">0.974</span> )
Likelihood ratio test=<span class="st"> </span><span class="fl">94.7</span>  on <span class="dv">9</span> df,   p=<span class="fl">2.22e-16</span>
Wald test            =<span class="st"> </span><span class="fl">76.7</span>  on <span class="dv">9</span> df,   p=<span class="fl">7.2e-13</span>
<span class="kw">Score</span> (logrank) test =<span class="st"> </span><span class="fl">83.8</span>  on <span class="dv">9</span> df,   p=<span class="fl">2.85e-14</span></code></pre>
<pre class="sourceCode R"><code class="sourceCode r">                       rho   chisq     p
...
EarlyGoogleTRUE   -<span class="fl">0.05167</span> <span class="fl">0.51424</span> <span class="fl">0.473</span>
GLOBAL                  <span class="ot">NA</span> <span class="fl">2.52587</span> <span class="fl">0.980</span></code></pre>
<p>As predicted, the pre-2005 variable does indeed correlate to less chance of being shutdown, is the third-largest predictor, and almost reaches a random<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a> level of statistical-significance - but it doesn‚Äôt trigger the assumption tester, so we‚Äôll keep using the Cox model.</p>
<p>Now let‚Äôs interpret the model. The covariates tell us that to reduce the risk of shutdown, you want to:</p>
<ol type="1">
<li>Not be an acquisition</li>
<li>Not be FLOSS</li>
<li>Be directly making money</li>
<li>Not be related to social networking</li>
<li>Have lots of Google hits relative to lifetime</li>
<li>Have been launched early in Google‚Äôs lifetime</li>
</ol>
<p>This all makes sense to me. I find particularly interesting the profit and social effects, but the odds are a little hard to understand intuitively; if being social increases the odds of shutdown by 1.9233 and not being directly profitable increases the odds by 1.215, what do those <em>look</em> like? We can graph pairs of survivorship curves, splitting the full dataset (omitting the confidence intervals for legibility, although they do overlap), to get a grasp of what these numbers mean:</p>
<figure>
<img alt="All products over time, split by Profit variable" height="403" src="./images/google/profit-survivorship-curve.png" width="603"/><figcaption>All products over time, split by <code>Profit</code> variable</figcaption>
</figure>
<figure>
<img alt="All products over time, split by Social variable" height="402" src="./images/google/social-survivorship-curve.png" width="603"/><figcaption>All products over time, split by <code>Social</code> variable</figcaption>
</figure>
</section>
<section id="random-forests" class="level3">
<h3>Random forests</h3>
<p>Because I can, I was curious how <a href="http://en.wikipedia.org/wiki/random%20forests" title="Wikipedia: random forests">random forests</a> (<a href="https://web.archive.org/web/20140430075247/http://oz.berkeley.edu/~breiman/randomforest2001.pdf" title="Random Forests">Breiman 2001</a>) might stack up to the logistic regression and against a base-rate predictor (that nothing was shut down, since ~65% of the products are still alive).</p>
<p>With <a href="http://cran.r-project.org/web/packages/randomForest/index.html"><code>randomForest</code></a>, I trained a random forest as a classifier, yielding reasonable looking error rates:</p>
<pre class="sourceCode R"><code class="sourceCode r">               Type of random forest:<span class="st"> </span>classification
                     Number of trees:<span class="st"> </span><span class="dv">500</span>
No. of variables tried at each split:<span class="st"> </span><span class="dv">2</span>

        OOB estimate of  error rate:<span class="st"> </span><span class="fl">31.71</span>%
Confusion matrix:
<span class="st">      </span><span class="ot">FALSE</span> <span class="ot">TRUE</span> class.error
<span class="ot">FALSE</span>   <span class="dv">216</span>   <span class="dv">11</span>     <span class="fl">0.04846</span>
<span class="ot">TRUE</span>    <span class="dv">100</span>   <span class="dv">23</span>     <span class="fl">0.81301</span></code></pre>
<p>To compare the random forest accuracy with the logistic model‚Äôs accuracy, I interpreted the logistic estimate of shutdown odds &gt;1 as predicting shutdown and &lt;1 as predicting not shutdown; I then compared the full sets of predictions with the actual shutdown status. (This is not a <a href="http://en.wikipedia.org/wiki/proper%20scoring%20rule" title="Wikipedia: proper scoring rule">proper scoring rule</a> like those I employed in grading forecasts of the <a href="2012%20election%20predictions">2012 American elections</a>, but this should be an intuitively understandable way of grading models‚Äô predictions.)</p>
<p>The base-rate predictor got 65% right by definition, the logistic managed to score 68% correct (<a href="http://en.wikipedia.org/wiki/Bootstrapping%20%28statistics%29" title="Wikipedia: Bootstrapping (statistics)">bootstrap</a><a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a> 95% CI: 66-72%), and the random forest similarly got 68% (67-78%). These rates are not quite as bad as they may seem: I excluded the lifetime length (<code>Days</code>) from the logistic and random forests because unless one is handling it specially with survival analysis, <a href="#leakage">it leaks information</a>; so there‚Äôs predictive power being left on the table. Regardless, there‚Äôs no real reason to switch to the more complex random forests.</p>
<section id="random-survival-forests" class="level4">
<h4>Random survival forests</h4>
<p>The obvious next step is to take into accounts lifetime length &amp; estimated survival curves. We can do that using <a href="http://arxiv.org/pdf/0811.1645" title="Ishwaran et al 2008">‚ÄúRandom survival forests‚Äù</a> (see also <a href="http://www.jstatsoft.org/v50/i11/paper" title="Evaluating Random Forests for Survival Analysis: Using Prediction Error Curves">‚ÄúMogensen et al 2012‚Äù</a>), implemented in <a href="http://cran.r-project.org/web/packages/randomForestSRC/index.html"><code>randomForestSRC</code></a> (successor to Ishwaran‚Äôs original library <a href="http://cran.r-project.org/web/packages/randomSurvivalForest/index.html"><code>randomSurvivalForest</code></a>). This initially seems very promising:</p>
<pre class="sourceCode R"><code class="sourceCode r">                         Sample size:<span class="st"> </span><span class="dv">350</span>
                    Number of deaths:<span class="st"> </span><span class="dv">122</span>
                     Number of trees:<span class="st"> </span><span class="dv">1000</span>
          Minimum terminal node size:<span class="st"> </span><span class="dv">3</span>
       Average no. of terminal nodes:<span class="st"> </span><span class="fl">61.05</span>
No. of variables tried at each split:<span class="st"> </span><span class="dv">3</span>
              Total no. of variables:<span class="st"> </span><span class="dv">7</span>
                            Analysis:<span class="st"> </span>Random Forests [S]RC
                              Family:<span class="st"> </span>surv
                      Splitting rule:<span class="st"> </span>logrank *random*
<span class="st">       </span>Number of random split points:<span class="st"> </span><span class="dv">1</span>
              Estimate of error rate:<span class="st"> </span><span class="fl">35.37</span>%</code></pre>
<p>and even gives us a cute plot of how accuracy varies with how big the forest is (looks like we don‚Äôt need to tweak it) and how important each variable is as a predictor:</p>
<figure>
<img alt="Visual comparison of the average usefulness of each variable to decision trees" height="393" src="./images/google/rsf-importance.png" width="694"/><figcaption>Visual comparison of the average usefulness of each variable to decision trees</figcaption>
</figure>
<p>Estimating the error rate for this random survival forest like we did previously, we‚Äôre happy to see a 78% error rate. Building a predictor based on the Cox model, we get a lesser (but still better than the non-survival models) 72% error rate.</p>
<p>How do these models perform when we check their robustness via the bootstrap? Not so great. The random survival forest collapses to 57-64% (95% on 200 replicates), but the Cox model just to 68-73%. This suggests to me that something is going wrong with the random survival forest model (overfitting? programming error?), so here too we‚Äôll stick with the ordinary Cox model.</p>
</section>
</section>
</section>
<section id="predictions" class="level2">
<h2>Predictions</h2>
<p>Before making explicit predictions of the future, let‚Äôs look at the <a href="http://en.wikipedia.org/wiki/relative%20risks" title="Wikipedia: relative risks">relative risks</a> for products which haven‚Äôt been shutdown. What does the Cox model consider the 10 most at risk and likely to be shutdown products?</p>
<p>It lists (in decreasingly risky order):</p>
<ol type="1">
<li>Schemer</li>
<li>Boutiques</li>
<li>Magnifier</li>
<li>Hotpot</li>
<li>Page Speed Online API</li>
<li>WhatsonWhen</li>
<li>Unofficial Guides</li>
<li>WDYL search engine</li>
<li>Cloud Messaging</li>
<li>Correlate</li>
</ol>
<p>These all seem like reasonable products to signal out (as much as I love Correlate for making it <a href="http://slatestarcodex.com/2013/02/16/google-correlate-does-not-imply-google-causation/">easier than ever to demonstrate ‚Äúcorrelation‚â†causation‚Äù</a>, I‚Äôm surprised it or Boutiques still exist), except for Cloud Messaging which seems to be a key part of a lot of Android. And likewise, the list of the 10 <em>least</em> risky (increasingly risky order):</p>
<ol type="1">
<li>Search</li>
<li>Translate</li>
<li>AdWords</li>
<li>Picasa</li>
<li>Groups</li>
<li>Image Search</li>
<li>News</li>
<li>Books</li>
<li>Toolbar</li>
<li>AdSense</li>
</ol>
<p>One can‚Äôt imagine flagship products like Search or Books ever being shut down, so this list is good as far as it goes; I am skeptical about the actual unriskiness of Picasa and Toolbar given their general neglect and old-fashionedness, though I understand why the model favors them (both are pre-2005, proprietary, many hits, and advertising-supported). But let‚Äôs get more specific; looking at still alive services, what predictions do we make about the odds of a selected batch surviving the next, say, 5 years? We can derive a survival curve for each member of the batch adjusted for each subject‚Äôs covariates (and they visibly differ from each other):</p>
<figure>
<img alt="Estimated curves for 15 interesting products (AdSense, Scholar, Voice, etc)" height="442" src="./images/google/15-predicted-survivorship-curves.png" width="666"/><figcaption>Estimated curves for 15 interesting products (AdSense, Scholar, Voice, etc)</figcaption>
</figure>
<p>But these are the curves for hypothetical populations all like the specific product in question, starting from Day 0. Can we extract specific estimates assuming the product has survived to today (as by definition these live services have done)? Yes, but extracting them turns out to be a pretty gruesome hack to extract predictions from survival curves; anyway, I derive the following 5-year estimates and as commentary, register my own best guesses as well (I‚Äôm <a href="Prediction%20markets">not too bad</a> at making predictions):</p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">Product</th>
<th style="text-align: left;">5-year survival</th>
<th style="text-align: left;">Personal guess</th>
<th style="text-align: left;">Relative risk vs average (lower=better)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">AdSense</td>
<td style="text-align: left;">100%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17897">99%</a></td>
<td style="text-align: left;">0.07</td>
</tr>
<tr class="even">
<td style="text-align: left;">Alerts</td>
<td style="text-align: left;">89%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17898">70%</a></td>
<td style="text-align: left;">0.21</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Analytics</td>
<td style="text-align: left;">76%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17899">97%</a></td>
<td style="text-align: left;">0.24</td>
</tr>
<tr class="even">
<td style="text-align: left;">Blogger</td>
<td style="text-align: left;">100%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17900">80%</a></td>
<td style="text-align: left;">0.32</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Calendar</td>
<td style="text-align: left;">66%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17901">95%</a></td>
<td style="text-align: left;">0.36</td>
</tr>
<tr class="even">
<td style="text-align: left;">Docs</td>
<td style="text-align: left;">63%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17902">95%</a></td>
<td style="text-align: left;">0.39</td>
</tr>
<tr class="odd">
<td style="text-align: left;">FeedBurner</td>
<td style="text-align: left;">43%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17903">35%</a></td>
<td style="text-align: left;">0.66</td>
</tr>
<tr class="even">
<td style="text-align: left;">Gmail</td>
<td style="text-align: left;">96%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17904">99%</a></td>
<td style="text-align: left;">0.08</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Google+</td>
<td style="text-align: left;">79%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17905">85%</a></td>
<td style="text-align: left;">0.36</td>
</tr>
<tr class="even">
<td style="text-align: left;">Scholar</td>
<td style="text-align: left;">92%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17906">85%</a></td>
<td style="text-align: left;">0.10</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Voice<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a></td>
<td style="text-align: left;">44%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17907">50%</a></td>
<td style="text-align: left;">0.78</td>
</tr>
<tr class="even">
<td style="text-align: left;">Chrome</td>
<td style="text-align: left;">70%</td>
<td style="text-align: left;">[95%][C]</td>
<td style="text-align: left;">0.24</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Project Glass</td>
<td style="text-align: left;">37%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17911">50%</a></td>
<td style="text-align: left;">0.10</td>
</tr>
<tr class="even">
<td style="text-align: left;">Search</td>
<td style="text-align: left;">96%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17912">100%</a></td>
<td style="text-align: left;">0.05</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Translate</td>
<td style="text-align: left;">92%</td>
<td style="text-align: left;"><a href="http://predictionbook.com/predictions/17913">95%</a></td>
<td style="text-align: left;">0.78</td>
</tr>
</tbody>
</table>
<p>One immediately spots that some of the model‚Äôs estimates seem questionable in the light of our greater knowledge of Google.</p>
<p>I am more pessimistic about the <a href="Google%20Alerts">much-neglected Alerts</a>. And I think it‚Äôs absurd to give any serious credence Analytics or Calendar or Docs being at risk (Analytics is a key part of the advertising infrastructure, and Calendar a sine qua non of any business software suite much less the core of said suite itself). The Glass estimate is also interesting: I don‚Äôt know if I agree with the model, given how famous Glass is and how much Google is pushing it - could its future really be so chancy? On the other hand, many tech fads have come and go without a trace, hardware is always tricky, the more intimate a gadget the more design matters (Glass seems like the sort of thing Apple could make a blockbuster, but can Google?), Glass has already received a <a href="http://www.businessinsider.com/nobody-really-likes-google-glass-2013-5">hefty helping of criticism</a>, particularly the man most experienced with such HUDs (<a href="http://en.wikipedia.org/wiki/Steve%20Mann" title="Wikipedia: Steve Mann">Steve Mann</a>) <a href="http://spectrum.ieee.org/geek-life/profiles/steve-mann-my-augmediated-life" title="Steve Mann: My 'Augmediated' Life: What I've learned from 35 years of wearing computerized eyewear">has criticized Glass</a> as being ‚Äúmuch less ambitious‚Äù than the state of the art and worries that ‚ÄúGoogle and certain other companies are neglecting some important lessons. Their design decisions could make it hard for many folks to use these systems. Worse, poorly configured products might even damage some people‚Äôs eyesight and set the movement back years. My concern comes from direct experience.‚Äù</p>
<p>But some estimates are more forgivable - Google <em>does</em> have a bad track record with social media so some level of skepticism about Google+ seems warranted - and on FeedBurner or Voice, I agree with the model that their future is cloudy. The extreme optimism about Blogger is interesting since before I began this project, I thought it was slowly dying and would inevitably shut down in a few years; but as I researched the timelines for various Google products, I noticed that Blogger seems to be favored in some ways: such as getting exclusive access to a few otherwise shutdown things (eg. Scribe &amp; Friend Connect); it was the ground zero for Google‚Äôs Dynamic Views skin redesign which was applied globally; and Google is still heavily using Blogger for all its official announcements even into the Google+ era.</p>
<p>Overall, these are pretty sane-sounding estimates.</p>
</section>
</section>
<section id="followups" class="level1">
<h1>Followups</h1>
<blockquote>
<p>Show me the person who doesn‚Äôt die -<br/>death remains impartial.<br/>I recall a towering man<br/>who is now a pile of dust.<br/>The World Below knows no dawn<br/>though plants enjoy another spring;<br/>those visiting this sorrowful place<br/>the pine wind slays with grief.<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a></p>
</blockquote>
<p>It seems like it might be worthwhile to continue compiling a database and do a followup analysis in 5 years (2018), by which point we can judge how my predictions stacked up against the model, and also because ~100 products may have been shut down (going by the &gt;30 casualties of 2011 and 2012) and the survival curve &amp; covariate estimates rendered that much sharper. So to compile updates, I‚Äôve:</p>
<ul>
<li><p>set up 2 Google Alerts searches:</p>
<ul>
<li><code>google (&quot;shut down&quot; OR &quot;shut down&quot; &quot;shutting&quot; OR &quot;closing&quot; OR &quot;killing&quot; OR &quot;abandoning&quot; OR &quot;leaving&quot;)</code></li>
<li><code>google (launch OR release OR announce)</code></li>
</ul></li>
<li><p>and subscribed to the aforementioned Google Operating System blog</p></li>
</ul>
<p>These sources yielded ~64 candidates over the following year before I shut down additions 4 June 2014.</p>
</section>
<section id="see-also" class="level1">
<h1>See also</h1>
<ul>
<li><a href="Archiving%20URLs" title="Go to wiki page: Archiving%20URLs">Archiving URLs</a></li>
<li><a href="hpmor#survival-analysis">survival analysis of <em>MoR</em> readers</a></li>
<li><a href="Wikipedia%20and%20Knol" title="Go to wiki page: Wikipedia%20and%20Knol">Wikipedia and Knol</a></li>
</ul>
</section>
<section id="external-links" class="level1">
<h1>External links</h1>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Archive%20Team" title="Wikipedia: Archive Team">Archive Team</a> (<a href="http://www.archiveteam.org/index.php?title=ArchiveTeam_Warrior">ArchiveTeam Warrior</a>: <a href="http://www.archiveteam.org/index.php?title=Google_Reader">Reader</a>)</li>
<li><p>Comments:</p>
<ul>
<li><a href="https://news.ycombinator.com/item?id=5653748">Hacker News</a></li>
<li><a href="http://www.metafilter.com/127712/In-a-few-cases-the-start-dates-are-wellinformed-guesses">Metafilter</a></li>
</ul></li>
<li><p>Article coverage:</p>
<ul>
<li><a href="http://arstechnica.com/business/2013/05/google-services-survive-if-they-make-money-arent-social/" title="Google services survive if they make money, aren't social: Statistical analysis of Google products finds a shutdown rate of 35 percent"><em>Ars Technica</em></a> (<a href="http://arstechnica.com/business/2013/05/google-services-survive-if-they-make-money-arent-social/?comments=1">comments</a>)</li>
<li><a href="http://www.theatlantic.com/technology/archive/2013/05/interesting-software-follow-up-scrivener-googles-orphans/275563/">‚Äú‚ÄòInteresting‚Äô Software Follow-Up: Scrivener, Google‚Äôs Orphans: A new, free guide to an exceptional program‚Äù</a> (James Fallows, <em>Atlantic</em>)</li>
<li><p><em>Forbes</em>:</p>
<ul>
<li><a href="http://www.forbes.com/sites/haydnshaughnessy/2013/05/08/what-is-driving-the-google-stock-price-up/">‚ÄúIs This One Emotion Driving The Google Stock Price Up?‚Äù</a></li>
<li><a href="http://www.forbes.com/sites/haydnshaughnessy/2013/05/09/google-glass-has-only-a-37-chance-of-going-five-years-lessons/">‚ÄúGoogle Glass - Is It Meant To Last?‚Äù</a></li>
</ul></li>
<li><a href="http://bgr.com/2013/05/07/google-services-shut-down-study/" title="Statistical analysis finds Google shuts down 35% of its services"><em>Boy Genius Report</em></a></li>
</ul></li>
<li><p>Acquisition background:</p>
<ul>
<li><a href="http://paulgraham.com/yahoo.html">‚ÄúWhat Happened to Yahoo‚Äù</a>, Paul Graham</li>
<li><p>37signals‚Äô <a href="http://www.google.com/search?q=%22Exit+Interview%22&amp;sitesearch=37signals.com/svn/posts/">‚ÄúExit Interview‚Äù series</a>:</p>
<ol type="1">
<li><a href="http://37signals.com/svn/posts/2883-exit-interview-jaikus-jyri-engestrm">‚ÄúExit interview: Jaiku‚Äôs Jyri Engestr√∂m‚Äù</a></li>
<li><a href="http://37signals.com/svn/posts/2942-exit-interview-founders-look-back-at-acquisitions-by-google-aol-microsoft-and-more">‚ÄúExit Interview: Founders look back at acquisitions by Google, AOL, Microsoft, and more‚Äù</a></li>
<li><a href="http://37signals.com/svn/posts/2806-exit-interview-ask-jeeves-acquisition-of-bloglines">‚ÄúExit Interview: Ask Jeeves‚Äô acquisition of Bloglines‚Äù</a></li>
<li><a href="http://37signals.com/svn/posts/2777-what-happens-after-yahoo-acquires-you">‚ÄúWhat happens after Yahoo acquires you‚Äù</a></li>
</ol></li>
</ul></li>
</ul>
<!--
How to handle updates? I guess we'll store it here. Idea: new covariate for 'business' vs 'consumer' vs 'both'. New data:

"Product","Dead","Started","Ended","Hits","Type","Profit","FLOSS","Acquisition","Social"
"Inactive Account Manager",FALSE,2013-04-11,NA,175000,service,FALSE,FALSE,FALSE,FALSE
"Affiliate Network",TRUE,2007-04-13,2013-04-16,3480000,service,TRUE,FALSE,TRUE,FALSE http://googleaffiliatenetwork-blog.blogspot.in/2013/04/an-update-on-google-affiliate-network.html
Behavio's FunF http://www.sfgate.com/technology/businessinsider/article/Google-Just-Bought-A-Cool-SXSW-Startup-Behavio-4430716.php supposedly Google will continue it but it may be killed too quickly to count for this list
"AdWords Pay Per Action",TRUE,2007-03-20,2008-08-31,144000,service,TRUE,FALSE,TRUE,FALSE
"AdSense Referrals",TRUE,2007-04-05,2008-08-31,25000,service,TRUE,FALSE,FALSE,FALSE
"Quick View in Search",TRUE,2009-10-07,2013-04-24,2460000,service,FALSE,FALSE,FALSE,FALSE http://googleblog.blogspot.com/2009/10/quickly-view-formatted-pdfs-in-your.html http://googlesystem.blogspot.com/2013/04/no-more-quick-view-in-google-search.html exact search query: "google search" "quick view"
WhatsonWhen: dead? site down after January 2013, but no apparent announcements. "google whatsonwhen " 48,400 results
Free Zone: http://www.nation.lk/edition/biz-news/item/17466-dialog-axiata-and-google-launch-free-zone.html 'google "free zone"' 3,030,000 results
Meebo Bar: http://www.pcmag.com/article2/0,2817,2418268,00.asp 'google "meebo bar"' 160,000 results killed 6 June 2013 http://gigaom.com/2013/06/06/a-year-after-google-acquistion-meebo-bar-will-discontinue-as-team-settles-in-with-google/ social acquisition ad-supported
'paid channels'? http://www.wired.com/gadgetlab/2013/05/youtube-adds-paid-channel-subscriptions/ http://youtube-global.blogspot.com/2013/05/yt-pc-2013.html
SMS down 9 May 2013:  http://productforums.google.com/forum/#!msg/websearch/yKG7BGro7QQ/ntAXQWWKj70J https://news.ycombinator.com/item?id=5695086 "google sms" 579,000 other results.
Scratchpad free program non-social non-acquisition non-floss  "google scratchpad" 965 results  dead 2 November 2012 http://www.omgchrome.com/google-stopping-development-of-scratchpad-web-app/ general description: https://support.google.com/chrome_webstore/answer/183097?hl=en merged into Keep? "* * Scratchpad is moving to Google Keep. * * If you have unsynced Scratchpad notes, please open the app and follow instructions to save your notes. You can add Google Keep here:  https://chrome.google.com/webstore/detail/google-keep/hmjkmjkepdijhoojdojkdfohbdgmmhki" when was it launched? best guess, January 2011 http://voices.yahoo.com/scratchpad-notepad-application-google-chrome-7581750.html?cat=15
Cloud Messaging for Chrome http://developer.chrome.com/apps/cloudMessaging.html "service",FALSE,FALSE,FALSE,FALSE launch: 10 May 2013 search 'Google "Cloud Messaging for Chrome"' 23,300 results
Google Play Music 15 05 2013 paid service non-acquisition non-FLOSS social? ("You can share songs on your google+") '"google play music"' About 37,300,000 results (0.46 seconds)
google Music: was actually dead 19 October 2012! http://www.reuters.com/article/2012/09/21/net-us-google-china-music-idUSBRE88K07120120921
Google+ Games, launched 11 08 2011 http://googleblog.blogspot.com/2011/08/games-in-google-fun-that-fits-your.html Dead, 30 June 2013 https://support.google.com/plus/answer/3123176?p=plus_games&rd=1 non-FLOSS, non-acquisition profit social, '"google+ games"' 142000 hits
Checkout Gadget: dead, see http://googlecommerce.blogspot.com/2013/05/an-update-to-google-checkout-for.html https://support.google.com/checkout/sell/answer/3080449
is checkout proper dead?
"On November 20th, 2013, Google will shut down its Checkout product. Here's how this may affect you:

    Merchants selling digital goods may transition to Google Wallet for digital goods
    Merchants selling through Google-hosted marketplaces (e.g. Google Play) will be unaffected
    Merchants selling physical goods will need to switch to third-party alternatives (see below)"
https://news.ycombinator.com/item?id=5740447 (also https://news.ycombinator.com/item?id=6579812 / https://news.ycombinator.com/item?id=6581906 )
"> 'Wait, so what's the difference between this and Google Wallet?' With Checkout, Google is a credit card processor. With Wallet, Google isn't a credit card processor, they are partnered with Bankcorp Bank who is issuing virtual cards which are funded either by transferring funds from the users bank account or by charging credit cards."

04:57:29 < quanticle> gwern: When you get this, would you mind giving me your opinion on the continued long-term viability of Google Tasks? My prediction is that it won't last much longer; it seems to be a massively underdeveloped part of GMail/Google Calendar, and the task-list/to-do-list market is fiercely competitive.
05:11:25 < harrow> old Closure Library codebase, guy who wrote it quit, no tie-in with G+
Tasks LIVE, service non-profit non-social non-acquisition '"Google Tasks"' 641000 hits launched: 8 December 2008 http://gmailblog.blogspot.com/2008/12/new-in-labs-tasks.html
http://googlesystem.blogspot.com/2013/06/googles-caldav-and-carddav-apis-for.html CalDav API given a reprieve?
what happened to Waze? http://www.globes.co.il/serveen/globes/docview.asp?did=1000850934&fid=1725 apparently decided to keep Waze app running, so it enters the listings: http://www.wired.com/gadgetlab/2013/06/google-waze-acquisition/ http://googleblog.blogspot.com/2013/06/google-maps-and-waze-outsmarting.html
also check these: "Google acquired personalized Website gadget developer Labpixies for $25 million and interactive video-clip developer Quiksee for $10 million. Both acquisitions were in 2010."
google Chrome Frame https://en.wikipedia.org/wiki/Google_Chrome_Frame "chrome frame" 2,830,000 "google chrome frame" 28,200,000 plugin "According to Alex Russell - who came to Google specifically to start the Chrome Frame project - the plug-in will officially be retired early in 2014, and the company is beginning to warn the consumers and businesses who use the tool. "We've gotten to the point now where the trend lines indicate the need for Chrome Frame is going to expire early next year, so we're giving consumers and enterprises a lot of heads up that Chrome Frame is going to be going away," he says." http://www.wired.com/wiredenterprise/2013/06/chrome-frame-ends/ http://www.techrepublic.com/blog/google-in-the-enterprise/google-chrome-frame-is-leaving-the-picture/ http://blog.chromium.org/2013/06/retiring-chrome-frame.html January 2014
AdWhirl bought with AdMob, died 30 September 2013 http://vator.tv/news/2013-06-14-google-shutting-down-adwhirl-at-the-end-of-september for profit open-source https://code.google.com/p/adwhirl/ "google adwhirl" 101,000 results
"Google Mine", Pinterest competitor http://googlesystem.blogspot.com/2013/06/google-mine.html unreleased as of June 21, 2013
"Google Latitude" dead 9 August 2013; search 'Google Latitude' pulls up 1,150,000 hits on 10 July ANN: https://support.google.com/gmm/answer/3001634 https://plus.google.com/+jlapenna/posts/deBP7kj7rMi
Google Shopping for Suppliers 'Google "Shopping for Suppliers"' 67,300  11 July 2013; profit www.google.com/shopping/suppliers/‚Äé started 28 Jan 2013
Alfred,  dead 19 July 2013, for-profit acquisition (14 December 2011) http://techcrunch.com/2013/07/11/google-will-shut-down-alfred-the-local-recommendations-app-july-19/ http://www.ubergizmo.com/2013/07/google-to-shut-down-alfred-on-july-19th/ http://www.engadget.com/2011/12/14/google-buys-alfred-maker-clever-sense-brings-us-closer-to-perso/ "google alfred"  46,100   17 July 2013
Chromecast product for-profit non-social? non-FLOSS  341,000,000 hits 'google chromecast' 29 July 2013  http://www.n3rdabl3.co.uk/2013/07/google-announce-chromecast-a-new-way-to-share-on-your-tv/
Google Shopper: dead. 30 August 2013 http://www.fiercemobilecontent.com/story/google-shopper-price-comparison-app-shutting-down-aug-30/2013-07-29 "google shopper", 297000 hits as of 2 August 2013
Google Earth Tour Builder  http://googlesystem.blogspot.com/2013/08/google-earth-tour-builder.html https://tourbuilder.withgoogle.com/about/faq About 28,500 results (0.33 seconds) as of 1 September 2013
`google "tour builder"` About 24,800 results (19 September 2013) non-profit software FLOSS https://code.google.com/p/tour-builder/
Google Lime Scholarship 'google lime scholarship' About 1,030,000 results (0.23 seconds) (19 September 2013) started 15 April 2009 http://googleforstudents.blogspot.com/2011/04/2011-google-lime-scholars-announced.html http://googleforstudents.blogspot.com/2012/03/announcing-2012-google-lime-scholars.html  http://google.about.com/b/2009/04/15/google-lime-scholarship.htm http://www.limeconnect.com/opportunities/page/google-lime-scholarship-program http://www.google.com/edu/students/scholarships.html
Course Builder non-profit FLOSS https://code.google.com/p/course-builder/ program 11 September 2012 '"google course builder"' About 264,000 results  (19 September 2013) (sunsetted by Open edX)
Open edX 10 September 2013;  '"Open edX"' About 18,900 results (19 September 2013) http://googleresearch.blogspot.com/2013/09/we-are-joining-open-edx-platform.html FLOSS? program? service?
Bump start: 17 September 2013; acquisition: http://www.iol.co.za/scitech/technology/business/google-buys-bump-app-1.1578671 http://techcrunch.com/2013/09/16/bump-mobile-contact-sharing-app-acquired-by-google-will-stay-alive-for-now/ application non-profit non-FLOSS '"google bump"' About 116,000 results Dead: 31 January 2014 http://blog.bu.mp/post/71781606704/all-good-things http://www.hngn.com/articles/20977/20140102/google-shutting-down-bump-flock-apps-end-january.htm
Flock start: 17 September 2013; acquisition: http://www.iol.co.za/scitech/technology/business/google-buys-bump-app-1.1578671 http://techcrunch.com/2013/09/16/bump-mobile-contact-sharing-app-acquired-by-google-will-stay-alive-for-now/ application non-profit non-FLOSS 'google flock' About 34,200,000 results Dead: 31 January 2014
Google Web Designer: 30 September 2013 ' 1,790,000 results ' "google web designer"' https://news.ycombinator.com/item?id=6470426 for-profit program non-FLOSS (see https://support.google.com/webdesigner/answer/3413931?hl=en ) non-acquisition
Google Shopping Express: 25 September 2013 service for-profit non-acquisition non-floss http://www.siliconbeat.com/2013/09/25/public-launch-of-google-shopping-express-a-challenge-to-amazon-ebay/  '"google shopping express"' 1,480,000 results  (1 October 2013)
Flutter https://flutterapp.com/home/ http://arstechnica.com/gadgets/2013/10/hands-on-with-googles-latest-acquisition-flutter-a-webcam-gesture-app/ 2 October 2013  "Flutter users will be able to continue to use the app, and stay tuned for future updates." https://flutterapp.com/ closed-source non-profit? flutter,  18,800,000;  '"google flutter"'  3,980
Google Tag Manager alive released Oct 1, 2012; http://www.google.com/tagmanager/ for-profit service non-acquisition  262,000 results  'google "tag manager"' 10 October 2013
Google TV killed? '"google tv"' 7,860,000 17 Oct 2013 http://www.hngn.com/articles/14778/20131013/google-tv-shuts-down-three-years-shifting-focus-android.htm http://gigaom.com/2013/10/10/google-tv-rebranded-android-tv/
Google WiFi Passport http://googlesystem.blogspot.com/2013/10/google-wifi-passport.html launched 16 October 2013 http://arstechnica.com/civis/viewtopic.php?f=9&t=1221867 for-profit service non-floss  'google "wifi passport"' 26 October 2013 1,090 results
'google helpouts' 4 November 2013 http://blogs.wsj.com/digits/2013/11/04/google-to-launch-helpouts-on-monday/ service, for-profit non-acquisition '"google helpouts"' 140,000 results
Ingress 14 December 2013 http://venturebeat.com/2013/11/04/googles-niantic-labs-to-formally-launch-massive-ingress-augmented-reality-game-on-dec-14/ application closed-source for-profit (sponsorship)
Google Trader: dead 10 December 2013, google.com.ng; '"Google Trader"' 14 Nov 2013: 101k results http://niyitabiti.net/2013/11/google-trader-nigeria-shut-down-website/
"Offer Extensions"  22 Feb 2013; 14 Nov 2013: 'google "Offer Extensions"' 35.6k hits dead 1 November 2013 http://searchengineland.com/adwords-offer-extensions-get-shut-down-in-favor-of-google-offers-176566
Google Newsstand http://www.pcmag.com/article2/0,2817,2427413,00.asp http://play.google.com/about/newsstand/ Android app 20 Nov 2013 '"Google Newsstand"' 71500 for-profit closed-source
Google Partners 28 November 2013 '"Google Partners"' 289k 3 December 2013;  http://articles.timesofindia.indiatimes.com/2013-11-28/services-apps/44545708_1_google-india-google-partners-agencies https://www.google.com/partners/ http://www.zdnet.com/in/launch-of-google-partners-to-help-smes-in-india-7000023740/ organization, for-profit
Android Gallery, dead Dec 2013? http://www.androidcentral.com/stock-android-gallery-app-no-more-new-google-play-edition-devices http://www.techhive.com/article/2079353/google-is-killing-the-android-gallery-app-so-that-google-plus-may-live.html 'google "android gallery"' 13 Dec 2013 516,000 results
Open Gallery, 10 December 2013 https://twitter.com/digitalfay/status/410361346522611712 http://www.google.com/opengallery https://support.google.com/opengallery/ closed-source service non-profit 'google "open gallery"' 1,110,000 17 Dec 2013
LiquidFun 11 December 2013 FLOSS software non-profit http://google-opensource.blogspot.ca/2013/12/liquidfun-rigid-body-physics-library.html http://www.i-programmer.info/news/144-graphics-and-games/6711-googles-liquidfun-for-fluid-simulation-.html http://google.github.io/liquidfun/ https://github.com/google/liquidfun  'google liquidfun' 14 Dec 2013 174000 results
Google+ Auto Backup ios/android application closed-source non-profit start: 24 June 2013? http://www.vlogg.com/10149/how-to-disable-auto-backup-of-photos-in-android/ https://support.google.com/plus/answer/1647509?hl=en http://www.techrepublic.com/blog/google-in-the-enterprise/quick-tip-back-up-your-photos-to-google-plus/ http://googlesystem.blogspot.com/2013/12/google-auto-backup-for-desktop.html http://www.gplusexpertise.com/2013/12/google-photos-auto-backup-now-in-picasa.html 'Google+ Auto Backup' 8,830,000 results  28 Dec 2013
7 February 2014 Schemer http://googlesystem.blogspot.com/2013/12/schemer-to-be-discontinued.html to be shut down in 2014? confirmed: "All your schemes are available for download until February 7, 2014, after which all data will be permanently deleted." https://plus.google.com/+Schemer/posts/AhXkhvzRtek
Google Talk Windows client? http://www.tbreak.ae/news/google-killing-google-talk-windows-client "Connections from the Google Talk client to Google Talk will continue to operate for the first two months of 2014. Users may continue to see the deprecation warning in the client during this time. After this period has elapsed the client will be unable to connect to the Google Talk service."
Timely 2014-01-04 closed-source application Bitspin acquisition non-profit (since Google made the paid Android Timely app free) http://www.bitspin.ch/google http://www.pcworld.com/article/2084140/google-makes-timely-buy-of-swiss-app-maker.html "For new and existing users, Timely will continue to work as it always has." '"google timely"' 2014-01-08: 25,400 results
Nest: "Nest Learning Thermostat", "Protect"  13 January 2013 https://investor.google.com/releases/2014/0113.html http://techcrunch.com/2014/01/13/google-just-bought-connected-device-company-nest-for-3-2b-in-cash/ https://nest.com/blog/2014/01/13/nest-google-and-you/ acquisition thing for-profit "google nest"  'About 87,300,000 results (0.42 seconds) '
Google Currents: dead 19 February 2014 http://www.androidpolice.com/2014/02/19/google-currents-is-officially-dead-with-latest-update-transitions-users-to-play-newsstand-and-disappears-for-good/
Wildfire 31 July 2012 http://wildfireapp.blogspot.com/2012/07/wildfire-is-joining-google.html acquisition social for-profit Maintenance stops 1 September 2014 "A source tells us that Wildfire has begun contacting its clients ‚Äî which have included Cisco, Amazon, DQ, Gap, Jamba Juice, and McCann ‚Äî to tell them that their business would no longer be serviced after the end of 2015."; shutdown: http://wildfireapp.blogspot.com/2014/03/accelerating-our-wildfire-integration.html
Know Your Candidate http://www.google.co.in/elections/ed/in/districts http://www.businessinsider.in/Indian-Election-Fever-Has-Hit-Google-As-They-Launch-KnowYour-Candidate-Tool/articleshow/33444019.cms 8 April 2014 website nonprofit non-FLOSS 'google "know your candidate"' 25 April 2014: 2,190,000 results
TODO: Hangout, Photo
6 May 2014 http://www.digitaljournal.com/pr/1903697 for-profit acquisition service https://www.google.com/search?num=100&q=google%20ezanga 14 May 2014 '34,400 results'
Google Zavers http://www.google.com/get/zavers/ coupon business for-profit acquisition service launched January 2013 death leaked 2 June 2014 http://recode.net/2014/06/02/google-will-kill-off-its-digital-coupon-business-zavers/ '"google zavers"' 5 June 2014:  1,540 hits ('zavers': 51,500)
Orkut: dead 30 September 2014 http://en.blog.orkut.com/2014/06/tchau-orkut.html
QuickOffice dead http://www.slashgear.com/quickoffice-closing-down-replaced-by-google-drive-30335806/ http://googleappsupdates.blogspot.sg/2014/06/removal-of-quickoffice-from-google-play.html 'upcoming weeks'
Google Sync, dead August 1 2014 / 2014-08-01 free service https://support.google.com/a/answer/2716936 (already in original compilation)
followup window closed 4 June 2014: no more new entries unless they came into existence before then! too much work
-->
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<section id="source-code" class="level2">
<h2>Source code</h2>
<p>Run as <code>R --slave --file=google.r</code>:</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">7777</span>) <span class="co"># for reproducible numbers</span>

<span class="kw">library</span>(survival)
<span class="kw">library</span>(randomForest)
<span class="kw">library</span>(boot)
<span class="kw">library</span>(randomForestSRC)
<span class="kw">library</span>(prodlim) <span class="co"># for 'sindex' call</span>
<span class="kw">library</span>(rms)

<span class="co"># Generate Google corpus model for use in main analysis</span>
<span class="co"># Load the data, fit, and plot:</span>
index &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://www.gwern.net/docs/2013-google-index.csv&quot;</span>,
                   <span class="dt">colClasses=</span><span class="kw">c</span>(<span class="st">&quot;Date&quot;</span>,<span class="st">&quot;double&quot;</span>,<span class="st">&quot;character&quot;</span>))
<span class="co"># an exponential doesn't fit too badly:</span>
model1 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(Size) ~<span class="st"> </span>Date, <span class="dt">data=</span>index); <span class="kw">summary</span>(model1)
<span class="co"># plot logged size data and the fit:</span>
<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/google/www-index-model.png&quot;</span>, <span class="dt">width =</span> <span class="dv">3</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">plot</span>(<span class="kw">log</span>(index$Size) ~<span class="st"> </span>index$Date, <span class="dt">ylab=</span><span class="st">&quot;WWW index size&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Date&quot;</span>)
<span class="kw">abline</span>(model1)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())

<span class="co"># Begin actual data analysis</span>
google &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://www.gwern.net/docs/2013-google.csv&quot;</span>,
                    <span class="dt">colClasses=</span><span class="kw">c</span>(<span class="st">&quot;character&quot;</span>,<span class="st">&quot;logical&quot;</span>,<span class="st">&quot;Date&quot;</span>,<span class="st">&quot;Date&quot;</span>,<span class="st">&quot;double&quot;</span>,<span class="st">&quot;factor&quot;</span>,
                                 <span class="st">&quot;logical&quot;</span>,<span class="st">&quot;logical&quot;</span>,<span class="st">&quot;logical&quot;</span>,<span class="st">&quot;logical&quot;</span>, <span class="st">&quot;integer&quot;</span>,
                                 <span class="st">&quot;numeric&quot;</span>, <span class="st">&quot;numeric&quot;</span>, <span class="st">&quot;numeric&quot;</span>, <span class="st">&quot;logical&quot;</span>, <span class="st">&quot;numeric&quot;</span>,
                                 <span class="st">&quot;numeric&quot;</span>, <span class="st">&quot;numeric&quot;</span>, <span class="st">&quot;numeric&quot;</span>))
<span class="co"># google$Days &lt;- as.integer(google$Ended - google$Started)</span>
<span class="co"># derive all the Google index-variables</span>
## hits per day to the present
<span class="co"># google$AvgHits &lt;- google$Hits / as.integer(as.Date(&quot;2013-04-01&quot;) - google$Started)</span>
## divide total hits for each product by total estimated size of Google index when that product started
<span class="co"># google$DeflatedHits &lt;- log(google$Hits / exp(predict(model1, newdata=data.frame(Date = google$Started))))</span>
## Finally, let's combine the two strategies: deflate and then average.
<span class="co"># google$AvgDeflatedHits &lt;- log(google$AvgHits) / google$DeflatedHits</span>
<span class="co"># google$DeflatedHits &lt;- log(google$DeflatedHits)</span>

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Overview of data:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">summary</span>(google[-<span class="dv">1</span>]))

dead &lt;-<span class="st"> </span>google[google$Dead,]

<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/google/openedvslifespan.png&quot;</span>, <span class="dt">width =</span> <span class="fl">1.5</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">plot</span>(dead$Days ~<span class="st"> </span>dead$Ended, <span class="dt">xlab=</span><span class="st">&quot;Shutdown&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Total lifespan&quot;</span>)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())

<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/google/shutdownsbyyear.png&quot;</span>, <span class="dt">width =</span> <span class="fl">1.5</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">hist</span>(dead$Ended, <span class="dt">breaks=</span><span class="kw">seq.Date</span>(<span class="kw">as.Date</span>(<span class="st">&quot;2005-01-01&quot;</span>), <span class="kw">as.Date</span>(<span class="st">&quot;2014-01-01&quot;</span>), <span class="st">&quot;years&quot;</span>),
                   <span class="dt">main=</span><span class="st">&quot;shutdowns per year&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Year&quot;</span>)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())
<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/google/shutdownsbyyear-kernel.png&quot;</span>, <span class="dt">width =</span> <span class="dv">1</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(<span class="kw">as.numeric</span>(dead$Ended)), <span class="dt">main=</span><span class="st">&quot;Shutdown kernel density over time&quot;</span>)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())

<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/google/startsbyyear.png&quot;</span>, <span class="dt">width =</span> <span class="fl">1.5</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">hist</span>(google$Started, <span class="dt">breaks=</span><span class="kw">seq.Date</span>(<span class="kw">as.Date</span>(<span class="st">&quot;1997-01-01&quot;</span>), <span class="kw">as.Date</span>(<span class="st">&quot;2014-01-01&quot;</span>), <span class="st">&quot;years&quot;</span>),
     <span class="dt">xlab=</span><span class="st">&quot;total products released in year&quot;</span>)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())

<span class="co"># extract the month of each kill</span>
m =<span class="st"> </span><span class="kw">months</span>(dead$Ended)
<span class="co"># sort by chronological order, not alphabetical</span>
m_fac =<span class="st"> </span><span class="kw">factor</span>(m, <span class="dt">levels =</span> month.name)
<span class="co"># count by month</span>
months &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="kw">sort</span>(m_fac))
<span class="co"># shutdowns by month are imbalanced:</span>
<span class="kw">print</span>(<span class="kw">chisq.test</span>(months))
<span class="co"># and visibly so:</span>
<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/google/shutdownsbymonth.png&quot;</span>, <span class="dt">width =</span> <span class="fl">1.5</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">plot</span>(months)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">First logistic regression:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">summary</span>(<span class="kw">glm</span>(Dead ~<span class="st"> </span>Type +<span class="st"> </span>Profit +<span class="st"> </span>FLOSS +<span class="st"> </span>Acquisition +<span class="st"> </span>Social +<span class="st"> </span><span class="kw">log</span>(Hits),
                  <span class="dt">data=</span>google, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)))

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Second logistic regression, focusing on treacherous hit data:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">summary</span>(<span class="kw">glm</span>(Dead ~<span class="st"> </span><span class="kw">log</span>(Hits), <span class="dt">data=</span>google,<span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)))
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Total + average:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">summary</span>(<span class="kw">glm</span>(Dead ~<span class="st"> </span><span class="kw">log</span>(Hits) +<span class="st"> </span><span class="kw">log</span>(AvgHits), <span class="dt">data=</span>google,<span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)))
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Total, average, deflated:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">summary</span>(<span class="kw">glm</span>(Dead ~<span class="st"> </span><span class="kw">log</span>(Hits) +<span class="st"> </span><span class="kw">log</span>(AvgHits) +<span class="st"> </span>DeflatedHits, <span class="dt">data=</span>google,<span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)))
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">All:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">summary</span>(<span class="kw">glm</span>(Dead ~<span class="st"> </span><span class="kw">log</span>(Hits) +<span class="st"> </span><span class="kw">log</span>(AvgHits) +<span class="st"> </span>DeflatedHits +<span class="st"> </span>AvgDeflatedHits,
                  <span class="dt">data=</span>google, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)))
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Stepwise regression through possible logistic regressions involving the hit variables:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">summary</span>(<span class="kw">step</span>(<span class="kw">glm</span>(Dead ~<span class="st"> </span>Type +<span class="st"> </span>Profit +<span class="st"> </span>FLOSS +<span class="st"> </span>Acquisition +<span class="st"> </span>Social +
<span class="st">                        </span><span class="kw">log</span>(Hits) +<span class="st"> </span><span class="kw">log</span>(AvgHits) +<span class="st"> </span>DeflatedHits +<span class="st"> </span>AvgDeflatedHits,
                       <span class="dt">data=</span>google, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>))))

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Entering survival analysis section:</span><span class="ch">\n</span><span class="st">&quot;</span>)

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Unconditional Kaplan-Meier survival curve:</span><span class="ch">\n</span><span class="st">&quot;</span>)
surv &lt;-<span class="st"> </span><span class="kw">survfit</span>(<span class="kw">Surv</span>(google$Days, google$Dead, <span class="dt">type=</span><span class="st">&quot;right&quot;</span>) ~<span class="st"> </span><span class="dv">1</span>)
<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/google/overall-survivorship-curve.png&quot;</span>, <span class="dt">width =</span> <span class="fl">1.5</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">plot</span>(surv, <span class="dt">xlab=</span><span class="st">&quot;Days&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Survival Probability function with 95% CI&quot;</span>)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Cox:</span><span class="ch">\n</span><span class="st">&quot;</span>)
cmodel &lt;-<span class="st"> </span><span class="kw">coxph</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Acquisition +<span class="st"> </span>FLOSS +<span class="st"> </span>Profit +<span class="st"> </span>Social +<span class="st"> </span>Type +<span class="st"> </span>DeflatedHits,
                <span class="dt">data =</span> google)
<span class="kw">print</span>(<span class="kw">summary</span>(cmodel))
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Test proportional assumption:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">cox.zph</span>(cmodel))

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Primitive check for regime change (re-regress &amp; check):</span><span class="ch">\n</span><span class="st">&quot;</span>)
google$EarlyGoogle &lt;-<span class="st"> </span>(<span class="kw">as.POSIXlt</span>(google$Started)$year<span class="dv">+1900</span>) &lt;<span class="st"> </span><span class="dv">2005</span>
cmodel &lt;-<span class="st"> </span><span class="kw">coxph</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Acquisition +<span class="st"> </span>FLOSS +<span class="st"> </span>Profit +<span class="st"> </span>Social +<span class="st"> </span>Type +
<span class="st">                                   </span>DeflatedHits +<span class="st"> </span>EarlyGoogle,
                <span class="dt">data =</span> google)
<span class="kw">print</span>(<span class="kw">summary</span>(cmodel))
<span class="kw">print</span>(<span class="kw">cox.zph</span>(cmodel))

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Generating intuitive plots of social &amp; profit;</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Plot empirical survival split by profit...</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/google/profit-survivorship-curve.png&quot;</span>, <span class="dt">width =</span> <span class="fl">1.5</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
smodel1 &lt;-<span class="st"> </span><span class="kw">survfit</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Profit, <span class="dt">data =</span> google);
<span class="kw">plot</span>(smodel1, <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">xlab=</span><span class="st">&quot;Days&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Fraction surviving by Day&quot;</span>);
<span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Profit = no&quot;</span>, <span class="st">&quot;Profit = yes&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span> ,<span class="dv">2</span>), <span class="dt">inset=</span><span class="fl">0.02</span>)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Split by social...</span><span class="ch">\n</span><span class="st">&quot;</span>)
smodel2 &lt;-<span class="st"> </span><span class="kw">survfit</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Social, <span class="dt">data =</span> google)
<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/google/social-survivorship-curve.png&quot;</span>, <span class="dt">width =</span> <span class="fl">1.5</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">plot</span>(smodel2, <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="dt">xlab=</span><span class="st">&quot;Days&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Fraction surviving by Day&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Social = no&quot;</span>, <span class="st">&quot;Social = yes&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span> ,<span class="dv">2</span>), <span class="dt">inset=</span><span class="fl">0.02</span>)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Train some random forests for prediction:</span><span class="ch">\n</span><span class="st">&quot;</span>)
lmodel &lt;-<span class="st"> </span><span class="kw">glm</span>(Dead ~<span class="st"> </span>Acquisition +<span class="st"> </span>FLOSS +<span class="st"> </span>Profit +<span class="st"> </span>Social +<span class="st"> </span>Type +
<span class="st">                     </span>DeflatedHits +<span class="st"> </span>EarlyGoogle +<span class="st"> </span>Days,
                   <span class="dt">data=</span>google, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)
rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(<span class="kw">as.factor</span>(Dead) ~<span class="st"> </span>Acquisition +<span class="st"> </span>FLOSS +<span class="st"> </span>Profit +<span class="st"> </span>Social +
<span class="st">                                     </span>Type +<span class="st"> </span>DeflatedHits +<span class="st"> </span>EarlyGoogle,
                   <span class="dt">importance=</span><span class="ot">TRUE</span>, <span class="dt">data=</span>google)
<span class="kw">print</span>(rf)
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Variables by importance for forests:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">importance</span>(rf))

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Base-rate predictor of ~65% products alive:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">sum</span>(<span class="ot">FALSE</span> ==<span class="st"> </span>google$Dead) /<span class="st"> </span><span class="kw">nrow</span>(google))
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Logistic regression's correct predictions:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">sum</span>((<span class="kw">exp</span>(<span class="kw">predict</span>(lmodel))&gt;<span class="dv">1</span>) ==<span class="st"> </span>google$Dead) /<span class="st"> </span><span class="kw">nrow</span>(google))
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Random forest's correct predictions:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">sum</span>((<span class="kw">as.logical</span>(<span class="kw">predict</span>(rf))) ==<span class="st"> </span>google$Dead) /<span class="st"> </span><span class="kw">nrow</span>(google))

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Begin bootstrap test of predictive accuracy...</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Get a subsample, train logistic regression on it, test accuracy on original Google data:</span><span class="ch">\n</span><span class="st">&quot;</span>)
logisticPredictionAccuracy &lt;-<span class="st"> </span>function(gb, indices) {
  g &lt;-<span class="st"> </span>gb[indices,] <span class="co"># allows boot to select subsample</span>
  <span class="co"># train new regression model on subsample</span>
  lmodel &lt;-<span class="st"> </span><span class="kw">glm</span>(Dead ~<span class="st"> </span>Acquisition +<span class="st"> </span>FLOSS +<span class="st"> </span>Profit +<span class="st"> </span>Social +<span class="st"> </span>Type +
<span class="st">                       </span>DeflatedHits +<span class="st"> </span>EarlyGoogle +<span class="st"> </span>Days,
                   <span class="dt">data=</span>g, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)
  <span class="kw">return</span>(<span class="kw">sum</span>((<span class="kw">exp</span>(<span class="kw">predict</span>(lmodel, <span class="dt">newdata=</span>google))&gt;<span class="dv">1</span>) ==<span class="st"> </span>google$Dead) /<span class="st"> </span><span class="kw">nrow</span>(google))
}
lbs &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>google, <span class="dt">statistic=</span>logisticPredictionAccuracy, <span class="dt">R=</span><span class="dv">20000</span>, <span class="dt">parallel=</span><span class="st">&quot;multicore&quot;</span>, <span class="dt">ncpus=</span><span class="dv">4</span>)
<span class="kw">print</span>(<span class="kw">boot.ci</span>(lbs, <span class="dt">type=</span><span class="st">&quot;norm&quot;</span>))

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Ditto for random forests:</span><span class="ch">\n</span><span class="st">&quot;</span>)
randomforestPredictionAccuracy &lt;-<span class="st"> </span>function(gb, indices) {
  g &lt;-<span class="st"> </span>gb[indices,]
  rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(<span class="kw">as.factor</span>(Dead) ~<span class="st"> </span>Acquisition +<span class="st"> </span>FLOSS +<span class="st"> </span>Profit +<span class="st"> </span>Social +<span class="st"> </span>Type +
<span class="st">                       </span>DeflatedHits +<span class="st"> </span>EarlyGoogle +<span class="st"> </span>Days,
                   <span class="dt">data=</span>g)
  <span class="kw">return</span>(<span class="kw">sum</span>((<span class="kw">as.logical</span>(<span class="kw">predict</span>(rf))) ==<span class="st"> </span>google$Dead) /<span class="st"> </span><span class="kw">nrow</span>(google))
}
rfbs &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>google, <span class="dt">statistic=</span>randomforestPredictionAccuracy, <span class="dt">R=</span><span class="dv">20000</span>, <span class="dt">parallel=</span><span class="st">&quot;multicore&quot;</span>, <span class="dt">ncpus=</span><span class="dv">4</span>)
<span class="kw">print</span>(<span class="kw">boot.ci</span>(rfbs, <span class="dt">type=</span><span class="st">&quot;norm&quot;</span>))

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Fancier comparison: random survival forests and full Cox model with bootstrap</span><span class="ch">\n</span><span class="st">&quot;</span>)
rsf &lt;-<span class="st"> </span><span class="kw">rfsrc</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Acquisition +<span class="st"> </span>FLOSS +<span class="st"> </span>Profit +<span class="st"> </span>Social +<span class="st"> </span>Type +<span class="st"> </span>DeflatedHits +<span class="st"> </span>EarlyGoogle,
             <span class="dt">data=</span>google, <span class="dt">nsplit=</span><span class="dv">1</span>)
<span class="kw">print</span>(rsf)

<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/google/rsf-importance.png&quot;</span>, <span class="dt">width =</span> <span class="fl">1.5</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">plot</span>(rsf)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())

<span class="co"># calculate cumulative hazard function; adapted from Mogensen et al 2012</span>
predictSurvProb.rsf &lt;-<span class="st"> </span>function (object, newdata, times, ...) {
    N &lt;-<span class="st"> </span><span class="kw">NROW</span>(newdata)
    <span class="co"># class(object) &lt;- c(&quot;rsf&quot;, &quot;grow&quot;)</span>
    S &lt;-<span class="st"> </span><span class="kw">exp</span>(-<span class="kw">predict.rfsrc</span>(object, <span class="dt">test =</span> newdata)$chf)
    if(N ==<span class="st"> </span><span class="dv">1</span>) S &lt;-<span class="st"> </span><span class="kw">matrix</span>(S, <span class="dt">nrow =</span> <span class="dv">1</span>)
    Time &lt;-<span class="st"> </span>object$time.interest
    p &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, S)[, <span class="dv">1</span> +<span class="st"> </span><span class="kw">sindex</span>(Time, times),drop =<span class="st"> </span><span class="ot">FALSE</span>]
    if(<span class="kw">NROW</span>(p) !=<span class="st"> </span><span class="kw">NROW</span>(newdata) ||<span class="st"> </span><span class="kw">NCOL</span>(p) !=<span class="st"> </span><span class="kw">length</span>(times))
     <span class="kw">stop</span>(<span class="st">&quot;Prediction failed&quot;</span>)
    p
}
totals &lt;-<span class="st"> </span><span class="kw">as.integer</span>(<span class="kw">as.Date</span>(<span class="st">&quot;2013-04-01&quot;</span>) -<span class="st"> </span>google$Started)
randomSurvivalPredictionAccuracy &lt;-<span class="st"> </span>function(gb, indices) {
    g &lt;-<span class="st"> </span>gb[indices,]
    rsfB &lt;-<span class="st"> </span><span class="kw">rfsrc</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Acquisition +<span class="st"> </span>FLOSS +<span class="st"> </span>Profit +<span class="st"> </span>Social +<span class="st"> </span>Type +
<span class="st">                                     </span>DeflatedHits +<span class="st"> </span>EarlyGoogle,
                 <span class="dt">data=</span>g, <span class="dt">nsplit=</span><span class="dv">1</span>)

    predictionMatrix &lt;-<span class="st"> </span><span class="kw">predictSurvProb.rsf</span>(rsfB, google, totals)
    predictions &lt;-<span class="st"> </span><span class="ot">NULL</span>; for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(google)) { predictions[i] &lt;-<span class="st"> </span>predictionMatrix[i,i] }

    <span class="kw">return</span>(<span class="kw">sum</span>((predictions&lt;<span class="fl">0.50</span>) ==<span class="st"> </span>google$Dead) /<span class="st"> </span><span class="kw">nrow</span>(google))
}
<span class="co"># accuracy on full Google dataset</span>
<span class="kw">print</span>(<span class="kw">randomSurvivalPredictionAccuracy</span>(google, <span class="dv">1</span>:<span class="kw">nrow</span>(google)))
<span class="co"># check this high accuracy using bootstrap</span>
rsfBs &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>google, <span class="dt">statistic=</span>randomSurvivalPredictionAccuracy, <span class="dt">R=</span><span class="dv">200</span>, <span class="dt">parallel=</span><span class="st">&quot;multicore&quot;</span>, <span class="dt">ncpus=</span><span class="dv">4</span>)
<span class="kw">print</span>(rsfBs)
<span class="kw">print</span>(<span class="kw">boot.ci</span>(rsfBs, <span class="dt">type=</span><span class="st">&quot;perc&quot;</span>))

coxProbability &lt;-<span class="st"> </span>function(cm, d, t) {
    x &lt;-<span class="st"> </span><span class="kw">survfit</span>(cm, <span class="dt">newdata=</span>d)
    p &lt;-<span class="st"> </span>x$surv[<span class="kw">Position</span>(function(a) a&gt;t, x$time)]
    if (<span class="kw">is.null</span>(p)) { <span class="kw">coxProbability</span>(d, (t<span class="dv">-1</span>)) } else {if (<span class="kw">is.na</span>(p)) p &lt;-<span class="st"> </span><span class="dv">0</span>}
    p
    }
randomCoxPredictionAccuracy &lt;-<span class="st"> </span>function(gb, indices) {
    g &lt;-<span class="st"> </span>gb[indices,]
    cmodel &lt;-<span class="st"> </span>cmodel &lt;-<span class="st"> </span><span class="kw">coxph</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Acquisition +<span class="st"> </span>FLOSS +<span class="st"> </span>Profit +<span class="st"> </span>Social +<span class="st"> </span>Type +<span class="st"> </span>DeflatedHits,
                <span class="dt">data =</span> g)

    predictions &lt;-<span class="st"> </span><span class="ot">NULL</span>;
    for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(google)) { predictions[i] &lt;-<span class="st"> </span><span class="kw">coxProbability</span>(cmodel, google[i,], totals[i]) }

    <span class="kw">return</span>(<span class="kw">sum</span>((predictions&lt;<span class="fl">0.50</span>) ==<span class="st"> </span>google$Dead) /<span class="st"> </span><span class="kw">nrow</span>(google))
    }
<span class="kw">print</span>(<span class="kw">randomCoxPredictionAccuracy</span>(google, <span class="dv">1</span>:<span class="kw">nrow</span>(google)))
coxBs &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>google, <span class="dt">statistic=</span>randomCoxPredictionAccuracy, <span class="dt">R=</span><span class="dv">200</span>, <span class="dt">parallel=</span><span class="st">&quot;multicore&quot;</span>, <span class="dt">ncpus=</span><span class="dv">4</span>)
<span class="kw">print</span>(coxBs)
<span class="kw">print</span>(<span class="kw">boot.ci</span>(coxBs, <span class="dt">type=</span><span class="st">&quot;perc&quot;</span>))

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Ranking products by Cox risk ratio...</span><span class="ch">\n</span><span class="st">&quot;</span>)
google$RiskRatio &lt;-<span class="st"> </span><span class="kw">predict</span>(cmodel, <span class="dt">type=</span><span class="st">&quot;risk&quot;</span>)
alive &lt;-<span class="st"> </span>google[!google$Dead,]

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Extract the 10 living products with highest estimated relative risks:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">head</span>(alive[<span class="kw">order</span>(alive$RiskRatio, <span class="dt">decreasing=</span><span class="ot">TRUE</span>),], <span class="dt">n=</span><span class="dv">10</span>)$Product)

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Extract the 10 living products with lowest estimated relative risk:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">head</span>(alive[<span class="kw">order</span>(alive$RiskRatio, <span class="dt">decreasing=</span><span class="ot">FALSE</span>),], <span class="dt">n=</span><span class="dv">10</span>)$Product)

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Begin calculating specific numerical predictions about remaining lifespans..</span><span class="ch">\n</span><span class="st">&quot;</span>)
cpmodel &lt;-<span class="st"> </span><span class="kw">cph</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Acquisition +<span class="st"> </span>FLOSS +<span class="st"> </span>Profit +<span class="st"> </span>Social +<span class="st"> </span>Type +
<span class="st">                                  </span>DeflatedHits +<span class="st"> </span>EarlyGoogle,
               <span class="dt">data =</span> google, <span class="dt">x=</span><span class="ot">TRUE</span>, <span class="dt">y=</span><span class="ot">TRUE</span>, <span class="dt">surv=</span><span class="ot">TRUE</span>)
predictees &lt;-<span class="st"> </span><span class="kw">subset</span>(google, Product %in%<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Alerts&quot;</span>,<span class="st">&quot;Blogger&quot;</span>,<span class="st">&quot;FeedBurner&quot;</span>,<span class="st">&quot;Scholar&quot;</span>,
                                            <span class="st">&quot;Book Search&quot;</span>,<span class="st">&quot;Voice&quot;</span>,<span class="st">&quot;Gmail&quot;</span>,<span class="st">&quot;Analytics&quot;</span>,
                                            <span class="st">&quot;AdSense&quot;</span>,<span class="st">&quot;Calendar&quot;</span>,<span class="st">&quot;Alerts&quot;</span>,<span class="st">&quot;Google+&quot;</span>,<span class="st">&quot;Docs&quot;</span>,
                                            <span class="st">&quot;Search&quot;</span>, <span class="st">&quot;Project Glass&quot;</span>, <span class="st">&quot;Chrome&quot;</span>, <span class="st">&quot;Translate&quot;</span>))
conditionalProbability &lt;-<span class="st"> </span>function (d, followupUnits) {
    chances &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="ot">NA</span>, <span class="kw">nrow</span>(d)) <span class="co"># stash results</span>

    for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(d)) {

        <span class="co"># extract chance of particular subject surviving as long as it has:</span>
        beginProb &lt;-<span class="st"> </span><span class="kw">survest.cph</span>(cpmodel, d[i,], <span class="dt">times=</span>(d[i,]$Days))$surv
        if (<span class="kw">length</span>(beginProb)==<span class="dv">0</span>) { beginProb &lt;-<span class="st"> </span><span class="dv">1</span> } <span class="co"># set to a default</span>

        tmpFollowup &lt;-<span class="st"> </span>followupUnits <span class="co"># reset in each for loop</span>
        while (<span class="ot">TRUE</span>) {
            <span class="co"># extract chance of subject surviving as long as it has + an arbitrary additional time-units</span>
            endProb &lt;-<span class="st"> </span><span class="kw">survest.cph</span>(cpmodel, d[i,], <span class="dt">times=</span>(d[i,]$Days +<span class="st"> </span>tmpFollowup))$surv
            <span class="co"># survival curve may not reach that far! 'survexp returns 'numeric(0)' if it doesn't;</span>
            <span class="co"># so we shrink down 1 day and try again until 'survexp' *does* return a usable answer</span>
            if (<span class="kw">length</span>(endProb)==<span class="dv">0</span>) { tmpFollowup &lt;-<span class="st"> </span>tmpFollowup -<span class="st"> </span><span class="dv">1</span>} else { break }
        }

        <span class="co"># if 50% of all subjects survive to time t, and 20% of all survive to time t+100, say, what chance</span>
        <span class="co"># does a survivor - at exactly time t - have of making it to time t+100? 40%: 0.20 / 0.50 = 0.40</span>
        chances[i] &lt;-<span class="st"> </span>endProb /<span class="st"> </span>beginProb
    }
    <span class="kw">return</span>(chances)
}
## the risks and survival estimate have been stashed in the original CSV to save computation
<span class="co"># google$RelativeRisk &lt;- predict(cmodel, newdata=google, type=&quot;risk&quot;)</span>
<span class="co"># google$LinearPredictor &lt;- predict(cmodel, newdata=google, type=&quot;lp&quot;)</span>
<span class="co"># google$ExpectedEvents &lt;- predict(cmodel, newdata=google, type=&quot;expected&quot;)</span>
<span class="co"># google$FiveYearSurvival &lt;- conditionalProbability(google, 5*365.25)</span>

<span class="co"># graphs survival curves for each of the 15</span>
<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/google/15-predicted-survivorship-curves.png&quot;</span>, <span class="dt">width =</span> <span class="fl">1.5</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">plot</span>(<span class="kw">survfit</span>(cmodel, <span class="dt">newdata=</span>predictees),
     <span class="dt">xlab =</span> <span class="st">&quot;time&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Survival&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Survival curves for 15 selected Google products&quot;</span>)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Predictions for the 15 and also their relative risks:</span><span class="ch">\n</span><span class="st">&quot;</span>)
ps &lt;-<span class="st"> </span><span class="kw">conditionalProbability</span>(predictees, <span class="dv">5</span>*<span class="fl">365.25</span>)
<span class="kw">print</span>(<span class="kw">data.frame</span>(predictees$Product, ps*<span class="dv">100</span>))
<span class="kw">print</span>(<span class="kw">round</span>(<span class="kw">predict</span>(cmodel, <span class="dt">newdata=</span>predictees, <span class="dt">type=</span><span class="st">&quot;risk&quot;</span>), <span class="dt">digits=</span><span class="dv">2</span>))

<span class="co"># Analysis done</span>

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Optimizing the generated graphs by cropping whitespace &amp; losslessly compressing them...</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">system</span>(<span class="kw">paste</span>(<span class="st">'cd ~/wiki/images/google/ &amp;&amp;'</span>,
             <span class="st">'for f in *.png; do convert &quot;$f&quot; -crop'</span>,
             <span class="st">'`nice convert &quot;$f&quot; -virtual-pixel edge -blur 0x5 -fuzz 10% -trim -format'</span>,
             <span class="st">'</span><span class="ch">\'</span><span class="st">%wx%h%O</span><span class="ch">\'</span><span class="st"> info:` +repage &quot;$f&quot;; done'</span>))
<span class="kw">system</span>(<span class="st">&quot;optipng -o9 -fix ~/wiki/images/google/*.png&quot;</span>, <span class="dt">ignore.stdout =</span> <span class="ot">TRUE</span>)</code></pre>
</section>
<section id="leakage" class="level2">
<h2>Leakage</h2>
<p>While the hit-counts are a possible form of leakage, I accidentally caused a clear case of leakage while seeing how random forests would do in predicting shutdowns.</p>
<p>One way to get data leakage is if we include the end-date; early on in my analysis I removed the <code>Dead</code> variable but it didn‚Äôt occur to me to remove the <code>Ended</code> date factor. The random forest would predict correctly <em>every single shutdown</em> except for 8, for an error-rate of 2%. How did it turn in this nearly-omniscient set of predictions and why did it get those 8 wrong? Because the 8 products are correctly marked in the original dataset as ‚Äúdead‚Äù because their shutdown had been announced by Google, but had been scheduled by Google to die <em>after</em> the day I was running the code. So it turned out that the random forests were just emitting ‚Äòdead‚Äô for ‚Äòanything with an end date before 2013-04-04‚Äô, and alive for everything thereafter!</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">library</span>(randomForest)
rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(<span class="kw">as.factor</span>(Dead) ~<span class="st"> </span>., <span class="dt">data=</span>google[-<span class="dv">1</span>])
google[rf$predicted !=<span class="st"> </span>google$Dead,]
                     Product Dead    Started      Ended     Hits    Type Profit FLOSS Acquisition
<span class="dv">24</span> Gmail Exchange ActiveSync <span class="ot">TRUE</span> <span class="dv">2009-02-09</span> <span class="dv">2013-07-01</span>   <span class="dv">637000</span> service  <span class="ot">FALSE</span> <span class="ot">FALSE</span>       <span class="ot">FALSE</span>
<span class="dv">30</span>  CalDAV support for Gmail <span class="ot">TRUE</span> <span class="dv">2008-07-28</span> <span class="dv">2013-09-16</span>   <span class="dv">245000</span> service  <span class="ot">FALSE</span> <span class="ot">FALSE</span>       <span class="ot">FALSE</span>
<span class="dv">37</span>                    Reader <span class="ot">TRUE</span> <span class="dv">2005-10-07</span> <span class="dv">2013-07-01</span> <span class="dv">79100000</span> service  <span class="ot">FALSE</span> <span class="ot">FALSE</span>       <span class="ot">FALSE</span>
<span class="dv">38</span>               Reader Play <span class="ot">TRUE</span> <span class="dv">2010-03-10</span> <span class="dv">2013-07-01</span>    <span class="dv">43500</span> service  <span class="ot">FALSE</span> <span class="ot">FALSE</span>       <span class="ot">FALSE</span>
<span class="dv">39</span>                   iGoogle <span class="ot">TRUE</span> <span class="dv">2005-05-01</span> <span class="dv">2013-11-01</span> <span class="dv">33600000</span> service   <span class="ot">TRUE</span> <span class="ot">FALSE</span>       <span class="ot">FALSE</span>
<span class="dv">74</span>            Building Maker <span class="ot">TRUE</span> <span class="dv">2009-10-13</span> <span class="dv">2013-06-01</span>  <span class="dv">1730000</span> service  <span class="ot">FALSE</span> <span class="ot">FALSE</span>       <span class="ot">FALSE</span>
<span class="dv">75</span>             Cloud Connect <span class="ot">TRUE</span> <span class="dv">2011-02-24</span> <span class="dv">2013-04-30</span>   <span class="dv">530000</span> program  <span class="ot">FALSE</span> <span class="ot">FALSE</span>       <span class="ot">FALSE</span>
<span class="dv">77</span>   Search API for Shopping <span class="ot">TRUE</span> <span class="dv">2011-02-11</span> <span class="dv">2013-09-16</span>   <span class="dv">217000</span> service   <span class="ot">TRUE</span> <span class="ot">FALSE</span>       <span class="ot">FALSE</span>
   Social Days  AvgHits DeflatedHits AvgDeflatedHits
<span class="dv">24</span>  <span class="ot">FALSE</span> <span class="dv">1603</span>   <span class="fl">419.35</span>    <span class="fl">9.308e-06</span>         -<span class="fl">0.5213</span>
<span class="dv">30</span>  <span class="ot">FALSE</span> <span class="dv">1876</span>   <span class="fl">142.86</span>    <span class="fl">4.823e-06</span>         -<span class="fl">0.4053</span>
<span class="dv">37</span>   <span class="ot">TRUE</span> <span class="dv">2824</span> <span class="fl">28868.61</span>    <span class="fl">7.396e-03</span>         -<span class="fl">2.0931</span>
<span class="dv">38</span>  <span class="ot">FALSE</span> <span class="dv">1209</span>    <span class="fl">38.67</span>    <span class="fl">3.492e-07</span>         -<span class="fl">0.2458</span>
<span class="dv">39</span>   <span class="ot">TRUE</span> <span class="dv">3106</span> <span class="fl">11590.20</span>    <span class="fl">4.001e-03</span>         -<span class="fl">1.6949</span>
<span class="dv">74</span>  <span class="ot">FALSE</span> <span class="dv">1327</span>  <span class="fl">1358.99</span>    <span class="fl">1.739e-05</span>         -<span class="fl">0.6583</span>
<span class="dv">75</span>  <span class="ot">FALSE</span>  <span class="dv">796</span>   <span class="fl">684.75</span>    <span class="fl">2.496e-06</span>         -<span class="fl">0.5061</span>
<span class="dv">77</span>  <span class="ot">FALSE</span>  <span class="dv">948</span>   <span class="fl">275.73</span>    <span class="fl">1.042e-06</span>         -<span class="fl">0.4080</span>
rf
...
    Type of random forest:<span class="st"> </span>classification
                     Number of trees:<span class="st"> </span><span class="dv">500</span>
No. of variables tried at each split:<span class="st"> </span><span class="dv">3</span>

        OOB estimate of  error rate:<span class="st"> </span><span class="fl">2.29</span>%
Confusion matrix:
<span class="st">      </span><span class="ot">FALSE</span> <span class="ot">TRUE</span> class.error
<span class="ot">FALSE</span>   <span class="dv">226</span>    <span class="dv">0</span>     <span class="fl">0.00000</span>
<span class="ot">TRUE</span>      <span class="dv">8</span>  <span class="dv">115</span>     <span class="fl">0.06504</span></code></pre>
 
</section>
</section>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><p>One sobering example I mention in my <a href="Archiving%20URLs">link rot page</a>: <a href="http://arxiv.org/pdf/1209.3026v1.pdf" title="'Losing My Revolution: How Many Resources Shared on Social Media Have Been Lost?', SalahEldeen &amp; Nelson 2012">11% of Arab Spring-related tweets</a> were gone within a year. I do not know what the full dimension of the Reader RSS archive loss will be.<a href="#fnref1">‚Ü©</a></p></li>
<li id="fn2"><p>Google Reader affords examples of this lack of transparency on a key issue - Google‚Äôs willingness to support Reader (extremely relevant to users, and even more so to the third-party web services and applications which relied on Reader to function); from BuzzFeed‚Äôs <a href="http://www.buzzfeed.com/robf4/googles-lost-social-network">‚ÄúGoogle‚Äôs Lost Social Network: How Google accidentally built a truly beloved social network, only to steamroll it with Google+. The sad, surprising story of Google Reader‚Äù</a>:</p>
<blockquote>
<p>The difficulty was that Reader users, while hyper-engaged with the product, never snowballed into the tens or hundreds of millions. Brian Shih became the product manager for Reader in the fall of 2008. ‚ÄúIf Reader were its own startup, it‚Äôs the kind of company that Google would have bought. Because we were at Google, when you stack it up against some of these products, it‚Äôs tiny and isn‚Äôt worth the investment‚Äù, he said. At one point, Shih remembers, engineers were pulled off Reader to work on OpenSocial, a ‚Äúhalf-baked‚Äù development platform that never amounted to much. ‚ÄúThere was always a political fight internally on keeping people staffed on this little project‚Äù, he recalled. Someone hung a sign in the Reader offices that said ‚ÄúDAYS SINCE LAST THREAT OF CANCELLATION.‚Äù The number was almost always zero. At the same time, user growth - while small next to Gmail‚Äôs hundreds of millions - more than doubled under Shih‚Äôs tenure. But the ‚Äúsenior types‚Äù, as Bilotta remembers, ‚Äúwould look at absolute user numbers. They wouldn‚Äôt look at market saturation. So Reader was constantly on the chopping block.‚Äù</p>
<p>So when news spread internally of Reader‚Äôs gelding, it was like Hemingway‚Äôs line about going broke: ‚ÄúTwo ways. Gradually, then suddenly.‚Äù Shih found out in the spring that Reader‚Äôs internal sharing functions - the asymmetrical following model, endemic commenting and liking, and its advanced privacy settings - would be superseded by the forthcoming Google+ model. Of course, he was forbidden from breathing a word to users.</p>
</blockquote>
<p><a href="http://www.marco.org/2013/07/03/lockdown" title="Lockdown">Marco Arment</a> says ‚ÄúI‚Äôve heard from multiple sources that it effectively had a staff of zero for years‚Äù.<a href="#fnref2">‚Ü©</a></p></li>
<li id="fn3"><p>Shih further <a href="https://www.quora.com/Google-Reader-Shut-Down-March-2013/Why-is-Google-killing-Google-Reader">writes on Quora</a>:</p>
<blockquote>
<p>Let‚Äôs be clear that this has nothing to do with revenue vs operating costs. Reader never made money directly (though you could maybe attribute some of Feedburner and AdSense for Feeds usage to it), and it wasn‚Äôt the goal of the product. Reader has been fighting for approval/survival at Google since long before I was a PM for the product. I‚Äôm pretty sure Reader was threatened with de-staffing at least three times before it actually happened. It was often for some reason related to social:</p>
<ul>
<li>2008 - let‚Äôs pull the team off to build OpenSocial</li>
<li>2009 - let‚Äôs pull the team off to build Buzz</li>
<li>2010 - let‚Äôs pull the team off to build Google+</li>
</ul>
<p>It turns out they decided to kill it anyway in 2010, even though most of the engineers opted against joining G+. Ironically, I think the reason Google always wanted to pull the Reader team off to build these other social products was that the Reader team actually understood social (and tried a lot of experiments over the years that informed the larger social features at the company) [See Reader‚Äôs friends implementations v1, v2, and v3, comments, privacy controls, and sharing features. Actually wait, you can‚Äôt see those anymore, since they were all ripped out.]. Reader‚Äôs social features also evolved very organically in response to users, instead of being designed top-down like some of Google‚Äôs other efforts [Rob Fishman‚Äôs Buzzfeed article has good coverage of this: <a href="http://www.buzzfeed.com/robf4/googles-lost-social-network">Google‚Äôs Lost Social Network</a>]. I suspect that it survived for some time after being put into maintenance because they believed it could still be a useful source of content into G+. Reader users were always voracious consumers of content, and many of them filtered and shared a great deal of it. But after switching the sharing features over to G+ (the so called ‚Äúshare-pocalypse‚Äù) along with the redesigned UI, my guess is that usage just started to fall - particularly around sharing. I know that my sharing basically stopped completely once the redesign happened [<a href="http://www.brianshih.com/post/30194495552/reader-redesign-terrible-decision-or-worst-decision">Reader redesign: Terrible decision, or worst decision?</a> I was a lot angrier then than I am now ‚Äì now I‚Äôm just sad.]. Though Google did ultimately fix a lot of the UI issues, the sharing (and therefore content going into G+) would never recover. So with dwindling usefulness to G+, (likely) dwindling or flattening usage due to being in maintenance, and Google‚Äôs big drive to focus in the last couple of years, what choice was there but to kill the product?</p>
</blockquote>
<a href="#fnref3">‚Ü©</a></li>
<li id="fn4"><p>The sign story is confirmed by another Googler; <a href="http://gigaom.com/2013/03/13/chris-wetherll-google-reader/">‚ÄúGoogle Reader lived on borrowed time: creator Chris Wetherell reflects‚Äù</a>:</p>
<blockquote>
<p>‚ÄúWhen they replaced sharing with +1 on Google Reader, it was clear that this day was going to come‚Äù, he said. Wetherell, 43, is amazed that Reader has lasted this long. Even before the project saw the light of the day, Google executives were unsure about the service and it was through sheer perseverance that it squeaked out into the market. At one point, the management team threatened to cancel the project even before it saw the light of the day, if there was a delay. ‚ÄúWe had a sign that said, ‚Äò<em>days since cancellation</em>‚Äô and it was there from the very beginning‚Äù, added a very sanguine Wetherell. My translation: Google never really believed in the project. Google Reader started in 2005 at what was really the golden age of RSS, blogging systems and a new content ecosystem. The big kahuna at that time was <a href="http://en.wikipedia.org/wiki/Bloglines" title="Wikipedia: Bloglines">Bloglines</a> (acquired by <a href="http://en.wikipedia.org/wiki/Ask.com" title="Wikipedia: Ask.com">Ask.com</a>) and Google Reader was an upstart.</p>
</blockquote>
<a href="#fnref4">‚Ü©</a></li>
<li id="fn5"><p>The official PR release stated that too little usage was the reason Reader was being abandoned. Whether this is the genuine reason has been questioned by third parties, who observe that Reader seems to <a href="http://www.buzzfeed.com/jwherrman/google-reader-still-sends-far-more-traffic-than-google">drive far more traffic</a> than another service which Google has yet to ax, Google+; that one app had <a href="http://allthingsd.com/20130324/another-reason-google-reader-died-increased-concern-about-privacy-and-compliance/">&gt;2m users who also had Reader accounts</a>; that just one alternative to Reader (Feedly) had in excess of <a href="http://blog.feedly.com/2013/04/02/announcing-the-new-feedly-mobile-and-welcoming-3-million-reader-refugees/">3 million signups post-announcement</a> (reportedly, up to <a href="http://www.nytimes.com/2013/05/09/technology/personaltech/three-ways-feedly-outdoes-the-vanishing-google-reader.html?pagewanted=all" title="Google's Aggregator Gives Way to an Heir">4 million</a>); and the largest of several petitions to Google reached <a href="https://www.change.org/petitions/google-keep-google-reader-running">148k</a> signatures (less, though, than the <a href="http://web.archive.org/web/20130122102151/http://www.appbrain.com/app/google-reader/com.google.android.apps.reader">&gt;1m downloads</a> of the Android client). Given that few users will sign up at Feedly specifically, sign a petition, visit the BuzzFeed network, or use the apps in question, it seems likely that Reader had closer to 20m users than 2m users when its closure was announced. An unknown Google engineer has been quoted as <a href="http://www.quora.com/How-many-users-does-Google-Reader-have/answer/Andreas-Pizsa">saying in 2010</a> Reader had ‚Äútens of millions active monthly users‚Äù. <a href="http://www.forbes.com/sites/alexkantrowitz/2013/07/01/google-reader-founder-i-never-would-have-founded-reader-inside-todays-google/">Xoogler Jenna Bilotta</a> (left Google <a href="http://www.thepinkestblack.com/2011/11/all-good-things.html">November 2011</a>) said</p>
<blockquote>
<p>‚ÄúI think the reason why people are freaking out about Reader is because that Reader did stick,‚Äù she said, noting the widespread surprise that Google would shut down such a beloved product. ‚ÄúThe numbers, at least until I left, were still going up.‚Äù</p>
</blockquote>
<p>The most popular feed on Google Reader in March 2013 had <a href="http://googlesystem.blogspot.com/2013/03/google-reader-data-points.html">24.3m subscribers</a> (some <a href="http://googlesystem.blogspot.com/2013/03/google-reader-data-points.html?google_comment_id=z12wjrqpao3qhjgl223zzv1h4s2hx5na504">pixel-counting</a> of <a href="http://googlereader.blogspot.com/2010/09/welcome-and-look-back.html">an official user-count graph</a> &amp; inference from a <a href="http://blogoscoped.com/forum/108194.html">leaked video</a> suggests Reader in total may‚Äôve had &gt;36m users in Jan 2011). Jason Scott in 2009 reminded us that this lack of transparency is completely predictable: ‚ÄúSince the dawn of time, companies have hired people whose entire job is to tell you everything is all right and you can completely trust them and the company is as stable as a rock, and to do so until they, themselves, are fired because the company is out of business.‚Äù<a href="#fnref5">‚Ü©</a></p></li>
<li id="fn6"><p>This would not come as news to <a href="http://ascii.textfiles.com/archives/1717" title="FUCK THE CLOUD - January 16, 2009">Jason</a> <a href="http://ascii.textfiles.com/archives/2229" title="Oh Boy, The Cloud - October 5, 2009">Scott</a> of <a href="http://en.wikipedia.org/wiki/Archive%20Team" title="Wikipedia: Archive Team">Archive Team</a>, of course, but nevertheless <a href="http://www.theatlantic.com/technology/archive/2013/03/finale-for-now-on-googles-self-inflicted-trust-problem/274286/" title="Finale for Now on Google's Self-Inflicted Trust Problem">James Fallows points out</a> that when a cloud service evaporates, it‚Äôs simply gone and gives an interesting comparison:</p>
<blockquote>
<p><a href="http://en.wikipedia.org/wiki/Kevin%20Drum" title="Wikipedia: Kevin Drum">Kevin Drum</a>, <a href="http://www.motherjones.com/kevin-drum/2013/03/problem-google-and-cloud" title="The Problem With Google -- and The Cloud">in <em>Mother Jones</em></a>, on why the inability to rely on Google services is more disruptive than the familiar pre-cloud experience of having favorite programs get orphaned. My example is <a href="http://en.wikipedia.org/wiki/Lotus%20Agenda" title="Wikipedia: Lotus Agenda">Lotus Agenda</a>: it has officially been dead for nearly 20 years, but <em>I can still use it</em> (if I want, in a DOS session under the VMware Fusion Windows emulator on my Macs. Talk about layered legacy systems!). When a cloud program goes away, as Google Reader has done, it‚Äôs gone. There is no way you can keep using your own ‚Äúlegacy‚Äù copy, as you could with previous orphaned software.</p>
</blockquote>
<a href="#fnref6">‚Ü©</a></li>
<li id="fn7"><p>From Gannes‚Äôs <a href="http://allthingsd.com/20130324/another-reason-google-reader-died-increased-concern-about-privacy-and-compliance/">‚ÄúAnother Reason Google Reader Died: Increased Concern About Privacy and Compliance‚Äù</a></p>
<blockquote>
<p>But at the same time, Google Reader was too deeply integrated into Google Apps to spin it off and sell it, like the company did last year with its SketchUp 3-D modeling software.</p>
</blockquote>
<p><a href="https://news.ycombinator.com/item?id=5373584">mattbarrie</a> on Hacker News:</p>
<blockquote>
<p>I‚Äôm here with Alan Noble who runs engineering at Google Australia and ran the Google Reader project until 18 months ago. They looked at open sourcing it but it was too much effort to do so because it‚Äôs tied to closely to Google infrastructure. Basically it‚Äôs been culled due to long term declining use.</p>
</blockquote>
<a href="#fnref7">‚Ü©</a></li>
<li id="fn8"><p>The sheer size &amp; dominance of some Google services have lead to comparisons to natural monopolies, such as the <em>Economist</em> column <a href="http://www.economist.com/blogs/freeexchange/2013/03/utilities">‚ÄúGoogle‚Äôs Google problem‚Äù</a>. I saw this comparison mocked, but it‚Äôs worth noting that at least one Googler made the same comparison years before. From <a href="http://en.wikipedia.org/wiki/Steven%20Levy" title="Wikipedia: Steven Levy">Levy</a>‚Äôs <em><a href="http://en.wikipedia.org/wiki/In%20the%20Plex" title="Wikipedia: In the Plex">In the Plex</a></em> 2011, part 7, section 2:</p>
<blockquote>
<p>While some Googlers felt singled out unfairly for the attention, the more measured among them understood it as a natural consequence of Google‚Äôs increasing power, especially in regard to distributing and storing massive amounts of information. ‚ÄúIt‚Äôs as if Google took over the water supply for the entire United States‚Äù, says Mike Jones, who handled some of Google‚Äôs policy issues. ‚ÄúIt‚Äôs only fair that society slaps us around a little bit to make sure we‚Äôre doing the right thing.‚Äù</p>
</blockquote>
<a href="#fnref8">‚Ü©</a></li>
<li id="fn9"><p>Specifically, this can be seen as a sort of issue of reducing <a href="http://en.wikipedia.org/wiki/deadweight%20loss" title="Wikipedia: deadweight loss">deadweight loss</a>: in some of the more successful acquisitions, Google‚Äôs modus operandi was to take a very expensive or highly premium service and make it completely free while also improving the quality. Analytics, Maps, Earth, Feedburner all come to mind as services whose predecessors (multiple, in the cases of Maps and Earth) charged money for their services (sometimes a great deal). This leads to deadweight loss as people do not use them, who would benefit to some degree but not to the full amount of the price (plus other factors like riskiness of investing time and money into trying it out). Google cites figures like billions of users over the years for several of these formerly-premium services, suggesting the gains from reduced deadweight loss are large.<a href="#fnref9">‚Ü©</a></p></li>
<li id="fn10"><p>If there is one truth of the tech industry, it‚Äôs that no giant (except IBM) survives forever. Death rates for all corporations and nonprofits are <a href="Girl%20Scouts%20and%20good%20governance#fn1">very high</a>, but particularly so for tech. <a href="http://shkspr.mobi/blog/2013/03/preparing-for-the-collapse-of-digital-civilization/" title="Preparing for the Collapse of Digital Civilization">One blogger</a> asks a good question:</p>
<blockquote>
<p>As we come to rely more and more on the Internet, it‚Äôs becoming clear that there is a real threat posed by tying oneself to a 3rd party service. The Internet is famously designed to route around failures caused by a nuclear strike - but it cannot defend against a service being withdrawn or a company going bankrupt. It‚Äôs tempting to say that multi-billion dollar companies like Apple and Google will never disappear - but a quick look at history shows Nokia, Enron, Amstrad, Sega, and many more which have fallen from great heights until they are mere shells and no longer offer the services which many people once relied on‚Ä¶I like to pose this question to my photography friends - ‚ÄúWhat would you do if Yahoo! suddenly decided to delete all your Flickr photos?‚Äù Some of them have backups - most faint at the thought of all their work vanishing.</p>
</blockquote>
<a href="#fnref10">‚Ü©</a></li>
<li id="fn11"><p>Weber‚Äôs conclusion:</p>
<blockquote>
<p>We discovered there‚Äôs been a total of about 251 independent Google products since 1998 (avoiding add-on features and experiments that merged into other projects), and found that 90, or approximately 36% of them have been canceled. Awesomely, we also collected 8 major flops and 14 major successes, which means that 36% of its high-profile products are failures. That‚Äôs quite the coincidence! NOTE: We did not manipulate data to come to this conclusion. It was a happy accident.</p>
</blockquote>
<p>In an even more happy accident, my dataset of 350 products yields 123 canceled/shutdown entries, or 35%!<a href="#fnref11">‚Ü©</a></p></li>
<li id="fn12"><p><a href="http://www.blackcatpoems.com/t/the_mystic.html">‚ÄúThe Mystic‚Äù</a>, <em>Poems, Chiefly Lyrical</em>; <a href="http://en.wikipedia.org/wiki/Alfred%2C%20Lord%20Tennyson" title="Wikipedia: Alfred, Lord Tennyson">Lord Alfred Tennyson</a><a href="#fnref12">‚Ü©</a></p></li>
<li id="fn13"><p><a href="http://en.wikipedia.org/wiki/Yamabe%20no%20Akahito" title="Wikipedia: Yamabe no Akahito">Yamabe no Akahito</a>, <a href="http://en.wikipedia.org/wiki/Man%27y%C5%8Dsh%C5%AB"><em>Man‚Äôy≈çsh≈´</em></a> <a href="http://www.temcauley.staff.shef.ac.uk/waka0088.shtml">VIII: 1426</a><a href="#fnref13">‚Ü©</a></p></li>
<li id="fn14"><p>Some have justified Reader‚Äôs shutdown as simply a rational act, since Reader was not bringing in any money and Google is not a charity. The truth seems to be related more to Google‚Äôs lack of interest since the start - it‚Äôs hard to see how Google could possibly be able to monetize Gmail and not also monetize Reader, which is confirmed by two involved Googlers (from ‚ÄúGoogle Reader lived on borrowed time: creator Chris Wetherell reflects‚Äù):</p>
<blockquote>
<p>I wonder, did the company (Google) and the ecosystem at large misread the tea leaves? Did the world at large see an RSS/reader market when in reality the actual market opportunity was in data and sentiment analysis? [Chris] Wetherell agreed. ‚ÄúThe reader market never went past the experimental phase and none was iterating on the business model,‚Äù he said. ‚ÄúMonetization abilities were never tried.‚Äù</p>
<p>‚ÄúThere was so much data we had and so much information about the affinity readers had with certain content that we always felt there was monetization opportunity,‚Äù he said. Dick Costolo (currently CEO of Twitter), who worked for Google at the time (having sold Google his company, Feedburner), came up with many monetization ideas but they fell on deaf ears. Costolo, of course is working hard to mine those affinity-and-context connections for Twitter, and is succeeding. What Costolo understood, Google and its mandarins totally missed, as noted in this <a href="http://massless.org/?p=174" title="Dreams, discernment, and Google Reader">November 2011 blog post</a> by Chris who wrote:</p>
<blockquote>
<p><strong><em>Reader exhibits the best unpaid representation I‚Äôve yet seen of a consumer‚Äôs relationship to a content producer</em></strong>. You pay for HBO? That‚Äôs a strong signal. Consuming free stuff? Reader‚Äôs model was a dream. Even better than Netflix. You get affinity (which has clear monetary value) for free, and a tracked pattern of behavior for the act of iterating over differently sourced items - and a mechanism for distributing that quickly to an ostensible audience which didn‚Äôt include social guilt or gameification - along with an extensible, scalable platform available via commonly used web technologies - all of which would be an amazing opportunity for the right product visionary. <strong><em>Reader is (was?) for information junkies; not just tech nerds</em></strong>. This market totally exists and is weirdly under-served (and is possibly affluent).</p>
</blockquote>
</blockquote>
<p>Overall, from just the PR perspective, Google probably would have been better off switching Reader to a subscription model and then eventually killing it while claiming the fees weren‚Äôt covering the costs. Offhand, 3 examples of Google adding or increasing fees come to mind: the Maps API, Talk international calls (apparently free initially), and App Engine fees; the API price increase was eventually rescinded as far as I know, and no one remembers the latter two (not even App Engine devs).<a href="#fnref14">‚Ü©</a></p></li>
<li id="fn15"><p><a href="http://en.wikipedia.org/wiki/Mark%20Pilgrim" title="Wikipedia: Mark Pilgrim">Mark Pilgrim</a>, <a href="http://web.archive.org/web/20110726001925/http://diveintomark.org/archives/2004/05/14/freedom-0">‚ÄúFreedom 0‚Äù</a>; ironically, Pilgrim (hired by Google in 2007) seems to be responsible for at least one of the entries being marked dead, Google‚Äôs Doctype tech encyclopedia, since it disappeared around the time of his ‚Äúinfosuicide‚Äù and has not been resurrected - it was only <em>partially</em> FLOSS.<a href="#fnref15">‚Ü©</a></p></li>
<li id="fn16"><p><a href="http://en.wikipedia.org/wiki/Hanshan%20%28poet%29" title="Wikipedia: Hanshan (poet)">Han-Shan</a>; #18 in <a href="http://www.amazon.com/Collected-Mountain-Mandarin-Chinese-English/dp/1556591403/?tag=gwernnet-20"><em>The Collected Songs of Cold Mountain</em></a>, Red Pine 2000, ISBN 1-55659-140-3<a href="#fnref16">‚Ü©</a></p></li>
<li id="fn17"><p><a href="http://en.wikipedia.org/wiki/Julius%20Caesar%20%28play%29" title="Wikipedia: Julius Caesar (play)"><em>Google Reader</em></a> Act 1, scene 2, 15-19; with apologies.<a href="#fnref17">‚Ü©</a></p></li>
<li id="fn18"><p>Xoogler <a href="http://rachelbythebay.com/">Rachel Kroll</a> <a href="https://news.ycombinator.com/item?id=5653934">on this spike</a>:</p>
<blockquote>
<p>I have some thoughts about the spikes on the death dates.</p>
<p>September: all of the interns go back to school. These people who exist on the fringes of the system manage to get a lot of work done, possibly because they are free of most of the overhead facing real employees. Once they leave, it‚Äôs up to the FTEs [Full Time Employee] to own whatever was created, and that doesn‚Äôt always work. I wish I could have kept some of them and swapped them for some of the full-timers.</p>
<p>March/April: Annual bonus time? That‚Äôs what it used to be, at least, and I say this as someone who quit in May, and that was no accident. Same thing: people leave, and that dooms whatever they left.</p>
</blockquote>
<a href="#fnref18">‚Ü©</a></li>
<li id="fn19"><p>0.057; but as the old <a href="http://lesswrong.com/lw/g13/against_nhst/">criticism of NHST</a> goes, ‚Äúsurely God loves the 0.057 almost as much as the 0.050‚Äù.<a href="#fnref19">‚Ü©</a></p></li>
<li id="fn20"><p>Specifically: building a logistic model on a bootstrap sample and then testing accuracy against full Google dataset.<a href="#fnref20">‚Ü©</a></p></li>
<li id="fn21"><p>I include Voice even though I don‚Äôt use it or otherwise find it interesting (my criteria for the other 10) because <a href="http://www.wired.com/gadgetlab/2013/04/google-voice-future-uncertain/" title="Will Google Hang Up on Voice?">speculation</a> has been rife and because a prediction on its future <a href="http://lesswrong.com/r/discussion/lw/h3w/open_thread_april_115_2013/8p2q">was requested</a>.<a href="#fnref21">‚Ü©</a></p></li>
<li id="fn22"><p>Han-Shan, #50<a href="#fnref22">‚Ü©</a></p></li>
</ol>
</section>
</div>
</div>
<div id="footer">
<p>Still bored? Then try my <a href="https://plus.google.com/103530621949492999968/posts" title="Google+ posts">Google+ news feed</a>.</p>
<a href="https://docs.google.com/spreadsheet/viewform?formkey=dE5GLWpfX3RhX1c2Q1phcEo3U3VDVEE6MQ">Send anonymous feedback</a>
<br/>
<div id="license">
<p xmlns:dct="http://purl.org/dc/terms/" xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#">
<a rel="license" href="http://creativecommons.org/publicdomain/zero/1.0/">
<img src="http://i.creativecommons.org/p/zero/1.0/88x31.png" style="border-style: none;" alt="CC0" height="31" width="88"/>
</a>
</p>
</div>
</div>
 
<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
 
<script type="text/javascript" src="./static/js/footnotes.js"></script>
 
<script type="text/javascript" src="./static/js/abalytics.js"></script>
<script type="text/javascript">
      window.onload = function() {
      ABalytics.applyHtml();
      };
    </script>
 
<script id="googleAnalytics" type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-18912926-1']);

      ABalytics.init({
      indent: [
      {
      name: "none",
      "indent_class1": "<style>p + p { text-indent: 0.0em; margin-top: 0 }</style>"
      },
      {
      name: "indent0.1",
      "indent_class1": "<style>p + p { text-indent: 0.1em; margin-top: 0 }</style>"
      },
      {
      name: "indent0.5",
      "indent_class1": "<style>p + p { text-indent: 0.5em; margin-top: 0 }</style>"
      },
      {
      name: "indent1.0",
      "indent_class1": "<style>p + p { text-indent: 1.0em; margin-top: 0 }</style>"
      },
      {
      name: "indent1.5",
      "indent_class1": "<style>p + p { text-indent: 1.5em; margin-top: 0 }</style>"
      },
      {
      name: "indent2.0",
      "indent_class1": "<style>p + p { text-indent: 2.0em; margin-top: 0 }</style>"
      }
      ],
      }, _gaq);

      _gaq.push(['_trackPageview']);
      (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
 
<script id="outboundLinkTracking" type="text/javascript">
      $(function() {
      $("a").on('click',function(e){
      var url = $(this).attr("href");
      if (e.currentTarget.host != window.location.host) {
      _gat._getTrackerByName()._trackEvent("Outbound Links", e.currentTarget.host.replace(':80',''), url, 0);
      if (e.metaKey || e.ctrlKey || (e.button == 1)) {
      var newtab = true;
      }
      if (!newtab) {
      e.preventDefault();
      setTimeout('document.location = "' + url + '"', 100);
      }
      }
      });
      });
    </script>
 
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
 
<script type="text/javascript" src="./static/js/footnotes.js"></script>
 
<script type="text/javascript" src="./static/js/tablesorter.js"></script>
<script type="text/javascript" id="tablesorter">
      $(document).ready(function() {
      $("table").tablesorter();
      }); </script>
 
<div id="disqus_thread"></div>
<script type="text/javascript">
      if (document.title != 'Essays') { <!-- avoid Disqus comments on front page -->
      (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://disqus.com/forums/gwern/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
      var disqus_shortname = 'gwern';
      (function () {
      var s = document.createElement('script'); s.async = true;
      s.src = 'http://disqus.com/forums/gwern/count.js';
      (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
      }</script>
<noscript><p>Enable JavaScript for Disqus comments</p></noscript>
</body>
</html>

