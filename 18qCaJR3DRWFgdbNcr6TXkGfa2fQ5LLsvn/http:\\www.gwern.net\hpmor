http://www.gwern.net/hpmor
HTTP/1.1 200 OK
Server: cloudflare-nginx
Date: Wed, 23 Jul 2014 07:38:57 GMT
Content-Type: text/html; charset=utf-8
Connection: close
Set-Cookie: __cfduid=d7b9d618959a773f9e4a157554e3abae61406101137639; expires=Mon, 23-Dec-2019 23:50:00 GMT; path=/; domain=.gwern.net; HttpOnly
x-amz-id-2: ML6N043dxo/J6MEx5YVuCKH9XjtiuGg4Fub5pdGnlWKIeNEF9K/qsCvP+TQS7R7E
x-amz-request-id: A5DAF5451A771BB5
x-amz-meta-s3cmd-attrs: uid:1000/gname:gwern/uname:gwern/gid:1000/mode:33152/mtime:1405189121/atime:1405189118/ctime:1405189121
Cache-Control: max-age=604800, public
Last-Modified: Sat, 12 Jul 2014 18:50:54 GMT
CF-RAY: 14e638ae3eb702b8-IAD
Content-Encoding: gzip

<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8"/>
<meta name="generator" content="hakyll"/>
<meta name="google-site-verification" content="BOhOQI1uMfsqu_DopVApovk1mJD5ZBLfan0s9go3phk"/>
<meta name="author" content="gwern"/>
<meta name="description" content="Recording fan speculation for retrospectives; statistically modeling reviews for ongoing story with R"/>
<meta name="dc.date.issued" content="3 Nov 2012"/>
<meta name="dcterms.modified" content="09 Jan 2014"/>
<title>&quot;Methods of Rationality&quot; review statistics</title>
<link rel="stylesheet" type="text/css" href="./static/css/default.css"/>
<link href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed"/>
<link rel="shortcut icon" type="image/x-icon" href="./static/img/favicon.ico"/>
</head>
<body>
 
<div class="indent_class1"></div>
<div id="main">
<div id="sidebar">
<div id="logo"><img alt="Logo: a Gothic/Fraktur blackletter capital G/ùï≤" height="36" src="./images/logo.png" width="32"/></div>
<div id="sidebar-links">
<p>
<a href="./index" title="index: categorized list of articles">Home</a>
<a href="./About" title="Site ideals, source, content, traffic, examples, license">Site</a>
<a href="./Links" title="Who am I online, what have I done, what am I like? Contact information; sites I use; things I've worked on">Me</a>
</p>
<hr/>
<div id="sidebar-news">
<p>
<a href="./Changelog" title="What's new or updated">New:</a>
<a href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed">RSS</a>
<a href="http://eepurl.com/Kc155" title="Monthly mailing list: signup form">MAIL</a>
</p>
<hr/>
</div>
<div id="cse-sitesearch">
<script>
            (function() {
            var cx = '009114923999563836576:dv0a4ndtmly';
            var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
            gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//www.google.com/cse/cse.js?cx=' + cx;
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
            })();
          </script>
<div style="width:0px;overflow:hidden;height:0px;">
<gcse:search></gcse:search>
</div>
<form id="searchbox_009114923999563836576:dv0a4ndtmly">
<input value="009114923999563836576:dv0a4ndtmly" name="cx" type="hidden"/>
<input value="FORID:11" name="cof" type="hidden"/>
<input id="q" style name="q" size="5" type="text" placeholder="search"/>
</form>
</div>
</div>
<hr/>
<div id="metadata">
<div id="abstract"><em>Recording fan speculation for retrospectives; statistically modeling reviews for ongoing story with R</em></div>
<br/>
<div id="tags"><i><a href="./tags/statistics">statistics</a>, <a href="./tags/predictions">predictions</a>, <a href="./tags/Haskell">Haskell</a></i></div>
<br/>
<div id="page-created">created:
<br/>
<i>3 Nov 2012</i></div>
<div id="last-modified">modified:
<br/>
<i>09 Jan 2014</i></div>
<br/>
<div id="version">status:
<br/>
<i>finished</i></div>
<br/>
<div id="epistemological-status"><a href="./About#belief-tags" title="Explanation of 'belief' metadata">belief:</a>
<br/>
<i>likely</i>
</div>
<hr/>
</div>
<div id="donations">
<div id="bitcoin-donation-address">
<a href="http://en.wikipedia.org/wiki/Bitcoin">‡∏ø</a>: 18qCaJR3DRWFgdbNcr6TXkGfa2fQ5LLsvn
</div>
<div id="paypal">
<form style="display: inline" action="https://www.paypal.com/cgi-bin/webscr" method="post" onClick="_gaq.push(['_trackEvent', 'Click', 'PayPalClicked', '']);">
<div class="form-type">
<input type="hidden" name="cmd" value="_s-xclick"/>
<input type="hidden" name="hosted_button_id" value="8GSLCWGCC6AF8"/>
<input type="image" src="http://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif" name="submit" alt="Help support my writings!"/>
</div>
</form>
</div>
<div id="Gittip">
<script data-gittip-username="gwern" data-gittip-widget="button" src="//gttp.co/v1.js"></script>
</div>
</div>
</div>
 
<div id="adsense">
<a href="http://41j.com/ads/ad.html"><img alt="Advertisement for 'HTerm, The Graphical Terminal'" src="http://41j.com/ads/ad.png" height="90" width="728"></a>
</div>
<div id="header">
<h1>&quot;Methods of Rationality&quot; review statistics</h1>
</div>
<div id="content">
<div id="TOC"><ul>
<li><a href="#background">Background</a></li>
<li><a href="#data">Data</a><ul>
<li><a href="#r">R</a></li>
<li><a href="#descriptive">Descriptive</a><ul>
<li><a href="#filtered-reviews">Filtered reviews</a></li>
</ul></li>
</ul></li>
<li><a href="#analysis">Analysis</a><ul>
<li><a href="#linear-modeling">Linear modeling</a><ul>
<li><a href="#all-reviews">All reviews</a></li>
</ul></li>
<li><a href="#fitting-quadratics">Fitting quadratics</a><ul>
<li><a href="#predictions-confidence-intervals">Predictions / confidence intervals</a></li>
</ul></li>
<li><a href="#modeling-conclusions">Modeling conclusions</a></li>
<li><a href="#additional-graphs">Additional graphs</a></li>
<li><a href="#the-review-race-unexpected-circumstances-versus-mor">The Review Race: <em>Unexpected Circumstances</em> versus <em>MoR</em></a><ul>
<li><a href="#averaging">Averaging</a></li>
<li><a href="#modeling">Modeling</a></li>
</ul></li>
<li><a href="#survival-analysis">Survival analysis</a><ul>
<li><a href="#source-code">Source code</a></li>
</ul></li>
</ul></li>
<li><a href="#see-also">See also</a></li>
<li><a href="#external-links">External links</a></li>
</ul></div>
<blockquote>
<p>The unprecedented gap in <a href="http://www.hpmor.com/"><em>Methods of Rationality</em></a> updates prompts musing about whether readership is increasing enough &amp; what statistics one would use; I write code to download FF.net reviews, clean it, parse it, load into R, summarize the data &amp; depict it graphically, run linear regression on a subset &amp; all reviews, note the poor fit, develop a quadratic fit instead, and use it to predict future review quantities.</p>
<p>Then, I run a similar analysis on a competing fanfiction to find out when they will have equal total review-counts. A try at logarithmic fits fails; fitting a linear model to the previous 100 days of <em>MoR</em> and the competitor works much better, and they predict a convergence in &lt;5 years.</p>
<p>A survival analysis finds no major anomalies in reviewer lifetimes, but an apparent increase in mortality for reviewers who started reviewing with later chapters, consistent with (but far from proving) the original theory that the later chapters‚Äô delays are having negative effects.</p>
</blockquote>
<section id="background" class="level1">
<h1>Background</h1>
<p>In a <a href="http://lesswrong.com/lw/f8y/open_thread_november_115_2012/7qs9">LW comment</a>, I asked:</p>
<blockquote>
<p>I noticed in Eliezer‚Äôs latest [1 November 2012] MoR update that he now has <a href="http://hpmor.com/notes/progress-12-11-01/"><em>18,000 words written</em></a> [of chapter 86], and even when that ‚Äúchapter‚Äù is finished, he still doesn‚Äôt plan to post anything, on top of a drought that has now lasted more than half a year. This doesn‚Äôt seem very optimal from the point of view of gaining readers.</p>
<p>But I was wondering how one would quantify that - how one would estimate how many readers Eliezer‚Äôs MoR strategy of very rare huge dumps is costing him. Maybe survivorship curves, where survivorship is defined as ‚Äúposting a review on FF.net‚Äù? So if say during the weekly MoR updates, a reader who posted a review of chapter X posted a review anywhere in X to X+n, that counts as survival of that reader. One problem here is that new readers are constantly arriving‚Ä¶ You can‚Äôt simply say ‚ÄúX readers did not return from chapter 25 to chapter 30, while X+N did not return from chapter 85 to 86, therefore frequent updates are better‚Äù since you would expect the latter to be bigger simply because more people are reading. And even if you had data on unfavoriting or unfollowing, the important part is the opportunity cost - how many readers would have subscribed with regular updates</p>
<p>If you had total page views, that‚Äôd be another thing; you could look at conversions: what percentage of page views resulted in conversions to subscribers for the regular update period versus the feast/fame periods. But I don‚Äôt think FF.net provides it and while <a href="http://www.hpmor.com/">HP:MoR.com</a> has Google Analytics, I don‚Äôt have access. Maybe one could look at each chapter pair-wise, and seeing what fraction of reviewers return? The growth might average out since we‚Äôre doing so many comparisons‚Ä¶ But the delay is now so extreme this would fail: we‚Äôd expect a huge growth in reviewers from chapter 85 to chapter 86, for example, simply because it‚Äôs been like 8 months now - here too the problem is that the growth in reviewers will be less than what it ‚Äúshould be‚Äù. But how do we figure out ‚Äúshould be‚Äù?</p>
<p>After some additional discussion with clone_of_saturn, I‚Äôve rejected the idea of survivorship curves; we thought correlations between duration and total review count might work, but interpretation was not clear. So the best current idea is: gather duration between each chapter, the # of reviews posted within X days (where X is something like 2 or 3), plot the points, and then see whether a line fits it better or a logarithm/logistic curve - to see whether growth slowed as the updates were spaced further apart.</p>
<p>Getting the data is the problem. It‚Äôs easy to get total reviews for each chapter since FF.net provides them. I don‚Äôt know of any way to get total reviews after X days posted, though! A script or program could probably do it, but I‚Äôm not sure I want to bother with all this work (especially since I don‚Äôt know how to do Bayesian line or logistic-fitting) if it‚Äôs not of interest to anyone or Eliezer would simply ignore any results.</p>
</blockquote>
<p>I decided to take a stab at it.</p>
</section>
<section id="data" class="level1">
<h1>Data</h1>
<p><a href="http://en.wikipedia.org/wiki/FanFiction.Net" title="Wikipedia: FanFiction.Net">FanFiction.Net</a> does not provide published dates for each chapter, so we‚Äôll be working off solely the posted reviews. FF.net reviews for <em>MoR</em> are provided on 1256 pages (as of 3 November 2012), numbering from 1. If we go from 1-1256, starting with <code>http://www.fanfiction.net/r/5782108/85/1/</code>, we can download them with a script enumerating the URLs &amp; using string interpolation. Eyeballing a textual dump of the HTML (courtesy of <code>elinks</code>), it‚Äôs easy to extract with <code>grep</code> the line with user, chapter number, and date with a HTML string embedded as a link with each review, leading to this script:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="kw">for</span> <span class="kw">i</span> in <span class="dt">{1..1256}</span>
<span class="kw">do</span> <span class="kw">elinks</span> -dump-width 1000 -no-numbering -no-references -dump http://www.fanfiction.net/r/5782108/0/<span class="ot">$i</span>/ \
     <span class="kw">|</span> <span class="kw">grep</span> <span class="st">&quot;report review for abuse &quot;</span> <span class="kw">&gt;&gt;</span> reviews.txt
<span class="kw">done</span></code></pre>
<p>Since page #1 is for the most recent URLs and we started with 1, the reviews were dumped newest to oldest: eg.</p>
<pre><code>...
[IMG] report review for abuse Faerie of Tara 4/1/12 . chapter 3
[IMG] report review for abuse Faerie of Tara 4/1/12 . chapter 2
report review for abuse Aww 4/1/12 . chapter 81
report review for abuse bella-farfallina 4/1/12 . chapter 81
[IMG] report review for abuse Isis Aurora Tomoe 4/1/12 . chapter 2
[IMG] report review for abuse bosk 4/1/12 . chapter 1
[IMG] report review for abuse yamiishot 4/1/12 . chapter 81
report review for abuse Joe in Australia 4/1/12 . chapter 81
report review for abuse of course 3/31/12 . chapter 45
report review for abuse RickJs 3/31/12 . chapter 22
...</code></pre>
<p>We can clean this up with more shell scripting:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="kw">sed</span> -e <span class="st">'s/\[IMG\] //'</span> -e <span class="st">'s/.*for abuse //'</span> -e <span class="st">'s/ \. chapter / /'</span> reviews.txt <span class="kw">&gt;&gt;</span> filtered.txt</code></pre>
<p>This gives us better looking data like:</p>
<pre><code>NATWEST 3/4/10 4
hemotem 3/4/10 4
Isabelle Eir 3/4/10 4
Orc Shaman 3/3/10 1
Mariann's 3/3/10 4
cheekylildevil 3/3/10 4
Star Bear 3/3/10 4</code></pre>
<p>The spaces in names means we can‚Äôt simply do a search-and-replace to turn spaces into commas and call that a CSV which can easily be read in R; so more scripting, in Haskell this time (though I‚Äôm sure <code>awk</code> or something could do the job, I don‚Äôt know it):</p>
<pre class="sourceCode Haskell"><code class="sourceCode haskell"><span class="kw">import </span><span class="dt">Data.List</span> (intercalate)

<span class="ot">main ::</span> <span class="dt">IO</span> ()
main <span class="fu">=</span> <span class="kw">do</span> f <span class="ot">&lt;-</span> readFile <span class="st">&quot;filtered.txt&quot;</span>
          <span class="kw">let</span> parsed <span class="fu">=</span> map words (lines f)
          <span class="kw">let</span> unparsed <span class="fu">=</span> <span class="st">&quot;User,Date,Chapter\n&quot;</span> <span class="fu">++</span> unlines (map unsplit parsed)
          writeFile <span class="st">&quot;reviews.csv&quot;</span> unparsed

<span class="co">-- eg &quot;Star Bear 3/3/10 4&quot; ~&gt; [&quot;Star&quot;, &quot;Bear&quot;, &quot;3/3/10&quot;, &quot;4&quot;) ~&gt; &quot;Star Bear,3/3/10,4&quot;</span>
<span class="ot">unsplit ::</span> [<span class="dt">String</span>] <span class="ot">-&gt;</span> <span class="dt">String</span>
unsplit xs <span class="fu">=</span> <span class="kw">let</span> chapter <span class="fu">=</span> last xs
                 date <span class="fu">=</span> last (init xs)
                 user <span class="fu">=</span> unwords (init (init xs))
             <span class="kw">in</span> intercalate <span class="st">&quot;,&quot;</span> [user, date, chapter]</code></pre>
<section id="r" class="level2">
<h2>R</h2>
<p>A <code>runhaskell mor.hs</code> later and our <a href="./docs/2012-hpmor-reviews.csv">CSV of 18,852 reviews</a> is ready for analysis in the R interpreter:</p>
<pre class="sourceCode R"><code class="sourceCode r">data &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://www.gwern.net/docs/2012-hpmor-reviews.csv&quot;</span>)
data
...
<span class="dv">18840</span>                                        cheekylildevil  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18841</span>                                       scarletalphabet  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18842</span>                                           StoryTagger  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18843</span>                                                HJP265  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18844</span>                                              dougal74  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18845</span>                                                baceba  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18846</span>                                               Cibbler  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18847</span>                                               Davek86  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18848</span>                                        omegahurricane  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18849</span>                                         hhrforeverhhr  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18850</span>                                    anonnatymousMARTIN  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18851</span>                                            wetterwaxs  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="dv">18852</span>                                                Errrrr  <span class="dv">2</span>/<span class="dv">28</span>/<span class="dv">10</span>       <span class="dv">1</span>
<span class="co"># Parse the date strings into R date objects</span>
data$Date &lt;-<span class="st"> </span><span class="kw">as.Date</span>(data$Date,<span class="st">&quot;%m/%d/%Y&quot;</span>)
<span class="co"># Reorganize columns (we care more about the chapter &amp; date reviews were given on</span>
<span class="co"># than which user did a review):</span>
data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Chapter=</span>data$Chapter, <span class="dt">Date=</span>data$Date, <span class="dt">User=</span>data$User)</code></pre>
</section>
<section id="descriptive" class="level2">
<h2>Descriptive</h2>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">summary</span>(data)

    Chapter           Date                      User
 Min.   :<span class="st"> </span><span class="fl">1.00</span>   Min.   :<span class="dv">10-02-28</span>   Guest         :<span class="st">  </span><span class="dv">310</span>
 1st Qu.:<span class="fl">10.00</span>   1st Qu.:<span class="dv">10-06-16</span>   anon          :<span class="st">  </span><span class="dv">106</span>
 Median :<span class="fl">27.00</span>   Median :<span class="dv">10-11-26</span>   voodooqueen126:<span class="st">   </span><span class="dv">85</span>
 Mean   :<span class="fl">35.64</span>   Mean   :<span class="dv">11-02-10</span>   Kutta         :<span class="st">   </span><span class="dv">77</span>
 3rd Qu.:<span class="fl">63.00</span>   3rd Qu.:<span class="dv">11-09-02</span>   badkidoh      :<span class="st">   </span><span class="dv">73</span>
 Max.   :<span class="fl">85.00</span>   Max.   :<span class="dv">12-11-02</span>   AR            :<span class="st">   </span><span class="dv">72</span>
                                    (Other)       :<span class="dv">18129</span></code></pre>
<p>Surprisingly, it looks like only one user (‚Äòvoodooqueen126‚Äô) has reviewed every chapter. The dates also clearly indicate the slowdown in <em>MoR</em> posting, with the median chapter being posted all the way back in 2010. What does the per-day review pattern look like?</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">table</span>(data$Date))</code></pre>
<figure>
<img alt="Reviews posted, by day" height="733" src="./images/hpmor/perday.png" width="1509"/><figcaption>Reviews posted, by day</figcaption>
</figure>
<p>Quite spiky. At a guess, the days with huge spikes are new chapters, while a low-level average of what looks like ~10 reviews a day holds steady over time. What‚Äôs the cumulative total of all reviews over time?</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">cumsum</span>(<span class="kw">table</span>(data$Date)))</code></pre>
<figure>
<img alt="All (total cumulative) reviews posted, by day" height="781" src="./images/hpmor/runningtotal.png" width="1555"/><figcaption>All (total cumulative) reviews posted, by day</figcaption>
</figure>
<p>When we plot each review by the chapter it was reviewing and the date it was posted, we see more clear patterns:</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">plot</span>(data$Chapter,data$Date)</code></pre>
<figure>
<img alt="Scatterplot of all reviews, date posted versus chapter" height="547" src="./images/hpmor/allreviews.png" width="1060"/><figcaption>Scatterplot of all reviews, date posted versus chapter</figcaption>
</figure>
<p>Obviously no one can review a chapter before it was posted, which is the thick black line along the bottom. We can also see how the delays between chapters: originally very small, the gaps increase tremendously in the 70s (remember the whole y-axis is ~3 years) - just 2 of the delays look like they cover something like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>1</mn><mn>3</mn></mfrac><mo>‚àí</mo><mfrac><mn>1</mn><mn>2</mn></mfrac></mrow></math> the entire lifespan of <em>MoR</em>! Other interesting observations is that chapter 1 and chapter 5 are extremely popular for reviewers, with hardly a day passing without review even as vast wastelands open up in 2012 in chapters 40-60. A final curious observation is the presence of distinct horizontal lines; I am guessing that these are people prone to reviewing who are binging their way through <em>MoR</em>.</p>
<section id="filtered-reviews" class="level3">
<h3>Filtered reviews</h3>
<p>The next question is: if we plot only the subset of reviews which were made within 7 days of the first review, what does that look like?</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="co"># extract each chapter's reviews into its own vector:</span>
<span class="co"># one vector of dates for ch1, one vector of dates for ch2, for ch3 etc.</span>
myList &lt;-<span class="st"> </span><span class="kw">list</span>()
for (i in <span class="dv">1</span>:<span class="dv">85</span>) { myList[[i]] &lt;-<span class="st"> </span><span class="kw">rev</span>(data$Date[data$Chapter==i]) }
<span class="co"># take a look at the first entry in each vector as a sanity check</span>
<span class="kw">lapply</span>(myList, function(a) a[<span class="dv">1</span>])
[[<span class="dv">1</span>]]
[<span class="dv">1</span>] <span class="st">&quot;10-02-28&quot;</span>

[[<span class="dv">2</span>]]
[<span class="dv">1</span>] <span class="st">&quot;10-02-28&quot;</span>

[[<span class="dv">3</span>]]
[<span class="dv">1</span>] <span class="st">&quot;10-03-01&quot;</span>

[[<span class="dv">4</span>]]
[<span class="dv">1</span>] <span class="st">&quot;10-03-03&quot;</span>

[[<span class="dv">5</span>]]
[<span class="dv">1</span>] <span class="st">&quot;10-03-08&quot;</span>
...
<span class="co"># looks reasonable</span>
<span class="co"># what's the total by each chapter?</span>
totalreviews &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(myList, length))
totalreviews
 [<span class="dv">1</span>]  <span class="dv">823</span>  <span class="dv">391</span>  <span class="dv">205</span>  <span class="dv">293</span> <span class="dv">1159</span>  <span class="dv">499</span>  <span class="dv">419</span>  <span class="dv">375</span>  <span class="dv">423</span>  <span class="dv">498</span>  <span class="dv">332</span>  <span class="dv">215</span>  <span class="dv">270</span>  <span class="dv">283</span>  <span class="dv">222</span>
[<span class="dv">16</span>]  <span class="dv">291</span>  <span class="dv">385</span>  <span class="dv">302</span>  <span class="dv">270</span>  <span class="dv">311</span>  <span class="dv">248</span>  <span class="dv">199</span>  <span class="dv">215</span>  <span class="dv">211</span>  <span class="dv">141</span>  <span class="dv">243</span>  <span class="dv">283</span>  <span class="dv">138</span>  <span class="dv">136</span>  <span class="dv">143</span>
[<span class="dv">31</span>]  <span class="dv">160</span>  <span class="dv">204</span>  <span class="dv">148</span>  <span class="dv">119</span>   <span class="dv">28</span>   <span class="dv">77</span>  <span class="dv">106</span>   <span class="dv">48</span>  <span class="dv">130</span>  <span class="dv">101</span>  <span class="dv">117</span>  <span class="dv">183</span>   <span class="dv">52</span>   <span class="dv">33</span>  <span class="dv">139</span>
[<span class="dv">46</span>]  <span class="dv">246</span>  <span class="dv">334</span>  <span class="dv">120</span>  <span class="dv">148</span>  <span class="dv">104</span>  <span class="dv">181</span>   <span class="dv">69</span>   <span class="dv">47</span>  <span class="dv">183</span>   <span class="dv">38</span>  <span class="dv">143</span>   <span class="dv">74</span>  <span class="dv">202</span>   <span class="dv">80</span>  <span class="dv">131</span>
[<span class="dv">61</span>]  <span class="dv">127</span>  <span class="dv">164</span>  <span class="dv">303</span>  <span class="dv">175</span>  <span class="dv">146</span>   <span class="dv">42</span>  <span class="dv">147</span>  <span class="dv">130</span>  <span class="dv">253</span>  <span class="dv">296</span>  <span class="dv">217</span>  <span class="dv">398</span>  <span class="dv">136</span>  <span class="dv">249</span>  <span class="dv">187</span>
[<span class="dv">76</span>]  <span class="dv">231</span>  <span class="dv">525</span>  <span class="dv">185</span>  <span class="dv">153</span>  <span class="dv">308</span>  <span class="dv">235</span>  <span class="dv">113</span>   <span class="dv">74</span>  <span class="dv">154</span>  <span class="dv">236</span>
<span class="co"># Now the hard part: map over the list of vectors, &amp; filter out anything dated 7 days after the first date</span>
earlyreviews &lt;-<span class="st"> </span><span class="kw">unlist</span>(<span class="kw">lapply</span>(<span class="kw">lapply</span>(myList,
     function(a) <span class="kw">Filter</span>(function(b) {if (b &gt;<span class="st"> </span>a[<span class="dv">1</span>]+<span class="dv">7</span>) <span class="kw">return</span>(<span class="ot">FALSE</span>) else <span class="kw">return</span>(<span class="ot">TRUE</span>)}, a)), length))
 [<span class="dv">1</span>]  <span class="dv">17</span>  <span class="dv">11</span>  <span class="dv">13</span>  <span class="dv">42</span> <span class="dv">341</span> <span class="dv">191</span> <span class="dv">204</span> <span class="dv">210</span> <span class="dv">296</span> <span class="dv">282</span> <span class="dv">117</span> <span class="dv">136</span> <span class="dv">162</span> <span class="dv">144</span> <span class="dv">126</span> <span class="dv">155</span> <span class="dv">199</span> <span class="dv">199</span> <span class="dv">159</span>
[<span class="dv">20</span>] <span class="dv">209</span> <span class="dv">133</span> <span class="dv">125</span> <span class="dv">142</span> <span class="dv">111</span>  <span class="dv">92</span> <span class="dv">145</span> <span class="dv">193</span>  <span class="dv">96</span>  <span class="dv">92</span>  <span class="dv">43</span> <span class="dv">115</span> <span class="dv">170</span>  <span class="dv">78</span>  <span class="dv">88</span>   <span class="dv">5</span>  <span class="dv">34</span>  <span class="dv">77</span>  <span class="dv">17</span>
[<span class="dv">39</span>]  <span class="dv">48</span>  <span class="dv">71</span>  <span class="dv">88</span> <span class="dv">129</span>  <span class="dv">18</span>  <span class="dv">10</span>  <span class="dv">40</span> <span class="dv">164</span> <span class="dv">246</span> <span class="dv">100</span> <span class="dv">124</span>  <span class="dv">82</span> <span class="dv">150</span>  <span class="dv">51</span>  <span class="dv">32</span> <span class="dv">168</span>  <span class="dv">32</span> <span class="dv">129</span>  <span class="dv">62</span>
[<span class="dv">58</span>] <span class="dv">167</span>  <span class="dv">66</span> <span class="dv">116</span> <span class="dv">109</span> <span class="dv">140</span> <span class="dv">155</span>  <span class="dv">55</span> <span class="dv">114</span>  <span class="dv">19</span> <span class="dv">111</span> <span class="dv">104</span> <span class="dv">175</span> <span class="dv">187</span> <span class="dv">110</span> <span class="dv">175</span> <span class="dv">127</span> <span class="dv">212</span> <span class="dv">164</span> <span class="dv">181</span>
[<span class="dv">77</span>] <span class="dv">117</span> <span class="dv">169</span> <span class="dv">139</span> <span class="dv">299</span> <span class="dv">211</span> <span class="dv">106</span>  <span class="dv">69</span> <span class="dv">143</span> <span class="dv">106</span></code></pre>
<p>How did chapter 5 get <em>341</em> reviews within 7 days and have <em>1159</em> in total, you ask - might my data not be wrong? But if you check the <a href="http://www.fanfiction.net/r/5782108/5/1/">chapter 5 reviews</a> and scroll all the way down, you‚Äôll find that people back in 2010 were <em>very</em> chuffed by the Draco scenes.</p>
</section>
</section>
</section>
<section id="analysis" class="level1">
<h1>Analysis</h1>
<section id="linear-modeling" class="level2">
<h2>Linear modeling</h2>
<p>Moving onward, we want to graph the date and then fit a <a href="http://en.wikipedia.org/wiki/linear%20model" title="Wikipedia: linear model">linear model</a> (a line which minimizes the deviation of all of the points from it; I drew on a <a href="http://www.montefiore.ulg.ac.be/~kvansteen/GBIO0009-1/ac20092010/Class8/Using%20R%20for%20linear%20regression.pdf">handout for R programming</a>):</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="co"># turn vector into a plottable table &amp; plot it</span>
filtered &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Chapter=</span><span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">85</span>),<span class="dt">Reviews=</span>earlyreviews)
<span class="kw">plot</span>(filtered$Chapter, filtered$Reviews)

<span class="co"># run a linear model: is there any clear trend?</span>
<span class="kw">lm</span>(filtered$Reviews ~<span class="st"> </span>filtered$Chapter)
...
Coefficients:
<span class="st">     </span>(Intercept)  filtered$Chapter
        <span class="fl">128.3773</span>           -<span class="fl">0.0966</span>

<span class="co"># the slope doesn't look very interesting...</span>
<span class="co"># let's plot the regression line too:</span>
<span class="kw">abline</span>(<span class="kw">lm</span>(filtered$Reviews ~<span class="st"> </span>filtered$Chapter))</code></pre>
<figure>
<img alt="Graph reviews posted within a week, per chapter." height="561" src="./images/hpmor/filteredreviews.png" width="999"/><figcaption>Graph reviews posted within a week, per chapter.</figcaption>
</figure>
<p>Look at the noise. We have some huge spikes in the early chapters, some sort of dip in the 40s-70s, and then a similar spread of highly and lowly reviewed chapters at the end. The linear model is basically just the average (<code>mean(filtered$Reviews)</code> = 124.2); it‚Äôs actually predicting slight declines in reviews posted within 7 days, interestingly.</p>
<section id="all-reviews" class="level3">
<h3>All reviews</h3>
<p>Same thing if we look at <em>total</em> reviews for each chapter:</p>
<pre class="sourceCode R"><code class="sourceCode r">filtered2=<span class="kw">data.frame</span>(<span class="dt">Chapter=</span><span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">85</span>),<span class="dt">Reviews=</span>totalreviews)
<span class="kw">plot</span>(filtered2$Chapter, filtered2$Reviews)
model2 =<span class="st"> </span><span class="kw">lm</span>(filtered2$Reviews ~<span class="st"> </span>filtered2$Chapter)
<span class="kw">summary</span>(model2)
...
Residuals:
<span class="st">    </span>Min      1Q  Median      3Q     Max
-<span class="fl">215.47</span>  -<span class="fl">98.81</span>  -<span class="fl">24.02</span>   <span class="fl">58.08</span>  <span class="fl">834.21</span>

Coefficients:
<span class="st">                  </span>Estimate Std. Error t value <span class="kw">Pr</span>(&gt;<span class="er">|</span>t|)
(Intercept)       <span class="fl">338.3462</span>    <span class="fl">33.4318</span>  <span class="fl">10.120</span> <span class="fl">3.79e-16</span> **<span class="er">*</span>
filtered2$Chapter  -<span class="fl">2.7107</span>     <span class="fl">0.6753</span>  -<span class="fl">4.014</span>  <span class="fl">0.00013</span> **<span class="er">*</span>
---
Signif. codes:<span class="st">  </span><span class="dv">0</span> ‚Äò**<span class="er">*</span>‚Äô <span class="fl">0.001</span> ‚Äò**‚Äô <span class="fl">0.01</span> ‚Äò*‚Äô <span class="fl">0.05</span> ‚Äò.‚Äô <span class="fl">0.1</span> ‚Äò ‚Äô <span class="dv">1</span>

Residual standard error:<span class="st"> </span><span class="fl">152.8</span> on <span class="dv">83</span> degrees of freedom
Multiple R-squared:<span class="st"> </span><span class="fl">0.1626</span>, Adjusted R-squared:<span class="st"> </span><span class="fl">0.1525</span>
F-statistic:<span class="st"> </span><span class="fl">16.11</span> on <span class="dv">1</span> and <span class="dv">83</span> DF,  p-value:<span class="st"> </span><span class="fl">0.0001302</span>

<span class="kw">abline</span>(model2)</code></pre>
<figure>
<img alt="Graph all reviews by posted time and chapter" height="569" src="./images/hpmor/allreviewsbychapter.png" width="1020"/><figcaption>Graph all reviews by posted time and chapter</figcaption>
</figure>
</section>
</section>
<section id="fitting-quadratics" class="level2">
<h2>Fitting quadratics</h2>
<p>Looking at the line, the fit seems poor over all (large <a href="http://en.wikipedia.org/wiki/Errors%20and%20residuals%20in%20statistics" title="Wikipedia: Errors and residuals in statistics">residuals</a>) quite poor at the beginning and end - doesn‚Äôt it look like a big U? That sounds like a quadratic, so more Googling for R code and an hour or two later, I fit some quadratics to the 7-day reviews and the total reviews:</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">plot</span>(filtered$Chapter, filtered$Reviews)
model &lt;-<span class="st"> </span><span class="kw">lm</span>(filtered$Reviews ~<span class="st"> </span>filtered$Chapter +<span class="st"> </span><span class="kw">I</span>(filtered$Chapter^<span class="dv">2</span>))
<span class="kw">summary</span>(model)
...
Residuals:
<span class="st">     </span>Min       1Q   Median       3Q      Max
-<span class="fl">160.478</span>  -<span class="fl">42.795</span>   -<span class="fl">3.364</span>   <span class="fl">40.813</span>  <span class="fl">179.321</span>

Coefficients:
<span class="st">                       </span>Estimate Std. Error t value <span class="kw">Pr</span>(&gt;<span class="er">|</span>t|)
(Intercept)           <span class="fl">178.41168</span>   <span class="fl">22.70879</span>   <span class="fl">7.857</span> <span class="fl">1.34e-11</span> **<span class="er">*</span>
filtered$Chapter       -<span class="fl">3.54725</span>    <span class="fl">1.21875</span>  -<span class="fl">2.911</span>  <span class="fl">0.00464</span> **
<span class="kw">I</span>(filtered$Chapter^<span class="dv">2</span>)   <span class="fl">0.04012</span>    <span class="fl">0.01373</span>   <span class="fl">2.922</span>  <span class="fl">0.00449</span> **
---
Signif. codes:<span class="st">  </span><span class="dv">0</span> ‚Äò**<span class="er">*</span>‚Äô <span class="fl">0.001</span> ‚Äò**‚Äô <span class="fl">0.01</span> ‚Äò*‚Äô <span class="fl">0.05</span> ‚Äò.‚Äô <span class="fl">0.1</span> ‚Äò ‚Äô <span class="dv">1</span>

Residual standard error:<span class="st"> </span><span class="fl">68.15</span> on <span class="dv">82</span> degrees of freedom
Multiple R-squared:<span class="st"> </span><span class="fl">0.09533</span>,    Adjusted R-squared:<span class="st"> </span><span class="fl">0.07327</span>
F-statistic:<span class="st">  </span><span class="fl">4.32</span> on <span class="dv">2</span> and <span class="dv">82</span> DF,  p-value:<span class="st"> </span><span class="fl">0.01645</span>

xx &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(filtered$Chapter),<span class="kw">max</span>(filtered$Chapter),<span class="dt">len=</span><span class="dv">200</span>)
yy &lt;-<span class="st"> </span>model$coef %*%<span class="st"> </span><span class="kw">rbind</span>(<span class="dv">1</span>,xx,xx^<span class="dv">2</span>)
<span class="kw">lines</span>(xx,yy,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">3</span>)</code></pre>
<figure>
<img alt="Fit a quadratic (U-curve) to the filtered review-count for each chapter" height="561" src="./images/hpmor/filteredreviews-quadratic.png" width="999"/><figcaption>Fit a quadratic (U-curve) to the filtered review-count for each chapter</figcaption>
</figure>
<p>The quadratic fits nicely. And for total reviews, we observe a similar curve:</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">plot</span>(filtered2$Chapter, filtered2$Reviews)
model2 &lt;-<span class="st"> </span><span class="kw">lm</span>(filtered2$Reviews ~<span class="st"> </span>filtered2$Chapter +<span class="st"> </span><span class="kw">I</span>(filtered2$Chapter^<span class="dv">2</span>))
<span class="kw">summary</span>(model2)
...
Residuals:
<span class="st">    </span>Min      1Q  Median      3Q     Max
-<span class="fl">289.68</span>  -<span class="fl">56.83</span>  -<span class="fl">17.59</span>   <span class="fl">36.67</span>  <span class="fl">695.45</span>

Coefficients:
<span class="st">                        </span>Estimate Std. Error t value <span class="kw">Pr</span>(&gt;<span class="er">|</span>t|)
(Intercept)            <span class="fl">543.84019</span>   <span class="fl">41.43142</span>  <span class="fl">13.126</span>  &lt;<span class="st"> </span><span class="fl">2e-16</span> **<span class="er">*</span>
filtered2$Chapter      -<span class="fl">16.88265</span>    <span class="fl">2.22356</span>  -<span class="fl">7.593</span> <span class="fl">4.45e-11</span> **<span class="er">*</span>
<span class="kw">I</span>(filtered2$Chapter^<span class="dv">2</span>)   <span class="fl">0.16479</span>    <span class="fl">0.02505</span>   <span class="fl">6.578</span> <span class="fl">4.17e-09</span> **<span class="er">*</span>
---
Signif. codes:<span class="st">  </span><span class="dv">0</span> ‚Äò**<span class="er">*</span>‚Äô <span class="fl">0.001</span> ‚Äò**‚Äô <span class="fl">0.01</span> ‚Äò*‚Äô <span class="fl">0.05</span> ‚Äò.‚Äô <span class="fl">0.1</span> ‚Äò ‚Äô <span class="dv">1</span>

Residual standard error:<span class="st"> </span><span class="fl">124.3</span> on <span class="dv">82</span> degrees of freedom
Multiple R-squared:<span class="st"> </span><span class="fl">0.4518</span>, Adjusted R-squared:<span class="st"> </span><span class="fl">0.4384</span>
F-statistic:<span class="st"> </span><span class="fl">33.79</span> on <span class="dv">2</span> and <span class="dv">82</span> DF,  p-value:<span class="st"> </span><span class="fl">1.977e-11</span>

xx &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(filtered2$Chapter),<span class="kw">max</span>(filtered2$Chapter),<span class="dt">len=</span><span class="dv">200</span>)
yy &lt;-<span class="st"> </span>model2$coef %*%<span class="st"> </span><span class="kw">rbind</span>(<span class="dv">1</span>,xx,xx^<span class="dv">2</span>)
<span class="kw">lines</span>(xx,yy,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">3</span>)</code></pre>
<figure>
<img alt="Fit a quadratic (U-curve) to the final 3 November 2012 review-count for each chapter" height="569" src="./images/hpmor/allreviews-quadratic.png" width="1009"/><figcaption>Fit a quadratic (U-curve) to the final 3 November 2012 review-count for each chapter</figcaption>
</figure>
<section id="predictions-confidence-intervals" class="level3">
<h3>Predictions / confidence intervals</h3>
<p>Linear models, even as a quadratic, are pretty simple. They don‚Äôt have to be a black box, and they‚Äôre interpretable the moment you are given the coefficients: ‚Äòthis‚Äô times ‚Äòthat‚Äô equals ‚Äòthat other thing‚Äô. We learned these things in middle school by hand-graphing functions. So it‚Äôs not hard to figure out what function our fitted lines represent, or to extend them rightwards to uncharted waters.</p>
<p>Our very first linear model for <code>filtered$Reviews ~ filtered$Chapter</code> told us that the coefficients were Intercept: 128.3773, <code>filtered$Chapter</code>: -0.0966. Going back to middle school math, the intercept is the <em>y</em>-intercept, the value of <em>y</em> when <em>x</em>=0; then the other number must be how much we multiply <em>x</em> by.</p>
<p>So the linear model is simply this: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><mo>‚àí</mo><mn>0</mn><mo>.</mo><mn>0966</mn><mi>x</mi><mo>+</mo><mn>128</mn><mo>.</mo><mn>3773</mn></mrow></math>. To make predictions, we substitute in for <em>x</em> as we please; so chapter 1 is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>‚àí</mo><mn>0</mn><mo>.</mo><mn>0966</mn><mo>√ó</mo><mn>1</mn><mo>+</mo><mn>128</mn><mo>.</mo><mn>3773</mn><mo>=</mo><mn>128</mn><mo>.</mo><mn>28</mn></mrow></math>, chapter 85 is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mo>‚àí</mo><mn>0</mn><mo>.</mo><mn>0966</mn><mo>√ó</mo><mn>85</mn><mo>+</mo><mn>128</mn><mo>.</mo><mn>3773</mn><mo>=</mo><mn>120</mn><mo>.</mo><mn>16</mn></mrow></math> (well, what did you expect of a nearly-flat line?) and so on.</p>
<p>We can do this for the quadratic too; your ordinary quadratic looked like <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mi>x</mi><mo>+</mo><mn>1</mn></mrow></math>, so it‚Äôs easy to guess what the filled-in equivalent will be. We‚Äôll look at total reviews this time (<code>filtered2$Reviews ~ filtered2$Chapter + I(filtered2$Chapter^2)</code>). The intercept was 543.84019, the first coefficient was -16.88265, and the third mystery number was 0.16479, so the new equation is <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><mn>0</mn><mo>.</mo><mn>16479</mn><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo><mo>‚àí</mo><mn>16</mn><mo>.</mo><mn>88265</mn><mi>x</mi><mo>+</mo><mn>543</mn><mo>.</mo><mn>84019</mn></mrow></math>. What‚Äôs the prediction for chapter 86? <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>0</mn><mo>.</mo><mn>16479</mn><mo>√ó</mo><mo stretchy="false">(</mo><msup><mn>86</mn><mn>2</mn></msup><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mo>‚àí</mo><mn>16</mn><mo>.</mo><mn>88265</mn><mo stretchy="false">)</mo><mo>√ó</mo><mn>86</mn><mo>+</mo><mn>543</mn><mo>.</mo><mn>8401</mn><mo>=</mo><mn>310</mn><mo>.</mo><mn>719</mn></mrow></math>. And for chapter 100, it‚Äôd go <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>0</mn><mo>.</mo><mn>16479</mn><mo>√ó</mo><mo stretchy="false">(</mo><msup><mn>100</mn><mn>2</mn></msup><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mo>‚àí</mo><mn>16</mn><mo>.</mo><mn>88265</mn><mo stretchy="false">)</mo><mo>√ó</mo><mn>100</mn><mo>+</mo><mn>543</mn><mo>.</mo><mn>8401</mn><mo>=</mo><mn>503</mn><mo>.</mo><mn>475</mn></mrow></math>.</p>
<p>Can we automate this? Yes. Continuing with total reviews per chapter:</p>
<pre class="sourceCode R"><code class="sourceCode r">x =<span class="st"> </span>filtered2$Chapter; y =<span class="st"> </span>filtered2$Reviews
model2 &lt;-<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x +<span class="st"> </span><span class="kw">I</span>(x^<span class="dv">2</span>))
<span class="kw">predict.lm</span>(model2, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">86</span>:<span class="dv">100</span>)))
       <span class="dv">1</span>        <span class="dv">2</span>        <span class="dv">3</span>        <span class="dv">4</span>        <span class="dv">5</span>        <span class="dv">6</span>        <span class="dv">7</span>        <span class="dv">8</span>
<span class="fl">310.7242</span> <span class="fl">322.3504</span> <span class="fl">334.3061</span> <span class="fl">346.5914</span> <span class="fl">359.2063</span> <span class="fl">372.1507</span> <span class="fl">385.4248</span> <span class="fl">399.0284</span>
       <span class="dv">9</span>       <span class="dv">10</span>       <span class="dv">11</span>       <span class="dv">12</span>       <span class="dv">13</span>       <span class="dv">14</span>       <span class="dv">15</span>
<span class="fl">412.9616</span> <span class="fl">427.2244</span> <span class="fl">441.8168</span> <span class="fl">456.7387</span> <span class="fl">471.9903</span> <span class="fl">487.5714</span> <span class="fl">503.4821</span></code></pre>
<p>(Notice that this use of <code>predict</code> spits out the same answers we calculated for chapter 86 and chapter 100.)</p>
<p>So for example, the fitted quadratic model predicts ~503 total reviews for chapter 100. Making further assumptions (the error in estimating is independent of time, normally distributed, has zero mean, and constant variance), we <a href="http://predictionbook.com/predictions/9270" title="HP MoR: chapter 100 will, after 1 month, have 215-792 reviews posted on FF.net">predict</a> that our 95% confidence interval 215-792 total reviews of chapter 100:</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">predict.lm</span>(model2, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="dv">100</span>), <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)
       fit    lwr      upr
  <span class="fl">503.4821</span> <span class="fl">215.05</span> <span class="fl">791.9141</span></code></pre>
<p>Clearly the model isn‚Äôt making very strong predictions because the standard error is so high. We get a similar result asking just for the 7-day reviews:</p>
<pre class="sourceCode R"><code class="sourceCode r">filtered=<span class="kw">data.frame</span>(<span class="dt">Chapter=</span><span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">85</span>),<span class="dt">Reviews=</span>earlyreviews)
y=filtered$Reviews; x=filtered$Chapter

model =<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x +<span class="st"> </span><span class="kw">I</span>(x^<span class="dv">2</span>))
model
...
Coefficients:
(Intercept)            x       <span class="kw">I</span>(x^<span class="dv">2</span>)
  <span class="fl">178.41168</span>     -<span class="fl">3.54725</span>      <span class="fl">0.04012</span>

<span class="kw">predict.lm</span>(model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="dv">100</span>), <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)
      fit      lwr      upr
<span class="dv">1</span> <span class="fl">224.925</span> <span class="fl">66.83383</span> <span class="fl">383.0162</span></code></pre>
<p>This <em>seems</em> like a fairly reasonable <a href="http://predictionbook.com/predictions/9343" title="HP MoR: chapter 100 will, after 7 days, have 66-383 reviews posted on FF.net">prediction</a>: 66-383 reviews posted within 7 days. But this is based purely off the model, is it taking into account that even the quadratic model has some pretty large residuals? Let‚Äôs plot an entire <a href="http://en.wikipedia.org/wiki/confidence%20band" title="Wikipedia: confidence band">confidence band</a>:</p>
<pre class="sourceCode R"><code class="sourceCode r">filtered &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">Chapter=</span><span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">85</span>),<span class="dt">Reviews=</span>earlyreviews)
y &lt;-<span class="st"> </span>filtered$Reviews; x =<span class="st"> </span>filtered$Chapter
<span class="kw">plot</span>(x, <span class="dt">xlab=</span><span class="st">&quot;Chapter&quot;</span>,
     y, <span class="dt">ylab=</span><span class="st">&quot;Reviews posted within week&quot;</span>,
     <span class="dt">main=</span><span class="st">&quot;7-Day Reviews per Chapter with Quadratic Fit and 95% Prediction Intervals&quot;</span>)

model &lt;-<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x +<span class="st"> </span><span class="kw">I</span>(x^<span class="dv">2</span>))
xx &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(x),<span class="kw">max</span>(x),<span class="dt">len=</span><span class="dv">200</span>)
yy &lt;-<span class="st"> </span>model$coef %*%<span class="st"> </span><span class="kw">rbind</span>(<span class="dv">1</span>,xx,xx^<span class="dv">2</span>)
<span class="kw">lines</span>(xx,yy,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">3</span>)

prd &lt;-<span class="st"> </span><span class="kw">predict</span>(model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">85</span>)), <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">85</span>),prd[,<span class="dv">2</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">85</span>),prd[,<span class="dv">3</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre>
<figure>
<img alt="7-Day Reviews per Chapter with Quadratic Fit and 95% Prediction Intervals" height="825" src="./images/hpmor/filteredreviews-quadratic-ci.png" width="1504"/><figcaption>7-Day Reviews per Chapter with Quadratic Fit and 95% Prediction Intervals</figcaption>
</figure>
<p>The prediction bands look like they‚Äôre more or less living up to their claimed coverage (7/85 isn‚Äôt hugely far from 5/100) but do this by being very large and embarrassingly can‚Äôt even exclude zero. This should temper our forecasting enthusiasm. What might predict the residuals better? Day of week? Length of chapter update? Interval between updates? Hard to know.</p>
</section>
</section>
<section id="modeling-conclusions" class="level2">
<h2>Modeling conclusions</h2>
<p>Sadly, neither the quadratic nor the linear fits lend any support to the original notion that we could estimate how much readership <em>MoR</em> is being cost by the exponentially-slowing updates for the simple reason that the observed reviews per chapter do not match any simple increasing model of readership but a stranger U-curve (why do reviews decline so much in the middle when updates were quite regular?).</p>
<p>We could argue that the sluggishness of the recent upturn in the U-curve (in the 80s) is due to there only being a few humongous chapters, but those chapters are quantitatively and qualitatively different from the early chapters (especially chapter 5) so it‚Äôs not clear that any of them ‚Äúshould‚Äù have say, 341 reviews within a week.</p>
</section>
<section id="additional-graphs" class="level2">
<h2>Additional graphs</h2>
<p><img alt="Total cumulative MoR reviews as a function of time, with linear fit" height="803" src="./images/hpmor/runningtotal-linear.png" width="1553"/> <img alt="Total MoR reviews vs time, quadratic fit" height="782" src="./images/hpmor/runningtotal-quadratic.png" width="1553"/> <img alt="Total cumulative MoR reviews, 100 days prior to 17 November 2012, linear fit" height="803" src="./images/hpmor/runningtotal-linear-100.png" width="1555"/></p>
</section>
<section id="the-review-race-unexpected-circumstances-versus-mor" class="level2">
<h2>The Review Race: <em>Unexpected Circumstances</em> versus <em>MoR</em></h2>
<p>An interesting question is whether and when <em>MoR</em> will become the most-reviewed fanfiction on FFN (and by extension, one of the most-reviewed fanfictions of all time). It has already passed such hits as <a href="http://www.fanfiction.net/s/6036478/1/Parachute">‚ÄúParachute‚Äù</a> &amp; <a href="http://www.fanfiction.net/s/6453369/1/Fridays_at_Noon">‚ÄúFridays at Noon‚Äù</a>, but the next competitor is <a href="http://www.fanfiction.net/s/6496709/1/Unexpected_Circumstances"><em>Unexpected Circumstances</em></a> (<em>UC</em>) at 24,163 reviews to <em>MoR</em>‚Äôs 18,917<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<section id="averaging" class="level3">
<h3>Averaging</h3>
<p>24.2k vs 19k is a large gap to cover. Some quick and dirty reasoning:</p>
<ul>
<li><em>MoR</em>: 18911 reviews, begun 02-28-10 or 992 days ago, so <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>18911</mn><mn>992</mn></mfrac><mo>=</mo><mn>19</mn><mo>.</mo><mn>1</mn></mrow></math> reviews per day (Status: in progress)</li>
<li><em>UC</em>: 24163 reviews, begun 11-22-10 or 725 days ago, so <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mn>24163</mn><mn>725</mn></mfrac><mo>=</mo><mn>33</mn><mo>.</mo><mn>3</mn></mrow></math> reviews per day (Status: complete)</li>
</ul>
<p>Obviously if <em>UC</em> doesn‚Äôt see its average review rate decline, it‚Äôll never be passed by <em>MoR</em>! But let‚Äôs assume the review rate has gone to zero (not <em>too</em> unrealistic a starting assumption, since <em>UC</em> is finished and <em>MoR</em> is not), how many days will it take <em>MoR</em> to catch up? <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mn>24163</mn><mo>‚àí</mo><mn>18911</mn></mrow><mfrac><mn>18911</mn><mn>992</mn></mfrac></mfrac><mo>=</mo><mn>275</mn><mo>.</mo><mn>5</mn></mrow></math> or under a year.</p>
</section>
<section id="modeling" class="level3">
<h3>Modeling</h3>
<section id="descriptive-data-and-graphs" class="level4">
<h4>Descriptive Data and Graphs</h4>
<p>We can take into account the existing review rates with the same techniques we used previously for investigating <em>MoR</em> review patterns. First, we can download &amp; parse <em>UC</em>‚Äôs reviews the same way as before: make the obvious tweaks to the shell script, run the <code>sed</code> command, re-run the Haskell, then load into R &amp; format:</p>
<pre class="sourceCode Bash"><code class="sourceCode bash"><span class="kw">for</span> <span class="kw">i</span> in <span class="dt">{1..1611}</span><span class="kw">;</span>
<span class="kw">do</span> <span class="kw">elinks</span> -dump-width 1000 -no-numbering -no-references -dump http://www.fanfiction.net/r/6496709/0/<span class="ot">$i</span>/
    <span class="kw">|</span> <span class="kw">grep</span> <span class="st">&quot;report review for abuse &quot;</span> <span class="kw">&gt;&gt;</span> reviews.txt<span class="kw">;</span>
<span class="kw">done</span>
<span class="kw">sed</span> -e <span class="st">'s/\[IMG\] //'</span> -e <span class="st">'s/.*for abuse //'</span> -e <span class="st">'s/ \. chapter / /'</span> reviews.txt <span class="kw">&gt;&gt;</span> filtered.txt
<span class="kw">runhaskell</span> mor.hs</code></pre>
<pre class="sourceCode R"><code class="sourceCode r">data &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;http://www.gwern.net/docs/2012-uc-reviews.csv&quot;</span>,<span class="dt">header=</span><span class="ot">TRUE</span>, <span class="dt">sep=</span><span class="st">&quot;,&quot;</span>,<span class="dt">quote=</span><span class="st">&quot;&quot;</span>)
<span class="co"># Convert the date strings to R dates</span>
data=<span class="kw">data.frame</span>(<span class="dt">Chapter=</span>data$Chapter, <span class="dt">Date=</span><span class="kw">as.Date</span>(data$Date,<span class="st">&quot;%m/%d/%Y&quot;</span>), <span class="dt">User=</span>data$User)
data
   Chapter     Date                     User
<span class="dv">1</span>       <span class="dv">39</span> <span class="dv">12-11-14</span>                    REL57
<span class="dv">2</span>       <span class="dv">27</span> <span class="dv">12-11-13</span>                    Guest
<span class="dv">3</span>       <span class="dv">26</span> <span class="dv">12-11-13</span>                  Melanie
<span class="dv">4</span>       <span class="dv">24</span> <span class="dv">12-11-13</span>                  Melanie
<span class="dv">5</span>       <span class="dv">39</span> <span class="dv">12-11-13</span>    Dream-with-your-heart
<span class="dv">6</span>       <span class="dv">39</span> <span class="dv">12-11-12</span>          ebdarcy.qt4good
<span class="dv">7</span>       <span class="dv">19</span> <span class="dv">12-11-10</span>                    Guest
<span class="dv">8</span>       <span class="dv">15</span> <span class="dv">12-11-10</span>                 JD Thorn
<span class="dv">9</span>       <span class="dv">39</span> <span class="dv">12-11-09</span>                 shollyRK
<span class="dv">10</span>       <span class="dv">9</span> <span class="dv">12-11-08</span> of-Snowfall-and-Seagulls
...
<span class="kw">summary</span>(data)
    Chapter           Date                          User
 Min.   :<span class="st"> </span><span class="fl">1.00</span>   Min.   :<span class="dv">10-11-22</span>   Guest             :<span class="st">   </span><span class="dv">54</span>
 1st Qu.:<span class="fl">12.00</span>   1st Qu.:<span class="dv">11-02-26</span>   AbruptlyChagrined :<span class="st">   </span><span class="dv">39</span>
 Median :<span class="fl">22.00</span>   Median :<span class="dv">11-06-20</span>   AlexaBrandonCullen:<span class="st">   </span><span class="dv">39</span>
 Mean   :<span class="fl">20.62</span>   Mean   :<span class="dv">11-06-10</span>   Angeldolphin01    :<span class="st">   </span><span class="dv">39</span>
 3rd Qu.:<span class="fl">29.00</span>   3rd Qu.:<span class="dv">11-09-09</span>   AnjieNet          :<span class="st">   </span><span class="dv">39</span>
 Max.   :<span class="fl">39.00</span>   Max.   :<span class="dv">12-11-14</span>   beachblonde2244   :<span class="st">   </span><span class="dv">39</span>
                                    (Other)           :<span class="dv">23915</span></code></pre>
<p>Unlike <em>MoR</em>, multiple people have reviewed all (39) chapters. Let‚Äôs look at total reviews posted each calendar day:</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">table</span>(data$Date))</code></pre>
<p>We‚Äôd like to see what a running or cumulative total reviews looks like: how fast is <em>UC</em>‚Äôs total review-count growing these days?</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">cumsum</span>(<span class="kw">table</span>(data$Date)))</code></pre>
<figure>
<img alt="Total reviews as a function of time" height="818" src="./images/hpmor/uc-runningtotal.png" width="1506"/><figcaption>Total reviews as a function of time</figcaption>
</figure>
<p>Visually, it‚Äôs not quite a straight line, not quite a quadratic‚Ä¶</p>
<figure>
<img alt="A graph of each day UC has existed against reviews left that day" height="733" src="./images/hpmor/uc-perday.png" width="1506"/><figcaption>A graph of each day UC has existed against reviews left that day</figcaption>
</figure>
<p>We also get a similar graph to <em>MoR</em> if we graph by chapter instead, with thick bars indicating reviews left at the ‚Äúlatest‚Äù chapter while the series was updating, and many reviews left at the final chapter as people binge through <em>UC</em>:</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="kw">plot</span>(data$Chapter,data$Date)</code></pre>
<figure>
<img alt="Total reviews on each chapter of UC" height="781" src="./images/hpmor/uc-perchapter.png" width="1547"/><figcaption>Total reviews on each chapter of UC</figcaption>
</figure>
<p>Let‚Äôs start modeling.</p>
</section>
<section id="linear-model" class="level4">
<h4>Linear model</h4>
<p>The linear doesn‚Äôt match well since it clearly is massively overpredicting future growth!</p>
<pre class="sourceCode R"><code class="sourceCode r">totals &lt;-<span class="st"> </span><span class="kw">cumsum</span>(<span class="kw">table</span>(data$Date))
days &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>:(<span class="kw">length</span>(totals)))
<span class="kw">plot</span>(days, totals)

model &lt;-<span class="st"> </span><span class="kw">lm</span>(totals ~<span class="st"> </span>days)
<span class="kw">abline</span>(model)</code></pre>
<figure>
<img alt="A graph of total reviews vs time with a (bad) linear fit overlaid" height="900" src="./images/hpmor/uc-linear.png" width="1600"/><figcaption>A graph of total reviews vs time with a (bad) linear fit overlaid</figcaption>
</figure>
</section>
<section id="quadratic-model" class="level4">
<h4>Quadratic model</h4>
<p>Well, the graph does look more like a quadratic, but this has another problem:</p>
<pre class="sourceCode R"><code class="sourceCode r">model &lt;-<span class="st"> </span><span class="kw">lm</span>(totals ~<span class="st"> </span>days  +<span class="st"> </span><span class="kw">I</span>(days^<span class="dv">2</span>))
xx &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(days),<span class="kw">max</span>(days),<span class="dt">len=</span><span class="dv">200</span>)
yy &lt;-<span class="st"> </span>model$coef %*%<span class="st"> </span><span class="kw">rbind</span>(<span class="dv">1</span>,xx,xx^<span class="dv">2</span>)
<span class="kw">plot</span>(days, totals)
<span class="kw">lines</span>(xx,yy,<span class="dt">lwd=</span><span class="dv">2</span>,<span class="dt">col=</span><span class="dv">3</span>)</code></pre>
<figure>
<img alt="Same graph, better (but still bad) quadratic fit overlaid" height="900" src="./images/hpmor/uc-quadratic.png" width="1600"/><figcaption>Same graph, better (but still bad) quadratic fit overlaid</figcaption>
</figure>
<p>It fits much better overall, but for our purposes it is still rubbish: it is now predicting <em>negative</em> growth in total review count (a parabola must come down, after all), which of course cannot happen since reviews are not deleted.</p>
</section>
<section id="logarithmic-model" class="level4">
<h4>Logarithmic model</h4>
<p>We need a logarithmic function to try to model this ‚Äúleveling-off‚Äù behavior:</p>
<pre class="sourceCode R"><code class="sourceCode r">logEstimate =<span class="st"> </span><span class="kw">lm</span>(totals ~<span class="st"> </span><span class="kw">log</span>(days))
<span class="kw">plot</span>(days,<span class="kw">predict</span>(logEstimate),<span class="dt">type=</span><span class="st">'l'</span>,<span class="dt">col=</span><span class="st">'blue'</span>)
<span class="kw">points</span>(days,totals)</code></pre>
<figure>
<img alt="I‚Äôm a logarithm, and I‚Äôm OK‚Ä¶" height="787" src="./images/hpmor/uc-logarithmic.png" width="1509"/><figcaption>I‚Äôm a logarithm, and I‚Äôm OK‚Ä¶</figcaption>
</figure>
<p>The fit is very bad at the start, but the part we care about, recent reviews, seems better modeled than before. Progress! With a working model, let‚Äôs ask the original question: with these models predicting growth for <em>UC</em> and <em>MoR</em>, when do they finally have an equal number of reviews? We can use the <code>predict.lm</code> function to help us</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="co"># for MoR</span>
<span class="kw">predict.lm</span>(logEstimate, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">days =</span> <span class="kw">c</span>(<span class="dv">10000</span>:<span class="dv">20000</span>)))
...
    <span class="dv">9993</span>     <span class="dv">9994</span>     <span class="dv">9995</span>     <span class="dv">9996</span>     <span class="dv">9997</span>     <span class="dv">9998</span>     <span class="dv">9999</span>    <span class="dv">10000</span>
<span class="fl">31781.54</span> <span class="fl">31781.78</span> <span class="fl">31782.03</span> <span class="fl">31782.27</span> <span class="fl">31782.52</span> <span class="fl">31782.76</span> <span class="fl">31783.01</span> <span class="fl">31783.25</span>
   <span class="dv">10001</span>
<span class="fl">31783.50</span>
<span class="co"># for UC</span>
<span class="kw">predict.lm</span>(logEstimate, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">days =</span> <span class="kw">c</span>(<span class="dv">10000</span>:<span class="dv">20000</span>)))
...
    <span class="dv">9993</span>     <span class="dv">9994</span>     <span class="dv">9995</span>     <span class="dv">9996</span>     <span class="dv">9997</span>     <span class="dv">9998</span>     <span class="dv">9999</span>    <span class="dv">10000</span>
<span class="fl">50129.51</span> <span class="fl">50129.88</span> <span class="fl">50130.26</span> <span class="fl">50130.64</span> <span class="fl">50131.02</span> <span class="fl">50131.39</span> <span class="fl">50131.77</span> <span class="fl">50132.15</span>
   <span class="dv">10001</span>
<span class="fl">50132.53</span></code></pre>
<p>Growth in reviews has asymptoted for each way out at 10,000 days or 27 years, but still <em>UC</em> &gt; <em>MoR</em>! This feels a little unlikely: the log function grows very slowly, but are the reviews really slowing down <em>that</em> much?</p>
</section>
<section id="linear-model-revisited" class="level4">
<h4>Linear model revisited</h4>
<p>Let‚Äôs ask a more restricted question - instead of trying to model the entire set of reviews, why not look at just the most recent chunk, the one sans any updates (for both, sadly), say the last 100 days?</p>
<pre class="sourceCode R"><code class="sourceCode r">totals &lt;-<span class="st"> </span><span class="kw">cumsum</span>(<span class="kw">table</span>(data$Date))
totals &lt;-<span class="st"> </span>totals[(<span class="kw">length</span>(totals)-<span class="dv">100</span>):<span class="kw">length</span>(totals)]
days &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>:(<span class="kw">length</span>(totals)))
days &lt;-<span class="st"> </span>days[(<span class="kw">length</span>(days)-<span class="dv">100</span>):<span class="kw">length</span>(days)]
<span class="kw">plot</span>(days, totals)
model &lt;-<span class="st"> </span><span class="kw">lm</span>(totals ~<span class="st"> </span>days)
<span class="kw">summary</span>(model)
...
Residuals:
<span class="st">     </span>Min       1Q   Median       3Q      Max
-<span class="fl">18.4305</span>  -<span class="fl">4.4631</span>   <span class="fl">0.2601</span>   <span class="fl">7.4555</span>  <span class="fl">16.0125</span>

Coefficients:
<span class="st">             </span>Estimate Std. Error  t value <span class="kw">Pr</span>(&gt;<span class="er">|</span>t|)
(Intercept) <span class="fl">2.392e+04</span>  <span class="fl">1.769e+00</span> <span class="fl">13517.26</span>   &lt;<span class="fl">2e-16</span> **<span class="er">*</span>
days        <span class="fl">2.472e+00</span>  <span class="fl">3.012e-02</span>    <span class="fl">82.08</span>   &lt;<span class="fl">2e-16</span> **<span class="er">*</span>
---
Signif. codes:<span class="st">  </span><span class="dv">0</span> ‚Äò**<span class="er">*</span>‚Äô <span class="fl">0.001</span> ‚Äò**‚Äô <span class="fl">0.01</span> ‚Äò*‚Äô <span class="fl">0.05</span> ‚Äò.‚Äô <span class="fl">0.1</span> ‚Äò ‚Äô <span class="dv">1</span>

Residual standard error:<span class="st"> </span><span class="fl">8.826</span> on <span class="dv">99</span> degrees of freedom
Multiple R-squared:<span class="st"> </span><span class="fl">0.9855</span>, Adjusted R-squared:<span class="st"> </span><span class="fl">0.9854</span>
F-statistic:<span class="st">  </span><span class="dv">6737</span> on <span class="dv">1</span> and <span class="dv">99</span> DF,  p-value:<span class="st"> </span><span class="er">&lt;</span><span class="st"> </span><span class="fl">2.2e-16</span>

<span class="kw">abline</span>(model)</code></pre>
<figure>
<img alt="A linear fit to the last 100 days of UC reviews" height="803" src="./images/hpmor/uc-runningtotal-linear-100.png" width="1530"/><figcaption>A linear fit to the last 100 days of UC reviews</figcaption>
</figure>
<p>That‚Äôs an impressive fit: visually a great match, relatively small residuals, extreme statistical significance. (We saw a similar graph for <em>MoR</em> too.) So clearly the linear model is accurately describing recent reviews.</p>
<section id="predicting" class="level5">
<h5>Predicting</h5>
<p>Let‚Äôs use 2 models like that to make our predictions. We‚Äôll extract the 2 coefficients which define a <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><mi>a</mi><mi>x</mi><mo>+</mo><mi>c</mi></mrow></math> linear model, and solve for the equality:</p>
<pre class="sourceCode R"><code class="sourceCode r"><span class="co"># UC:</span>
<span class="kw">coefficients</span>(model)
 (Intercept)         days
<span class="fl">23918.704158</span>     <span class="fl">2.472312</span>
<span class="co"># MoR:</span>
<span class="kw">coefficients</span>(model)
 (Intercept)         days
<span class="fl">18323.275842</span>     <span class="fl">5.285603</span></code></pre>
<p>The important thing here is 2.4723 vs 5.2856: that‚Äôs how many reviews <em>UC</em> is getting per day versus <em>MoR</em> over the last 100 days according to the linear models which fit very well in that time period. Where do they intercept each other, or to put it in our middle-school math class terms, where do they equal the same <em>y</em> and thus equal each other?</p>
<ol type="1">
<li><em>UC</em>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><mn>2</mn><mo>.</mo><mn>472312</mn><mi>x</mi><mo>+</mo><mn>23918</mn><mo>.</mo><mn>704158</mn></mrow></math></li>
<li><em>MoR</em>: <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>y</mi><mo>=</mo><mn>5</mn><mo>.</mo><mn>285603</mn><mi>x</mi><mo>+</mo><mn>18323</mn><mo>.</mo><mn>275842</mn></mrow></math></li>
</ol>
<p>We solve for <em>x</em>:</p>
<ol type="1">
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>5</mn><mo>.</mo><mn>285603</mn><mi>x</mi><mo>+</mo><mn>18323</mn><mo>.</mo><mn>275842</mn><mo>=</mo><mn>2</mn><mo>.</mo><mn>472312</mn><mi>x</mi><mo>+</mo><mn>23918</mn><mo>.</mo><mn>704158</mn></mrow></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>5</mn><mo>.</mo><mn>285603</mn><mi>x</mi><mo>=</mo><mn>2</mn><mo>.</mo><mn>472312</mn><mi>x</mi><mo>+</mo><mo stretchy="false">(</mo><mn>23918</mn><mo>.</mo><mn>704158</mn><mo>‚àí</mo><mn>18323</mn><mo>.</mo><mn>275842</mn><mo stretchy="false">)</mo></mrow></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>5</mn><mo>.</mo><mn>285603</mn><mi>x</mi><mo>=</mo><mn>2</mn><mo>.</mo><mn>472312</mn><mi>x</mi><mo>+</mo><mn>5595</mn><mo>.</mo><mn>428</mn></mrow></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mn>5</mn><mo>.</mo><mn>285603</mn></mrow><mrow><mn>2</mn><mo>.</mo><mn>472312</mn></mrow></mfrac><mi>x</mi><mo>=</mo><mi>x</mi><mo>+</mo><mfrac><mrow><mn>5595</mn><mo>.</mo><mn>428</mn></mrow><mrow><mn>2</mn><mo>.</mo><mn>472312</mn></mrow></mfrac></mrow></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>2</mn><mo>.</mo><mn>137919</mn><mi>x</mi><mo>=</mo><mi>x</mi><mo>+</mo><mn>2263</mn><mo>.</mo><mn>237</mn></mrow></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>2</mn><mo>.</mo><mn>137919</mn><mi>x</mi><mo>‚àí</mo><mn>1</mn><mi>x</mi><mo>=</mo><mn>2263</mn><mo>.</mo><mn>237</mn></mrow></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mn>1</mn><mo>.</mo><mn>137919</mn><mi>x</mi><mo>=</mo><mn>2263</mn><mo>.</mo><mn>237</mn></mrow></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo>=</mo><mfrac><mrow><mn>2263</mn><mo>.</mo><mn>237</mn></mrow><mrow><mn>1</mn><mo>.</mo><mn>137919</mn></mrow></mfrac></mrow></math></li>
<li><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mi>x</mi><mo>=</mo><mn>1988</mn><mo>.</mo><mn>926</mn></mrow></math> days, or <math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><mrow><mfrac><mrow><mn>1988</mn><mo>.</mo><mn>926</mn></mrow><mrow><mn>365</mn><mo>.</mo><mn>25</mn></mrow></mfrac><mo>=</mo><mn>5</mn><mo>.</mo><mn>445383</mn></mrow></math> years</li>
</ol>
<p>Phew! That‚Äôs a long time. But it‚Äôs not ‚Äúnever‚Äù (as it is in the logarithmic fits). With that in mind, I don‚Äôt predict that <em>MoR</em> will surpass <em>UC</em> <a href="http://predictionbook.com/predictions/10022">within a year</a>, but bearing in mind that 5.4 years isn‚Äôt too far off and I expect additional bursts of reviews as <em>MoR</em> updates, I consider it <a href="http://predictionbook.com/predictions/10023">possible within 2 years</a>.</p>
</section>
</section>
</section>
</section>
<section id="survival-analysis" class="level2">
<h2>Survival analysis</h2>
<blockquote>
<p>I compile additional reviews and perform a survival analysis finding a difference in retention between roughly the first half of <em>MoR</em> and the second half, which I attribute to the changed update schedule &amp; slower production in the second half.</p>
</blockquote>
<p>On 17 May 2013, having finished learning <a href="http://en.wikipedia.org/wiki/survival%20analysis" title="Wikipedia: survival analysis">survival analysis</a> &amp; practicing it on a real dataset in <a href="Google%20shutdowns">my analysis of Google shutdowns</a>, I decided to return to <em>MoR</em>. At that point, more than 7 months had passed since my previous analysis, and more reviews had accumulated (1256 pages‚Äô worth in November 2012, 1364 by May 2013), so I downloaded as before, processed into a clean CSV, and wound up with 20447 reviews by 7508 users.</p>
<p>I then consolidated the reviews into per-user records: for each reviewer, I extracted their username, the first chapter they reviewed, the date of the first review, the last chapter they reviewed, and the last review‚Äôs date. A great many reviewers apparently leave only 1 review ever. The sparsity of reviews presents a challenge for defining an event or ‚Äòdeath‚Äô, since to me the most natural definition of ‚Äòdeath‚Äô is ‚Äúnot having reviewed the latest chapter‚Äù: with such sparsity, a reviewer may still be a reader (the property of interest) without having left a review on the latest chapter. Specifically, of the 7508 users, only 182 have reviewed chapter 87. To deal with this, I arbitrarily make the cutoff ‚Äúany review of chapter 82 or more recent‚Äù: a great deal of plot happened in those 5 chapters, so I reason anyone who isn‚Äôt moved to comment (however briefly) on any of them may not be heavily engaged with the story.</p>
<p>As the days pass, the number of reviewers who are that ‚Äòold‚Äô will dwindle as they check out or unsubscribe or simply stop being as enthusiastic as they used to be. We can graph this decline over time (zooming in to account for the issue that only 35% of reviewers leave &gt;1 reviews ever); no particular pattern jumps out beyond being a <a href="http://en.wikipedia.org/wiki/Survivorship%20curve" title="Wikipedia: Survivorship curve">type III curve</a> (‚Äúgreatest mortality is experienced early on in life, with relatively low rates of death for those surviving this bottleneck. This type of curve is characteristic of species that produce a large number of offspring (see <a href="http://en.wikipedia.org/wiki/R%2FK%20selection%20theory" title="Wikipedia: R/K selection theory">r/K selection theory</a>)‚Äù):</p>
<figure>
<img alt="Age of reviewers against probability they will not leave another review" height="374" src="./images/hpmor/survival-overall.png" width="1384"/><figcaption>Age of reviewers against probability they will not leave another review</figcaption>
</figure>
<p>But this is plotting age of reviewers and has no clear connection to calendar time, which was the original hypothesis: that the past 2 years have damaged readership more than the earlier chapters which were posted more timely. In particular, chapter posting seems to have slowed down ~ch62 (published 2010-11-27). So, if we were to look at every user who seems to have begun reading <em>MoR</em> only after that date (as evidenced by their first review postdating ch62‚Äôs publication) and compare them to those reading before, do they have additional mortality (above and beyond the low mortality we would expect by being relatively latecomers to <em>MoR</em>, which is adjusted for by the survival curve)? The answer seems to be yes, the survival curves look different when we partition users this way:</p>
<figure>
<img alt="Different mortality curves for 2 groups of reviewers" height="445" src="./images/hpmor/survival-earlylate-split.png" width="1413"/><figcaption>Different mortality curves for 2 groups of reviewers</figcaption>
</figure>
<pre class="sourceCode R"><code class="sourceCode r">  n=<span class="st"> </span><span class="dv">7508</span>, number of events=<span class="st"> </span><span class="dv">6927</span>

           coef <span class="kw">exp</span>(coef) <span class="kw">se</span>(coef)    z <span class="kw">Pr</span>(&gt;<span class="er">|</span>z|)
LateTRUE <span class="fl">0.2868</span>    <span class="fl">1.3322</span>   <span class="fl">0.0247</span> <span class="fl">11.6</span>   &lt;<span class="fl">2e-16</span>

         <span class="kw">exp</span>(coef) <span class="kw">exp</span>(-coef) lower .<span class="dv">95</span> upper .<span class="dv">95</span>
LateTRUE      <span class="fl">1.33</span>      <span class="fl">0.751</span>      <span class="fl">1.27</span>       <span class="fl">1.4</span></code></pre>
<p>A pretty clear difference between groups, and an <a href="http://en.wikipedia.org/wiki/odds%20ratio" title="Wikipedia: odds ratio">odds ratio</a> of 1.33 (<a href="http://en.wikipedia.org/wiki/Bootstrapping%20%28statistics%29" title="Wikipedia: Bootstrapping (statistics)">bootstrap</a> 95% CI: 1.26-1.39) is not trivially small. I think we can consider the original hypothesis supported to some degree: the later chapters, either because of the update schedule or perhaps the change in subject matter or attracting a different audience or some other reason, seem to be more likely to lose its readers.</p>
<p>But two further minor observations:</p>
<ul>
<li>Visually, the two groups of early &amp; late reviewers look like they might be converging after several years of aging. We might try to interpret this as a ‚Äòhardcore‚Äô vs ‚Äòpopulist‚Äô distinction: the early reviewers tend to all be early adopters hardcore geeks who become devoted fans in for the long haul, while the latecoming regular people get gradually turned off and only the remaining geeks continue to review.</li>
<li><p>Can we more specifically nail down this increased mortality to the interval between a reviewer‚Äôs last review and the next chapter? So far, no; there is actually a trivially small effect the other direction when we add that as a variable:</p>
<pre class="sourceCode R"><code class="sourceCode r">  n=<span class="st"> </span><span class="dv">7326</span>, number of events=<span class="st"> </span><span class="dv">6927</span>
   (<span class="dv">182</span> observations deleted due to missingness)

              coef <span class="kw">exp</span>(coef)  <span class="kw">se</span>(coef)     z <span class="kw">Pr</span>(&gt;<span class="er">|</span>z|)
LateTRUE  <span class="fl">0.433604</span>  <span class="fl">1.542808</span>  <span class="fl">0.024906</span>  <span class="fl">17.4</span>   &lt;<span class="fl">2e-16</span>
Delay    -<span class="fl">0.004236</span>  <span class="fl">0.995773</span>  <span class="fl">0.000262</span> -<span class="fl">16.2</span>   &lt;<span class="fl">2e-16</span>

         <span class="kw">exp</span>(coef) <span class="kw">exp</span>(-coef) lower .<span class="dv">95</span> upper .<span class="dv">95</span>
LateTRUE     <span class="fl">1.543</span>      <span class="fl">0.648</span>     <span class="fl">1.469</span>     <span class="fl">1.620</span>
Delay        <span class="fl">0.996</span>      <span class="fl">1.004</span>     <span class="fl">0.995</span>     <span class="fl">0.996</span></code></pre>
<p>I interpret this as indicating that an update which is posted after a long drought may be a <em>tiny</em> bit (95% CI: 0.9953-0.9963) more likely to elicit some subsequent reviews (perhaps because extremely long delays are associated with posting multiple chapters quickly), but nevertheless, that overall period with long delays is associated with increased reviewer loss. This makes some post hoc intuitive sense as a polarization effect: if a franchise after a long hiatus finally releases an exciting new work, you may be more likely to discuss or review it, even as the hiatuses make you more likely to finally ‚Äúcheck out‚Äù for good - you‚Äôll either engage or finally abandon it, but you probably won‚Äôt be indifferent.</p></li>
</ul>
<section id="source-code" class="level3">
<h3>Source code</h3>
<p>Generation and pre-processing, using the shell &amp; Haskell scripts from before:</p>
<pre class="sourceCode R"><code class="sourceCode r">data &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;2013-hpmor-rawreviews.csv&quot;</span>, <span class="dt">colClasses=</span><span class="kw">c</span>(<span class="st">&quot;integer&quot;</span>, <span class="st">&quot;Date&quot;</span>, <span class="st">&quot;factor&quot;</span>))
users &lt;-<span class="st"> </span><span class="kw">unique</span>(data$User)
lifetimes &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">User=</span><span class="ot">NA</span>, <span class="dt">Start=</span><span class="ot">NA</span>, <span class="dt">End=</span><span class="ot">NA</span>, <span class="dt">FirstChapter=</span><span class="ot">NA</span>, <span class="dt">LastChapter=</span><span class="ot">NA</span>)[-<span class="dv">1</span>,]
for (i in <span class="dv">1</span>:<span class="kw">length</span>(users)) { <span class="co"># isolate a specific user</span>
                  d &lt;-<span class="st"> </span>data[data$User ==<span class="st"> </span>users[i],]
                  <span class="co"># sort by date</span>
                  d &lt;-<span class="st"> </span>d[<span class="kw">order</span>(d$Date),]

                  first &lt;-<span class="st"> </span><span class="kw">head</span>(d, <span class="dt">n=</span><span class="dv">1</span>)
                  last &lt;-<span class="st"> </span><span class="kw">tail</span>(d, <span class="dt">n=</span><span class="dv">1</span>)
                  <span class="co"># build &amp; store new row (per user)</span>
                  new &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">User=</span>users[i], <span class="dt">Start=</span>first$Date, <span class="dt">End=</span>last$Date,
                                    <span class="dt">FirstChapter=</span>first$Chapter, <span class="dt">LastChapter=</span>last$Chapter)
                  lifetimes &lt;-<span class="st"> </span><span class="kw">rbind</span>(lifetimes, new)
}
lifetimes$Days &lt;-<span class="st"> </span>lifetimes$End -<span class="st"> </span>lifetimes$Start
<span class="kw">summary</span>(lifetimes$Days)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
    <span class="fl">0.0</span>     <span class="fl">0.0</span>     <span class="fl">0.0</span>    <span class="fl">55.2</span>     <span class="fl">6.0</span>  <span class="fl">1160.0</span>
lifetimes$Dead &lt;-<span class="st"> </span>lifetimes$LastChapter &lt;=<span class="st"> </span><span class="dv">82</span>

lifetimes$Late &lt;-<span class="st"> </span>lifetimes$Start &gt;=<span class="st"> </span><span class="kw">as.Date</span>(<span class="st">&quot;10-11-27&quot;</span>)

<span class="co"># figure out when each chapter was written by its earliest review</span>
chapterDates &lt;-<span class="st"> </span><span class="kw">as.Date</span>(<span class="ot">NA</span>)
for (i in <span class="dv">1</span>:<span class="kw">max</span>(data$Chapter)) { chapterDates[i] &lt;-<span class="st"> </span><span class="kw">min</span>(data[data$Chapter==i,]$Date) }

<span class="co"># how long after chapter X was posted did chapter X+1 come along? (obviously NA/unknown for latest chapter)</span>
intervals &lt;-<span class="st"> </span><span class="kw">integer</span>(<span class="dv">0</span>)
for (i in <span class="dv">1</span>:<span class="kw">length</span>(chapterDates)) { intervals[i] &lt;-<span class="st"> </span>chapterDates[i<span class="dv">+1</span>] -<span class="st"> </span>chapterDates[i] }

<span class="co"># look up for every user the relevant delay after their last review</span>
delay &lt;-<span class="st"> </span><span class="kw">integer</span>(<span class="dv">0</span>)
for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(lifetimes)) { delay[i] &lt;-<span class="st"> </span>intervals[lifetimes$LastChapter[i]] }
lifetimes$Delay &lt;-<span class="st"> </span>delay</code></pre>
<p>Now that we have a clean set of data with the necessary variables like dates, we can analyze:</p>
<pre class="sourceCode R"><code class="sourceCode r">lifetimes &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://www.gwern.net/docs/2013-hpmor-reviewers.csv&quot;</span>,
                       <span class="dt">colClasses=</span><span class="kw">c</span>(<span class="st">&quot;factor&quot;</span>, <span class="st">&quot;Date&quot;</span>, <span class="st">&quot;Date&quot;</span>, <span class="st">&quot;integer&quot;</span>, <span class="st">&quot;integer&quot;</span>,
                                    <span class="st">&quot;integer&quot;</span>, <span class="st">&quot;logical&quot;</span>, <span class="st">&quot;logical&quot;</span>,<span class="st">&quot;integer&quot;</span>))

<span class="kw">library</span>(survival)

surv &lt;-<span class="st"> </span><span class="kw">survfit</span>(<span class="kw">Surv</span>(Days, Dead, <span class="dt">type=</span><span class="st">&quot;right&quot;</span>) ~<span class="st"> </span><span class="dv">1</span>, <span class="dt">data=</span>lifetimes)

<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/hpmor/survival-overall.png&quot;</span>, <span class="dt">width =</span> <span class="dv">3</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">plot</span>(surv, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.35</span>))
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())

cmodel &lt;-<span class="st"> </span><span class="kw">coxph</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Late, <span class="dt">data=</span>lifetimes)
<span class="kw">print</span>(<span class="kw">summary</span>(cmodel))

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Test for assumption violation to confirm the Cox regression:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">print</span>(<span class="kw">cox.zph</span>(cmodel))

<span class="kw">png</span>(<span class="dt">file=</span><span class="st">&quot;~/wiki/images/hpmor/survival-earlylate-split.png&quot;</span>, <span class="dt">width =</span> <span class="dv">3</span>*<span class="dv">480</span>, <span class="dt">height =</span> <span class="dv">1</span>*<span class="dv">480</span>)
<span class="kw">plot</span>(<span class="kw">survfit</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Late, <span class="dt">data=</span>lifetimes),
     <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="fl">0.4</span>), <span class="dt">main=</span><span class="st">&quot;Reader survivorship: first review before or after chapter 62&quot;</span>,
     <span class="dt">ylab=</span><span class="st">&quot;Fraction of reader group surviving&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Days between reader's first &amp; last review&quot;</span>)
<span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Early readers&quot;</span>, <span class="st">&quot;Late readers&quot;</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span> ,<span class="dv">2</span>), <span class="dt">inset=</span><span class="fl">0.02</span>)
<span class="kw">invisible</span>(<span class="kw">dev.off</span>())

<span class="kw">print</span>(<span class="kw">summary</span>(<span class="kw">coxph</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Late +<span class="st"> </span>Delay, <span class="dt">data=</span>lifetimes)))

<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Begin bootstrap test of coefficient size...</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">library</span>(boot)
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Get a subsample, train Cox on it, extract coefficient estimate;</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="kw">cat</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">first the simplest early/late model, then the more complex:</span><span class="ch">\n</span><span class="st">&quot;</span>)
<span class="co"># TODO: factor out duplication</span>
coxCoefficient1 &lt;-<span class="st"> </span>function(gb, indices) {
  g &lt;-<span class="st"> </span>gb[indices,] <span class="co"># allows boot to select subsample</span>
  <span class="co"># train new regression model on subsample</span>
  cmodel &lt;-<span class="st"> </span><span class="kw">coxph</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Late, <span class="dt">data=</span>g)
  <span class="kw">return</span>(<span class="kw">coef</span>(cmodel))
}
cox1Bs &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>lifetimes, <span class="dt">statistic=</span>coxCoefficient1, <span class="dt">R=</span><span class="dv">20000</span>, <span class="dt">parallel=</span><span class="st">&quot;multicore&quot;</span>, <span class="dt">ncpus=</span><span class="dv">4</span>)
<span class="kw">print</span>(<span class="kw">boot.ci</span>(cox1Bs, <span class="dt">type=</span><span class="st">&quot;perc&quot;</span>))
coxCoefficient2 &lt;-<span class="st"> </span>function(gb, indices) {
  g &lt;-<span class="st"> </span>gb[indices,] <span class="co"># allows boot to select subsample</span>
  <span class="co"># train new regression model on subsample</span>
  cmodel &lt;-<span class="st"> </span><span class="kw">coxph</span>(<span class="kw">Surv</span>(Days, Dead) ~<span class="st"> </span>Late +<span class="st"> </span>Delay, <span class="dt">data=</span>g)
  <span class="kw">return</span>(<span class="kw">coef</span>(cmodel))
}
cox2Bs &lt;-<span class="st"> </span><span class="kw">boot</span>(<span class="dt">data=</span>lifetimes, <span class="dt">statistic=</span>coxCoefficient2, <span class="dt">R=</span><span class="dv">20000</span>, <span class="dt">parallel=</span><span class="st">&quot;multicore&quot;</span>, <span class="dt">ncpus=</span><span class="dv">4</span>)
<span class="kw">print</span>(<span class="kw">boot.ci</span>(cox2Bs, <span class="dt">type=</span><span class="st">&quot;perc&quot;</span>, <span class="dt">index=</span><span class="dv">1</span>)) <span class="co"># `Late`</span>
<span class="kw">print</span>(<span class="kw">boot.ci</span>(cox2Bs, <span class="dt">type=</span><span class="st">&quot;perc&quot;</span>, <span class="dt">index=</span><span class="dv">2</span>)) <span class="co"># `Delay`</span></code></pre>
 
</section>
</section>
</section>
<section id="see-also" class="level1">
<h1>See also</h1>
<ul>
<li><a href="hpmor-predictions">collected <em>MoR</em> plot predictions</a></li>
</ul>
</section>
<section id="external-links" class="level1">
<h1>External links</h1>
<ul>
<li><a href="http://lesswrong.com/r/discussion/lw/fag/analyzing_ffnet_reviews_of_harry_potter_and_the/#comments">LW discussion</a></li>
<li><a href="http://www.reddit.com/r/HPMOR/comments/12l6g6/ffnet_reviews_of_hpmor_statistics/">Reddit discussion</a>, <a href="http://www.reddit.com/r/HPMOR/comments/1ek6u3/ffnet_reviews_revisited_mortality_curves_for/">2</a></li>
<li><a href="http://www.reddit.com/r/HPMOR/comments/1h10vl/harry_potter_fanfiction_data_most_popular/">‚Äú[All] Harry Potter Fanfiction Data (Most popular characters, publishing rates, etc)‚Äù</a> -(scraping FF.net)</li>
</ul>
</section>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><p>The author points out that his <a href="http://www.fanfiction.net/s/7036128/1/Offside">‚ÄúOffside‚Äù</a> fanfiction is past 32,000 reviews.<a href="#fnref1">‚Ü©</a></p></li>
</ol>
</section>
</div>
</div>
<div id="footer">
<p>Still bored? Then try my <a href="https://plus.google.com/103530621949492999968/posts" title="Google+ posts">Google+ news feed</a>.</p>
<a href="https://docs.google.com/spreadsheet/viewform?formkey=dE5GLWpfX3RhX1c2Q1phcEo3U3VDVEE6MQ">Send anonymous feedback</a>
<br/>
<div id="license">
<p xmlns:dct="http://purl.org/dc/terms/" xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#">
<a rel="license" href="http://creativecommons.org/publicdomain/zero/1.0/">
<img src="http://i.creativecommons.org/p/zero/1.0/88x31.png" style="border-style: none;" alt="CC0" height="31" width="88"/>
</a>
</p>
</div>
</div>
 
<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
 
<script type="text/javascript" src="./static/js/footnotes.js"></script>
 
<script type="text/javascript" src="./static/js/abalytics.js"></script>
<script type="text/javascript">
      window.onload = function() {
      ABalytics.applyHtml();
      };
    </script>
 
<script id="googleAnalytics" type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-18912926-1']);

      ABalytics.init({
      indent: [
      {
      name: "none",
      "indent_class1": "<style>p + p { text-indent: 0.0em; margin-top: 0 }</style>"
      },
      {
      name: "indent0.1",
      "indent_class1": "<style>p + p { text-indent: 0.1em; margin-top: 0 }</style>"
      },
      {
      name: "indent0.5",
      "indent_class1": "<style>p + p { text-indent: 0.5em; margin-top: 0 }</style>"
      },
      {
      name: "indent1.0",
      "indent_class1": "<style>p + p { text-indent: 1.0em; margin-top: 0 }</style>"
      },
      {
      name: "indent1.5",
      "indent_class1": "<style>p + p { text-indent: 1.5em; margin-top: 0 }</style>"
      },
      {
      name: "indent2.0",
      "indent_class1": "<style>p + p { text-indent: 2.0em; margin-top: 0 }</style>"
      }
      ],
      }, _gaq);

      _gaq.push(['_trackPageview']);
      (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
 
<script id="outboundLinkTracking" type="text/javascript">
      $(function() {
      $("a").on('click',function(e){
      var url = $(this).attr("href");
      if (e.currentTarget.host != window.location.host) {
      _gat._getTrackerByName()._trackEvent("Outbound Links", e.currentTarget.host.replace(':80',''), url, 0);
      if (e.metaKey || e.ctrlKey || (e.button == 1)) {
      var newtab = true;
      }
      if (!newtab) {
      e.preventDefault();
      setTimeout('document.location = "' + url + '"', 100);
      }
      }
      });
      });
    </script>
 
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
 
<script type="text/javascript" src="./static/js/footnotes.js"></script>
 
<script type="text/javascript" src="./static/js/tablesorter.js"></script>
<script type="text/javascript" id="tablesorter">
      $(document).ready(function() {
      $("table").tablesorter();
      }); </script>
 
<div id="disqus_thread"></div>
<script type="text/javascript">
      if (document.title != 'Essays') { <!-- avoid Disqus comments on front page -->
      (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://disqus.com/forums/gwern/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
      var disqus_shortname = 'gwern';
      (function () {
      var s = document.createElement('script'); s.async = true;
      s.src = 'http://disqus.com/forums/gwern/count.js';
      (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
      }</script>
<noscript><p>Enable JavaScript for Disqus comments</p></noscript>
</body>
</html>

