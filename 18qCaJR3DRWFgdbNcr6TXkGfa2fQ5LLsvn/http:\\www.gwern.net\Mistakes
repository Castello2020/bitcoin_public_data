http://www.gwern.net/Mistakes
HTTP/1.1 200 OK
Server: cloudflare-nginx
Date: Tue, 22 Jul 2014 19:34:32 GMT
Content-Type: text/html; charset=utf-8
Connection: close
Set-Cookie: __cfduid=d91b7385bc21ddce2a1b73efda69ad6f71406057672538; expires=Mon, 23-Dec-2019 23:50:00 GMT; path=/; domain=.gwern.net; HttpOnly
x-amz-id-2: cPnTx+kbftjgrx+FkWyYF3f+kCaCB9Dxy1+tlsmjmI1KsMtZmGG9rTuMQmoaxuF+ZuIAwYaOLOQ=
x-amz-request-id: D91E5CEF8DBCB3A0
x-amz-meta-s3cmd-attrs: uid:1000/gname:gwern/uname:gwern/gid:1000/mode:33152/mtime:1405785763/atime:1405785760/ctime:1405785763
Cache-Control: max-age=604800, public
Last-Modified: Sat, 19 Jul 2014 16:10:14 GMT
CF-RAY: 14e21385504e0097-IAD
Content-Encoding: gzip

<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8"/>
<meta name="generator" content="hakyll"/>
<meta name="google-site-verification" content="BOhOQI1uMfsqu_DopVApovk1mJD5ZBLfan0s9go3phk"/>
<meta name="author" content="gwern"/>
<meta name="description" content="Things I have changed my mind about."/>
<meta name="dc.date.issued" content="15 Sep 2011"/>
<meta name="dcterms.modified" content="18 Jul 2014"/>
<title>My Mistakes</title>
<link rel="stylesheet" type="text/css" href="./static/css/default.css"/>
<link href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed"/>
<link rel="shortcut icon" type="image/x-icon" href="./static/img/favicon.ico"/>
</head>
<body>
 
<div class="indent_class1"></div>
<div id="main">
<div id="sidebar">
<div id="logo"><img alt="Logo: a Gothic/Fraktur blackletter capital G/ùï≤" height="36" src="./images/logo.png" width="32"/></div>
<div id="sidebar-links">
<p>
<a href="./index" title="index: categorized list of articles">Home</a>
<a href="./About" title="Site ideals, source, content, traffic, examples, license">Site</a>
<a href="./Links" title="Who am I online, what have I done, what am I like? Contact information; sites I use; things I've worked on">Me</a>
</p>
<hr/>
<div id="sidebar-news">
<p>
<a href="./Changelog" title="What's new or updated">New:</a>
<a href="./atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM/RSS Feed">RSS</a>
<a href="http://eepurl.com/Kc155" title="Monthly mailing list: signup form">MAIL</a>
</p>
<hr/>
</div>
<div id="cse-sitesearch">
<script>
            (function() {
            var cx = '009114923999563836576:dv0a4ndtmly';
            var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
            gcse.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//www.google.com/cse/cse.js?cx=' + cx;
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
            })();
          </script>
<div style="width:0px;overflow:hidden;height:0px;">
<gcse:search></gcse:search>
</div>
<form id="searchbox_009114923999563836576:dv0a4ndtmly">
<input value="009114923999563836576:dv0a4ndtmly" name="cx" type="hidden"/>
<input value="FORID:11" name="cof" type="hidden"/>
<input id="q" style name="q" size="5" type="text" placeholder="search"/>
</form>
</div>
</div>
<hr/>
<div id="metadata">
<div id="abstract"><em>Things I have changed my mind about.</em></div>
<br/>
<div id="tags"><i><a href="./tags/personal">personal</a>, <a href="./tags/philosophy">philosophy</a>, <a href="./tags/predictions">predictions</a>, <a href="./tags/Bitcoin">Bitcoin</a></i></div>
<br/>
<div id="page-created">created:
<br/>
<i>15 Sep 2011</i></div>
<div id="last-modified">modified:
<br/>
<i>18 Jul 2014</i></div>
<br/>
<div id="version">status:
<br/>
<i>in progress</i></div>
<br/>
<div id="epistemological-status"><a href="./About#belief-tags" title="Explanation of 'belief' metadata">belief:</a>
<br/>
<i>unlikely</i>
</div>
<hr/>
</div>
<div id="donations">
<div id="bitcoin-donation-address">
<a href="http://en.wikipedia.org/wiki/Bitcoin">‡∏ø</a>: 18qCaJR3DRWFgdbNcr6TXkGfa2fQ5LLsvn
</div>
<div id="paypal">
<form style="display: inline" action="https://www.paypal.com/cgi-bin/webscr" method="post" onClick="_gaq.push(['_trackEvent', 'Click', 'PayPalClicked', '']);">
<div class="form-type">
<input type="hidden" name="cmd" value="_s-xclick"/>
<input type="hidden" name="hosted_button_id" value="8GSLCWGCC6AF8"/>
<input type="image" src="http://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif" name="submit" alt="Help support my writings!"/>
</div>
</form>
</div>
<div id="Gittip">
<script data-gittip-username="gwern" data-gittip-widget="button" src="//gttp.co/v1.js"></script>
</div>
</div>
</div>
 
<div id="adsense">
<a href="http://41j.com/ads/ad.html"><img alt="Advertisement for 'HTerm, The Graphical Terminal'" src="http://41j.com/ads/ad.png" height="90" width="728"></a>
</div>
<div id="header">
<h1>My Mistakes</h1>
</div>
<div id="content">
<div id="TOC"><ul>
<li><a href="#changes">Changes</a><ul>
<li><a href="#religion">Religion</a></li>
<li><a href="#the-american-revolution">The American Revolution</a></li>
<li><a href="#communism">Communism</a></li>
<li><a href="#the-occult">The Occult</a></li>
<li><a href="#fiction">Fiction</a></li>
<li><a href="#nicotine">Nicotine</a></li>
<li><a href="#centralized-black-markets">Centralized black-markets</a></li>
</ul></li>
<li><a href="#potential-changes">Potential changes</a><ul>
<li><a href="#near-singularity">Near Singularity</a><ul>
<li><a href="#counter-point">Counter-point</a></li>
</ul></li>
<li><a href="#neo-luddism">Neo-Luddism</a><ul>
<li><a href="#evaluation">Evaluation</a></li>
<li><a href="#external-links">External links</a></li>
</ul></li>
<li><a href="#iq-race">IQ &amp; race</a><ul>
<li><a href="#mu"><em>Mu</em></a></li>
<li><a href="#value-of-information">Value of Information</a></li>
<li><a href="#further-reading">Further reading</a></li>
</ul></li>
</ul></li>
<li><a href="#see-also">See Also</a></li>
<li><a href="#appendix">Appendix</a><ul>
<li><a href="#miller-on-neo-luddism">Miller on neo-Luddism</a></li>
</ul></li>
</ul></div>
<blockquote>
<p>One does not care to acknowledge the mistakes of one‚Äôs youth.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
</blockquote>
<p>It is <a href="http://lesswrong.com/r/discussion/lw/94t/meta_analysis_of_writing_therapy/#thingrow_t3_94t">salutary for the soul</a> to review past events and perhaps keep a list of things one no longer believes, since <a href="http://lesswrong.com/lw/ur/crisis_of_faith/">such crises</a> are rare<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> and so easily pass from memory (there is no feeling of <em>being</em> wrong, only of having <em>been</em> wrong<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>). One does not need an <a href="http://lesswrong.com/lw/us/the_ritual/">elaborate ritual</a> (fun as they are to read about) to change one‚Äôs mind, but the changes must happen. If you are not changing, you are not growing<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>; no one has won the belief lottery and has a monopoly on truth<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>. To the honest inquirer, all surprises are pleasant ones<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>.</p>
<section id="changes" class="level1">
<h1>Changes</h1>
<blockquote>
<p>Only the most clever and the most stupid cannot change.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
</blockquote>
<p>This list is not for specific facts of which there are too many to record, nor is it for falsified predictions like my belief that George W. Bush would not be elected (for those see <a href="Prediction%20markets" title="Go to wiki page: Prediction%20markets">Prediction markets</a> or my <a href="http://predictionbook.com/users/gwern">PredictionBook.com page</a>), nor mistakes in my private life (which go into a private file), nor things I never had an initial strong position on (Windows vs Linux, Java vs Haskell). The following are some major ideas or sets of ideas that I have changed my mind about:</p>
<section id="religion" class="level2">
<h2>Religion</h2>
<blockquote>
<p>For I count being refuted a greater good, insofar as it is a greater good to be rid of the greatest evil from oneself than to rid someone else of it. I don‚Äôt suppose that any evil for a man is as great as false belief about the things we‚Äôre discussing right now‚Ä¶<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
</blockquote>
<p>I think religion was the first subject in my life that I took seriously. As best as I can recall at this point, I have no ‚Äúdeconversion story‚Äù or tale to tell, since I don‚Äôt remember ever seriously believing<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> - the stories in the Bible or at my Catholic church were interesting, but they were obviously fiction to some degree. I wasn‚Äôt going to reject religion out of hand because some of the stories were made-up (any more than I believed George Washington didn‚Äôt exist because the story of him chopping down an apple tree was made-up), but the big claims didn‚Äôt seem to be panning out either:</p>
<ol type="1">
<li>My prayers received no answers of any kind, not even a voice in my head</li>
<li>I didn‚Äôt see any miracles or intercessions like I expected from a omnipotent loving god</li>
</ol>
<p>The latter was probably due to the cartoons I watched on TV, which seemed quite sensible to me: a powerful figure like a god would act in all sorts of ways. If there really was a god, that was something that ought to be quite obvious to anyone who ‚Äòhad eyes to see‚Äô. I had more evidence that China existed than did God, which seemed backwards. Explanations for the absence of divine action ranged from the strained to so ludicrously bad that they corroded what little faith I possessed<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a>. I would later recognize my own doubts in passages of skeptical authors like Edward Gibbon and his <em>Decline</em>:</p>
<blockquote>
<p>‚Ä¶From the first of the fathers to the last of the popes, a succession of bishops, of saints, of martyrs, and of miracles, is continued without interruption; and the progress of superstition was so gradual, and almost imperceptible, that we know not in what particular link we should break the chain of tradition. Every age bears testimony to the wonderful events by which it was distinguished, and its testimony appears no less weighty and respectable than that of the preceding generation, till we are insensibly led on to accuse our own inconsistency, if in the eighth or in the twelfth century we deny to the venerable Bede, or to the holy Bernard, the same degree of confidence which, in the second century, we had so liberally granted to Justin or to Irenaeus. If the truth of any of those miracles is appreciated by their apparent use and propriety, every age had unbelievers to convince, heretics to confute, and idolatrous nations to convert; and sufficient motives might always be produced to justify the interposition of Heaven. And yet, since every friend to revelation is persuaded of the reality, and every reasonable man is convinced of the cessation, of miraculous powers, it is evident that there must have been some period in which they were either suddenly or gradually withdrawn from the Christian church. Whatever aera is chosen for that purpose, the death of the apostles, the conversion of the Roman empire, or the extinction of the Arian heresy, the insensibility of the Christians who lived at that time will equally afford a just matter of surprise. They still supported their pretensions after they had lost their power. Credulity performed the office of faith; fanaticism was permitted to assume the language of inspiration, and the effects of accident or contrivance were ascribed to supernatural causes. The recent experience of genuine miracles should have instructed the Christian world in the ways of Providence, and habituated their eye (if we may use a very inadequate expression) to the style of the divine artist‚Ä¶Whatever opinion may be entertained of the miracles of the primitive church since the time of the apostles, this unresisting softness of temper, so conspicuous among the believers of the second and third centuries, proved of some accidental benefit to the cause of truth and religion. In modern times, a latent and even involuntary scepticism adheres to the most pious dispositions. Their admission of supernatural truths is much less an active consent than a cold and passive acquiescence.</p>
</blockquote>
<p>I have seen these reasons <a href="http://www.firstthings.com/article/2010/04/believe-it-or-not">mocked as simplistic and puerile</a>, and I was certainly aware that there were subtle arguments which intelligent philosophers believed resolved the <a href="http://en.wikipedia.org/wiki/theodicy" title="Wikipedia: theodicy">theodicy</a> (such as <a href="http://en.wikipedia.org/wiki/Alvin%20Plantinga%27s%20free%20will%20defense" title="Wikipedia: Alvin Plantinga‚Äôs free will defense">Alvin Plantinga‚Äôs free will defense</a>, which is valid but I do not consider it sound since it requires the meaningless concept of free will) and that Christians of various stripes had various complicated explanations for why this world was consistent with there being a God (if for no other reason than that I observed there were theists as intelligent or more intelligent than me). But the basic concept seemed confused, free will was an even more dubious plank to go on, and in general the entire complex of historical claims, metaphysics, and activities of religious people did not seem convincing. (Richard Carrier‚Äôs 2011 <a href="http://www.amazon.com/Why-Am-Not-Christian-Conclusive/dp/1456588850/?tag=gwernnet-20"><em>Why I Am Not A Christian</em></a> expresses the general tenor of my misgivings, especially after I checked out everything the library had on <a href="http://en.wikipedia.org/wiki/higher%20Biblical%20criticism" title="Wikipedia: higher Biblical criticism">higher Biblical criticism</a>, <a href="http://en.wikipedia.org/wiki/Josephus" title="Wikipedia: Josephus">Josephus</a>, the Gnostics, and early Christianity - <em>Je n‚Äôavais pas besoin de cette hypoth√®se-l√†</em>, basically.)</p>
<p>So I never believed (although it was obvious enough that there was no point in discussing this since it might just lead to me going to church more and sitting on the hard wooden pews), but there was still the troubling matter of Heaven &amp; Hell: those infinities meant I couldn‚Äôt simply dismiss religion and continue reading about dinosaurs or Alcatraz. If I got religion wrong, I would have gotten literally the most important possible thing wrong! Nothing else was as important - if you‚Äôre wrong about a round earth, at worst you will never be a good geographer or astronomer; if you‚Äôre wrong about believing in astrology, at worst you waste time and money; if you‚Äôre wrong about evolution and biology, at worst you endanger your life; and so on. But if you‚Äôre wrong about religion, wasting your life is about the least of the consequences. And <em>everyone</em> accepts a religion or at least the legitimacy of religious claims, so it would be unspeakably arrogant of a kid to dismiss religion entirely - the evidence is simply not there<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>. (Oddly enough, atheists - who are not immediately shown to <a href="http://tvtropes.org/pmwiki/pmwiki.php/Main/FlatEarthAtheist">be mistaken</a> or <a href="http://tvtropes.org/pmwiki/pmwiki.php/Main/HollywoodAtheist">fools</a> - are even rarer in books and cartoons than they are in real life.)</p>
<p>Kids actually are kind of skeptical if they have reason to be skeptical, and likewise will believe all sorts of strange things if the source was previously trustworthy<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a>. This is as it should be! Kids cannot come prewired with 100% correct beliefs, and must be able to learn all sorts of strange (but true) things from reliable authorities; these strategies are exactly what one would advise. It is not their fault that some of the most reliable authorities in their lives (their parents) are mistaken about one major set of beliefs. They simply have bad <a href="http://www.iep.utm.edu/epi-luck/" title="Internet Encyclopedia of Philosophy">epistemic luck</a>.</p>
<p>So I read the Bible, which veered from boring to incoherent to disgusting. (I became a fan of the <a href="http://en.wikipedia.org/wiki/Wisdom%20literature" title="Wikipedia: Wisdom literature">Wisdom literature</a>, however, and still periodically read the Book of Job, Ecclesiastes, and Proverbs.) That didn‚Äôt help much. Well, maybe Christianity was not the right religion? My elementary school library had a rather strange selection of books which included various Eastern texts or anthologies (I remember in particular one anthology on meditation, which was a hodge-podge of religious instruction manuals, essays, and scientific studies on meditation - that took me a long time to read, and it was only in high school and college that I really became comfortable reading psychology papers). I continued reading in this vein for years, in between all my more normal readings. The Koran was interesting and in general much better than the Bible. Shinto texts were worthless mythologizing. Taoism had some very good early texts (the <em>Chuang-tzu</em> in particular) but then bizarrely degenerated into alchemy. Buddhism was strange: I rather liked the general philosophical approach, but there were many populist elements in Mahayana texts that bothered me. Hinduism had a strange beauty, but my reaction was similar to that of the early translators, who condemned it for sloth and lassitude. I also considered the Occult seriously and began reading the Skeptical literature on that and related topics (see the <a href="#the-occult">later section</a>).</p>
<p>By this point in my reading, I had reached middle school; this summary makes my reading sound more systematic than it was. I still hadn‚Äôt found any especially good reason to believe in God or any gods, and had a jaundiced view of many texts I had read. <a href="http://en.wikipedia.org/wiki/Higher%20Biblical%20criticism" title="Wikipedia: Higher Biblical criticism">Higher Biblical criticism</a> was a shock when I finally became capable of reading it and source-texts like Josephus: it‚Äôs amazing just how uncertain, variable, self-contradictory, edited, and historically inconsistent both the Old and New Testaments are. There are hundreds of major variants of the various books, there are countless thousands of textual variants (many of key theological passages) leaving traces of ideological fabrication throughout besides the casual falsification of many historical events for dramatic effect or faked coherence with Old Testament ‚Äòprophecies‚Äô (we all know of false claims like the Massacre of the Innocents or Jesus being born of a ‚Äòvirgin‚Äô to fit an erroneous translation or the sun stopping or the Temple veil being ripped or the Roman census) but it‚Äôs dramatic to find that Jesus was an utter nobody even in Josephus, where the only mention of Jesus seems to be falsified (by Christians of course) despite him name-dropping constantly - and speaking of Josephus, it‚Äôs hard not to be impressed that while one recites every Sunday how Jesus was ‚Äúcrucified under Pontius Pilate‚Äù as proof we are told of Jesus‚Äôs historicity &amp; that he was not a story or mythology, the Pontius Pilate in Josephus is a corrupt merciless Roman official who doesn‚Äôt hesitate to get his hands bloody if necessary and who doesn‚Äôt match the Gospel story in the slightest; indeed, reading through Josephus‚Äôs accounts of constant turmoil in Jerusalem of various false prophets and messiahs and rebels (hmm‚Ä¶.) and the difficulties the authorities face, one can‚Äôt believe the entire story of Pilate because to not either immediately execute Jesus or leave the whole matter until well after the very politically sensitive holiday is to assume both the Roman and Jewish officials suffered a sudden attack of the stupids and a collective amnesia of how they usually dealt with such problems. The whole story is blatant rubbish! Yet my religion teachers kept occasionally emphasizing how Jesus was a historical figure. The mythicist case is not compelling but they do a good job of showing how elements of the story were standard tropes for the ancient world, with many parts of the narrative having multiple precedents or weirdly-interpreted Old Testament justifications. In short, reading through higher Biblical criticism, it‚Äôs not a surprise that it is such anathema to modern Christian sects and scriptural inerrancy developed in reaction to it.</p>
<p>At some point, I shrugged and gave up and decided I was an atheist<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a> because certainly I felt nothing<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a>. Theology was interesting to some extent, but there were better things to read about. (My literary interest in Taoism and philosophical interest in Buddhism remain, but I put no stock in any supernatural claims they might make.)</p>
</section>
<section id="the-american-revolution" class="level2">
<h2>The American Revolution</h2>
<p>In middle school, we were assigned a pro-con debate about the American Revolution; I happened to be on the pro side, but as I read through the arguments, I became increasingly disturbed and eventually decided that the pro-Revolution arguments were weak or fallacious.</p>
<p>The Revolution was a bloodbath with ~100,000 casualties or fatalities followed by 62,000 Loyalist/Tory refugees <a href="https://en.wikipedia.org/wiki/Loyalist_%28American_Revolution%29#Emigration_from_the_United_States">fleeing the country</a> for fear of retaliation and their expropriation (the ones who stayed <a href="http://handleshaus.wordpress.com/2014/06/05/suppressing-tories/" title="Suppressing Tories; From: _Conceived in Liberty_, Volume IV, Part II, Chapter 13. Murray Rothbard, 1976">did not escape persecution</a>); this is a butcher‚Äôs bill that did not seem justified in the least by anything in Britain or America‚Äôs subsequent history (what, were the British going to randomly massacre Americans for fun?), even now with a population of &gt;300 million, and much less back when the population was 1/100th the size. Independence was granted to similar English colonies at the smaller price of ‚Äúwaiting a while‚Äù: Canada was essentially autonomous by 1867 (less than a century later) and Australia was first settled in 1788 with autonomous colonies not long behind and the current Commonwealth formed by 1901. (Nor did Canada or Australia suffer worse at England‚Äôs hands during the waiting period than, say, America in that time suffered at its own hands.) In the long run, independence <em>may</em> have been good for the USA, but this would be due to sheer accident: the British were holding the frontier at the Appalachians (see <a href="http://en.wikipedia.org/wiki/Royal%20Proclamation%20of%201763" title="Wikipedia: Royal Proclamation of 1763">Royal Proclamation of 1763</a>), and Napoleon likely would not have been willing engage in the <a href="http://en.wikipedia.org/wiki/Louisiana%20Purchase" title="Wikipedia: Louisiana Purchase">Louisiana Purchase</a> with English colonies inasmuch as he was at war with England. (Assuming we see this as a good thing: Bryan Caplan describes that as removing <a href="http://econlog.econlib.org/archives/2007/07/independence_da.html">‚Äúthe last real check on American aggression against the Indians‚Äù</a>.)</p>
<p>Neither of these is a very strong argument; the British could easily have revoked the Proclamation in face of the colonial resistance (and in practice <em>did</em><a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a>), and Napoleon could not hold onto New France for very long against the British fleets. The argument from ‚Äòfreedom‚Äô is a buzzword or unsupported by the facts - Canada and Australia are hardly hellhole bastions of totalitarianism, and are ranked by <a href="http://en.wikipedia.org/wiki/Freedom%20House" title="Wikipedia: Freedom House">Freedom House</a> as being as free as the USA. (Steve Sailer asks <a href="http://takimag.com/article/declamations_of_independence_steve_sailer">‚ÄúYet how much real difference did the very different political paths of America and Canada make in the long run?‚Äù</a>)</p>
<p>And there are important arguments for the opposite, that America would have been better off under British rule - Britain <a href="http://en.wikipedia.org/wiki/Slavery%20Abolition%20Act%201833" title="Wikipedia: Slavery Abolition Act 1833">ended slavery</a> very early on and likely would have ended slavery in the colonies as well. (Some have argued that with continued control of the southern colonies, Britain would have not been able to do this; but the usual arguments for the Revolution center on the tyranny of Britain - so was the dog wagging the tail or the tail the dog?) The South crucially depended on England‚Äôs tacit support (seeing the South as a counterweight to the dangerous North?), so the <a href="http://en.wikipedia.org/wiki/American%20Civil%20War" title="Wikipedia: American Civil War">American Civil War</a> would either never have started or have been suppressed very quickly. The Civil War would also have lacked its intellectual justification of <a href="http://en.wikipedia.org/wiki/states%27%20rights" title="Wikipedia: states‚Äô rights">states‚Äô rights</a> if the states had remained Crown colonies. The Civil War was so bloody and destructive<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a> that avoiding it is worth a great deal indeed. And then there comes WWI and WWII. It is not hard to see how America remaining a colony would have been better for both Europe and America.</p>
<p>Since that paradigm shift in middle school, my view has changed little:</p>
<ul>
<li>Crane Brinton‚Äôs <em><a href="http://en.wikipedia.org/wiki/The%20Anatomy%20of%20Revolution" title="Wikipedia: The Anatomy of Revolution">The Anatomy of Revolution</a></em> confirmed my beliefs with statistics about the economic class of participants: naked financial self-interest is not a very convincing argument for plunging a country into war, given that England had incurred substantial debt defending and expanding the colonies and their tax burden - that they endlessly complained of - was comically tiny compared to England proper. One of the interesting points Brinton makes was that contrary to the universal belief, revolutions do not universally tend to occur at times of poverty or increasing wealth inequality; indeed, before the American revolution, the colonists were less taxed, <a href="http://www.aae.wisc.edu/events/papers/DevEcon/2012/williamson.10.18.pdf" title="'American Incomes 1774-1860', Lindhart &amp; Williamson 2012">wealthier &amp; more equal than the English</a>.</li>
<li><p>continuing the economic theme, the burdens on the American colonists such as the <a href="http://en.wikipedia.org/wiki/Navigation%20Acts" title="Wikipedia: Navigation Acts">Navigation Acts</a> are now considered to not be burdensome at all, but negligible or positive, especially compared to independence. Famed Scottish economist Adam Smith supported the Navigation Acts as a critical part of the Empire‚Äôs defense<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a> (which included the American colonies; but see again the colonies‚Äô gratitude for the French-Indian War). Their light burden has become economic history consensus since the discussion was sparked in the 1960s (eg. <a href="./docs/1965-thomas.pdf" title="A Quantitative Approach to the Study of the Effects of British Imperial Policy upon Colonial Welfare: Some Preliminary Findings">Thomas 1965</a>, <a href="./docs/1968-thomas.pdf" title="British Imperial Policy and the Economic Interpretation of the American Revolution">Thomas 1968</a>): in 1994, <a href="http://employees.csbsju.edu/JOLSON/ECON315/Whaples2123771.pdf" title="'Where Is There Consensus Among American Economic Historians? The Results of a Survey on Forty Propositions', Whaples 1994">198 economic historians were surveyed</a> asked several questions on this point finding that:</p>
<ol type="1">
<li>132 disagreed with the proposition ‚ÄúOne of the primary causes of the American Revolution was the behavior of British and Scottish merchants in the 1760s and 1770s, which threatened the abilities of American merchants to engage in new or even traditional economic pursuits.‚Äù</li>
<li>178 agreed or partially agreed that ‚ÄúThe costs imposed on the colonists by the trade restrictions of the Navigation Acts were small.‚Äù</li>
<li>111 disagreed that ‚ÄúThe economic burden of British policies was the spark to the American Revolution.‚Äù</li>
<li>117 agreed or partially agreed that ‚ÄúThe <em>personal</em> economic interests of delegates to the Constitutional Convention generally had a [substantial] effect on their voting behavior.‚Äù</li>
</ol></li>
<li><p>Mencius Moldbug discussed good deal of <a href="http://unqualified-reservations.blogspot.com/2009/01/gentle-introduction-to-unqualified_15.html">primary source</a> <a href="http://unqualified-reservations.blogspot.com/2007/12/why-i-am-not-libertarian.html">material</a> which supported my interpretation.</p>
<p>I particularly enjoyed <a href="http://unqualified-reservations.blogspot.com/2007/12/why-i-am-not-libertarian.html">his description</a> of the Pulitzer-winning <em><a href="http://en.wikipedia.org/wiki/The%20Ideological%20Origins%20of%20the%20American%20Revolution" title="Wikipedia: The Ideological Origins of the American Revolution">The Ideological Origins of the American Revolution</a></em>, a study of the popular circulars and essays (of which Thomas Paine‚Äôs <em><a href="http://en.wikipedia.org/wiki/Common%20Sense%20%28pamphlet%29" title="Wikipedia: Common Sense (pamphlet)">Common Sense</a></em> is only the most famous): the author finds that the rebels and their leaders believed there was a conspiracy by English elites to strip them of their freedoms and crush the Protestants under the yoke of the <a href="http://en.wikipedia.org/wiki/Church%20of%20England" title="Wikipedia: Church of England">Church of England</a>.</p>
Bailyn points out that no traces of any such conspiracy has ever been found in the diaries or memorandums or letters of said elites. Hence the Founding Fathers were, as Moldbug claimed, <em>exactly</em> analogous to <a href="http://en.wikipedia.org/wiki/9%2F11%20Truthers" title="Wikipedia: 9/11 Truthers">9/11 Truthers</a> or <a href="http://en.wikipedia.org/wiki/Birthers" title="Wikipedia: Birthers">Birthers</a>. Moldbug further points out that reality has directly contradicted their predictions, as both the Monarchy and Church of England have seen their power continuously decreasing to their present-day ceremonial status, a diminution in progress long before the American Revolution.</li>
<li>Possibly on Moldbug‚Äôs advice, I then read volume 1 of Murray Rothbard‚Äôs <em><a href="http://en.wikipedia.org/wiki/Conceived%20in%20Liberty" title="Wikipedia: Conceived in Liberty">Conceived in Liberty</a></em>. I was unimpressed. Rothbard seems to think he is justifying the Revolution as a noble libertarian thing (except for those other scoundrels who just want to take over); but all I saw were scoundrels.</li>
<li><p>Attempting to take an outside view and ignore the cult built up around the Founding Fathers, viewing them as a cynical foreigner might, the Fathers do not necessarily come off well.</p>
For example, one can compare George Washington to <a href="http://en.wikipedia.org/wiki/Robert%20Mugabe" title="Wikipedia: Robert Mugabe">Robert Mugabe</a>: both led a guerrilla revolution of British colonies against the country which had built their colony up into a wealthy regional powerhouse, and they or their allies employed mobs and terrorist tactics; both oversaw hyperinflation of their currency; both expropriated politically disfavored groups, and engaged in give-aways to supporters (Mugabe redistributed land to black supporters, Washington approved <a href="http://en.wikipedia.org/wiki/Alexander%20Hamilton%23Report%20on%20Public%20Credit" title="Wikipedia: Alexander Hamilton#Report on Public Credit">Alexander Hamilton‚Äôs assumption</a> of states‚Äô war-debts - an incredible windfall for the speculators, who supported the <a href="http://en.wikipedia.org/wiki/Federalist%20party" title="Wikipedia: Federalist party">Federalist party</a>); both were overwhelmingly voted into office and commanded mass popularity even after major failures of their policies became evident (economic growth &amp; hyperinflation for Mugabe, the Whiskey Rebellion for Washington), being hailed as fathers of their countries; and both wound up one of, if not <em>the</em>, most wealthy men in the country (Mugabe‚Äôs fortune has been estimated at anywhere from <a href="http://visualeconomics.creditloan.com/the-wealth-of-world-leaders/">$3b</a> to <a href="http://www.thepowerindex.com.au/dictator-watch/robert-mugabe">$10b</a>; Washington, in inflation-adjusted terms, has been estimated at <a href="http://www.georgetowner.com/articles/2012/feb/07/wealth-presidents/">$0.5b</a>).</li>
<li><p>Jeremy Bentham amusingly <a href="http://persistentenlightenment.wordpress.com/2014/07/03/of-rights-and-witches-benthams-critique-of-the-declaration-of-independence/">eviscerates the Independence‚Äôs complaints</a></p></li>
</ul>
 
 
</section>
<section id="communism" class="level2">
<h2>Communism</h2>
<p>In roughly middle school as well, I was very interested in economic injustice and guerrilla warfare, which naturally led me straight into the communist literature. I grew out of this when I realized that while I might not be able to pinpoint the problems in communism, a lot of that was due to the sheer obscurity and bullshitting in the literature (I finally gave up with <a href="http://en.wikipedia.org/wiki/Empire%20%28book%29" title="Wikipedia: Empire (book)"><em>Empire</em></a>, concluding the problem was not me, Marxism was really that intellectually worthless), and the practical results with economies &amp; human lives spoke for themselves: the ideas were tried in so many countries by so many groups in so many different circumstances over so many decades that if there were anything to them, at least one country would have succeeded. In comparison, even with the broadest sample including hellholes like the Belgian Congo, capitalism can still point to success stories like Japan.</p>
<p>(Similar arguments can be used for science and religion: after early science got the basic inductive empirical formula right, it took off and within 2 or 3 centuries had conquered the intellectual world and assisted the conquest of much of the real world too; in contrast, 2 or 3 centuries after Christianity began, its texts were beginning to finally congeal into the beginnings of a canon, it was minor, and the Romans were still making occasional efforts to exterminate this irksome religion. Charles Murray, in a book I otherwise approve of, attempts to argue in <a href="./docs/2003-murray-human-accomplishment.pdf"><em>Human Accomplishment</em></a> that Christianity was a key factor in the great accomplishments of Western science &amp; technology by some gibberish involving human dignity; the argument is intrinsically absurd - Greek astronomy and philosophy were active when Christianity started, St.¬†Paul literally debated the Greek philosophers in Athens, and yet Christianity did not spark any revolution in the 100s, or 200s, or 300s, or for the next millennium, nor the next millennium and a half. It would literally be fairer to attribute science to William the Conqueror, because that‚Äôs a gap one-third the size and there‚Äôs at least a direct line from William the Conqueror to the Royal Society! If we try to be fairer and say it‚Äôs <em>late</em> Christianity as exemplified by the philosophy of Thomas Aquinas - as influenced by non-Christian thought like Aristotle as it is - that still leaves us a gap of something like 300-500 years. Let us say I would find Murray‚Äôs argument of more interest if it were coming from a non-Christian‚Ä¶)</p>
</section>
<section id="the-occult" class="level2">
<h2>The Occult</h2>
<p>This is not a particular error but a whole class of them. I was sure that the overall theistic explanations were false, but surely there were real phenomenon going on? I‚Äôd read up on individual things like Nostradamus‚Äôs prophecies or the Lance of Longinus, check the skeptics literature, and disbelieve; rinse and repeat until I finally dismiss the entire area with some exceptions like the mental &amp; physical benefits of meditation. One might say my experience was a little like <a href="http://en.wikipedia.org/wiki/Susan%20Blackmore" title="Wikipedia: Susan Blackmore">Susan Blackmore</a>‚Äôs career as recounted in <a href="http://www.susanblackmore.co.uk/Articles/si87.html">‚ÄúThe Elusive Open Mind: Ten Years of Negative Research in Parapsychology‚Äù</a>, sans the detailed experiments. (I am still annoyed that I was unable to disbelieve the research on <a href="http://en.wikipedia.org/wiki/Transcendental%20Meditation" title="Wikipedia: Transcendental Meditation">Transcendental Meditation</a> until I read more about the corruption, deception, and falsified predictions of the TM organization itself.) Fortunately, I had basically given up on occult things by high school, before I read Eco‚Äôs <em><a href="http://en.wikipedia.org/wiki/Foucault%27s%20Pendulum" title="Wikipedia: Foucault‚Äôs Pendulum">Foucault‚Äôs Pendulum</a></em>, so I don‚Äôt feel <em>too</em> chagrined about this.</p>
</section>
<section id="fiction" class="level2">
<h2>Fiction</h2>
<p>I spend most of my time reading; I also spent most of my time in elementary, middle, and high school reading. What has changed in <em>what</em> I read - I now read principally nonfiction (philosophy, economics, random sciences, etc.), where I used to read almost exclusively fiction. (I would include one nonfiction book in my stacks of books to check out, on a sort of ‚Äòvegetables‚Äô approach. Eat your vegetables and you can have dessert.) I, in fact, aspired to be a novelist. I thought fiction was a noble task, the highest production of humanity, and writers some of the best people around, producing immortal works of truth. Slowly this changed. I realized fiction changed nothing, and when it did change things, it was as oft as not for the worse. Fiction promoted simplification, focus on sympathetic examples, and I recognized how much of my own infatuation with the Occult (among other errors) could be traced to fiction. What a strange belief, that you could find truths in lies.<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a> And there are so many of them, too! So very many. (I wrote one essay on this topic, <a href="Culture%20is%20not%20about%20Esthetics" title="Go to wiki page: Culture%20is%20not%20about%20Esthetics">Culture is not about Esthetics</a>.) I still produce <a href="index#fiction">some fiction</a> these days, but mostly when I can‚Äôt help it or as a writing exercise.</p>
</section>
<section id="nicotine" class="level2">
<h2>Nicotine</h2>
<p>I changed my mind about <a href="Nicotine">nicotine</a> in 2011. I had naturally assumed, in line with the usual American cultural messages, that there was nothing good about tobacco and that smoking is deeply shameful, proving that you are a selfish lazy short-sighted person who is happy to commit slow suicide (taking others with him via second-hand smoke) and cost society a fortune in medical care. Then some mentions of nicotine as useful came up and I began researching it. I‚Äôm still not a fan of <em>smoking</em>, and I regard any tobacco with deep trepidation, but <a href="Nicotine#performance">the research literature</a> seems pretty clear: nicotine enhances mental performance in multiple domains and may have some minor health benefits to boot. Nicotine sans tobacco seems like a clear win. (It amuses me that of the changes listed here, this is probably the one people will find most revolting and bizarre.)</p>
</section>
<section id="centralized-black-markets" class="level2">
<h2>Centralized black-markets</h2>
<p>I overestimated the stability of Bitcoin+Tor black-markets such as <a href="Silk%20Road">Silk Road</a>: I was aware that the centralization of the first-generation black-markets (SR/BMR/Atlantis/Sheep) meant that the site operators had a strong temptation to steal all deposit &amp; escrows, but I thought that the value of future escrow commissions provided enough incentive to make rip-and-run scams rare - certainly they were fairly rare during the Silk Road 1 era.</p>
<p>After Silk Road was shut down in October 2013, SR turned out to be highly unusual: both less hacked than most markets, and it seems that whatever his (many) other failings, Ross Ulbricht genuinely believed his own ideology and so was running Silk Road out of principle rather than greed (which also explains why he didn‚Äôt retire despite a fortune larger than he could spend in a lifetime). Attracted by the sudden void in a large market, and by the FBI‚Äôs press releases crowing over how many hundreds of millions of dollars Silk Road had earned, dozens of new markets sprang up to fill the void. Many then proceeded to scam users (often taking advantage of the standard ‚Äòseller bonds‚Äô: sellers would deposit a large sum as a guarantee against scamming buyers in the early period where they were accepting orders but most packages would not have arrived) or alternately, be hacked due to the operators‚Äô get-rich-quick incompetence and rather than refund users from future profits, decide to steal everything the hacker didn‚Äôt get. As of April 2014, it seems users have <em>mostly</em> learned caution, and the shift to multisig escrow removes the need to trust market operators and hence the risk from the operators or hackers, so matters may finally be stabilizing.</p>
<p>I think my original point is still correct that markets can be trusted as long as the discounted present value of their future earnings exceeds the amount they can steal. My mistake here was overestimating the net present value: I didn‚Äôt realize that site operators had such high discount rates (one, PBF, pulled its scam after perhaps a few thousand dollars‚Äô worth of Bitcoin had been deposited despite positive initial reviews) and there was so much risk involved (the Bitcoin exchange rate, arrest, hacking; all exacerbated by the incompetence of many site operators).</p>
<p>This mistake lead to complacency on my part in archiving the markets &amp; forums: if you expect a market to be around for years, there is no particular need to try to mirror them weekly. And so while I have good coverage of the black-markets post-December-2013, I am missing most of the markets before then.</p>
</section>
</section>
<section id="potential-changes" class="level1">
<h1>Potential changes</h1>
<blockquote>
<p>The mind cannot foresee its own advance.<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a></p>
</blockquote>
<p>There are some things I used to be certain about, but I am no longer certain either way; I await future developments which may tip me one way or the other.</p>
<section id="near-singularity" class="level2">
<h2>Near Singularity</h2>
<p>I am no longer certain that <a href="http://en.wikipedia.org/wiki/the%20Singularity" title="Wikipedia: the Singularity">the Singularity</a> is near.</p>
<p>In the 1990s, all the numbers seem to be ever-accelerating. Indeed, I could feel with Kurzweil that <em><a href="http://en.wikipedia.org/wiki/The%20Singularity%20is%20Near" title="Wikipedia: The Singularity is Near">The Singularity is Near</a></em>. But an odd thing happened in the 2000s (a dreary decade, distracted by the dual dissipation of Afghanistan &amp; Iraq). The hardware kept getting better mostly in line with Moore‚Äôs Law (troubling as the flight to parallelism is), but the AI software didn‚Äôt seem to keep up. I am only a layman, but it looks as if all the AI applications one might cite in 2011 as progress are just old algorithms now practical with newer hardware. And economic growth slowed down, and the stock market ticked along, barely maintaining itself. The Human Genome Project completely fizzled out, with interesting insights and not much else. (It‚Äôs great that genome sequencing has improved exactly as promised, but what about <em>everything else</em>? Where are our embryo selections, our germ-line engineering, our universal genetic therapies, our customized drugs?<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a>) The pharmaceutical industry has reached such diminishing returns that even the optimists have noticed the problems in the drug pipeline, problems so severe that it‚Äôs hard to wave them away as due to that dratted FDA or ignorant consumers. As of 2007, the increases in longevity for the elderly<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a> in the US has continued to be less each year and <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3163136/">‚Äúare probably getting slower‚Äù</a>, which isn‚Äôt good news for those hoping to reach <a href="http://en.wikipedia.org/wiki/Aubrey%20de%20Grey" title="Wikipedia: Aubrey de Grey">Aubrey de Grey</a>‚Äôs ‚Äúescape velocity‚Äù; and medicine has been a repeated disappointment even to <a href="http://lesswrong.com/lw/8yp/prediction_is_hard_especially_of_medicine/">forecasting-savvy predictors</a> (the ‚Äô90s and the genetic revolution being especially remarkable for its lack of concrete improvements). Kurzweil published <a href="http://www.kurzweilai.net/predictions.php">an evaluation of his predictions</a> up to ~2009 with great fanfare and self-congratulation, but reading through them, I was struck by how many he weaseled out on (claiming as a hit anything that existed in a lab or a microscopic market segment, even though in context he had clearly expected it to be widespread) and how often they failed due to unintelligent software.</p>
<p>And there are many troubling long-term metrics. I was deeply troubled to read <a href="http://en.wikipedia.org/wiki/Charles%20Murray" title="Wikipedia: Charles Murray">Charles Murray</a>‚Äôs <em><a href="http://en.wikipedia.org/wiki/Human%20Accomplishment" title="Wikipedia: Human Accomplishment">Human Accomplishment</a></em> pointing out a long-term decline in discoveries per capita (despite ever increasing scientists and artists per capita!), even after he corrected for everything he could think of. I didn‚Äôt see any obvious mistakes. <a href="http://en.wikipedia.org/wiki/Tyler%20Cowen" title="Wikipedia: Tyler Cowen">Tyler Cowen</a>‚Äôs <em><a href="http://en.wikipedia.org/wiki/The%20Great%20Stagnation" title="Wikipedia: The Great Stagnation">The Great Stagnation</a></em> twisted the knife further, and then I read <a href="http://en.wikipedia.org/wiki/Joseph%20Tainter" title="Wikipedia: Joseph Tainter">Joseph Tainter</a>‚Äôs <em><a href="http://en.wikipedia.org/wiki/The%20Collapse%20of%20Complex%20Societies" title="Wikipedia: The Collapse of Complex Societies">The Collapse of Complex Societies</a></em>. I have kept notes since and see little reason to expect a general exponential upwards over all fields, including the ones minimally connected to computing. (<a href="http://en.wikipedia.org/wiki/Peter%20Thiel" title="Wikipedia: Peter Thiel">Peter Thiel</a>‚Äôs <a href="https://web.archive.org/web/20111218234418/http://www.nationalreview.com/articles/print/278758">‚ÄúThe End of the Future‚Äù</a> makes a distinction between ‚Äúthe progress in computers and the failure in energy‚Äù; he also makes an interesting link between the lack of progress and the many recent speculative bubbles in <a href="http://www.hoover.org/publications/policy-review/article/5646">‚ÄúThe Optimistic Thought Experiment‚Äù</a>.) The Singularity is still more likely than not, but these days, I tend to look towards emulation of human brains via scanning of <a href="plastination">plastinated brains</a> as the cause. Whole brain emulation is not likely for many decades, given the extreme computational demands (even if we are optimistic and take the <a href="http://www.fhi.ox.ac.uk/reports/2008-3.pdf" title="Sandberg &amp; Bostrom 2008">Whole Brain Emulation Roadmap</a> figures, one would not expect a upload until the 2030s) and it‚Äôs not clear how useful an upload would be in the first place. It seems entirely possible that the mind will run slowly, be able to self-modify only in trivial ways, and in general be a curiosity akin to the Space Shuttle than a pivotal moment in human history deserving of the title Singularity.</p>
<section id="counter-point" class="level3">
<h3>Counter-point</h3>
<blockquote>
<p><a href="http://lesswrong.com/r/discussion/lw/dm5/why_could_you_be_optimistic_that_the_singularity/#comments" title="Why could you be optimistic that the Singularity is Near?">LessWrong discussion</a></p>
</blockquote>
<p>I respect my own opinion, but at the same time I know I am not immune to common beliefs; so it bothers me to see ‚Äòstagnation‚Äô and pessimistic ideas become more widespread because this means I may just be following a trend. I did not like agreeing with any of <em>Wired</em>‚Äòs hyperbolic forecasts back in the 1990s, and I do not like agreeing with Peter Thiel or Neal Stephenson now. One of Buffet‚Äôs classic sayings is ‚Äúif they [investors] insist on trying to time their participation in equities, they should try to be fearful when others are greedy and greedy when others are fearful.‚Äù What grounds do I have for being ‚Äôgreedy‚Äô now, when many are being ‚Äòfearful‚Äô? What <a href="http://www.bloomberg.com/news/2011-10-24/bias-blindness-and-how-we-truly-think-part-1-daniel-kahneman.html">Kahneman-style pre-mortem</a> would I give for explaining why the Singularity might indeed be Near?</p>
<p>First, one could point out that a number of technological milestones seem to be catching up, after long stagnations. From 2009-2012, there were a number of unexpected achievements: Google‚Äôs robotic car astounded me, the long AI-resistant game of <a href="http://en.wikipedia.org/wiki/Go%20%28game%29" title="Wikipedia: Go (game)">Go</a> is falling to <a href="http://en.wikipedia.org/wiki/Computer%20Go" title="Wikipedia: Computer Go">AIs using</a> <a href="http://en.wikipedia.org/wiki/Monte%20Carlo%20method%23Games" title="Wikipedia: Monte Carlo method#Games">Monte Carlo tree</a> techniques are closing in the Go masters (I expect computers to <a href="http://predictionbook.com/predictions/5915">take world champion by 2030</a>), online education seems to be starting to realize its promise (eg. the success of <a href="http://en.wikipedia.org/wiki/Khan%20Academy" title="Wikipedia: Khan Academy">Khan Academy</a>), private space exploitation is doing surprisingly well (as are Tesla electric cars, which seem to be moving from playthings to perhaps mass market cars), smartphones - after a decade of being crippled by telecoms and limited computing power - are becoming ubiquitous and desktop replacements. Old <a href="http://en.wikipedia.org/wiki/cypherpunk" title="Wikipedia: cypherpunk">cypherpunk</a> dreams like <a href="http://en.wikipedia.org/wiki/Bitcoin" title="Wikipedia: Bitcoin">anonymous digital cash</a> and online <a href="Silk%20Road">black markets</a> have swung into action and given rise to active &amp; growing communities. In the larger picture, the Long Depression beginning in 2008 has wreaked havoc on young people, but China has not imploded while continuing to move up the quality chain (replacing laborers with robots) and far from being ‚Äòthe crisis of capitalism‚Äô or ‚Äòthe end of capitalism‚Äô, in general, global life is going on. Even Africa, while the population size is exploding, is growing economically - perhaps thanks to universally available cheap cellphones. Peak Oil continues to be delayed by new developments like fracking and resultant gluts of natural gas (the US now <em>exports</em> energy!), although the long-term scientific productivity trends seem to still point downwards.</p>
<p>What many of these points have in common is that their forebears germinated for a long time in niches and did not live up to the forecasts of their proponents - smartphones, for example, have been expected to revolutionize everything since at least the 1980s (by members of the <a href="http://en.wikipedia.org/wiki/MIT%20Media%20Lab" title="Wikipedia: MIT Media Lab">MIT Media Lab</a> in particular). And indeed, they are revolutionizing everything, worldwide, 30 years later. This exemplifies a line attributed to Roy Amara:</p>
<blockquote>
<p>We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run.</p>
</blockquote>
<p>A number of current but disappointing trends may be disappointing only in the ‚Äúshort run‚Äù, the flat part of their respective exponential or sigmoid development curves<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a>. For example, DNA sequencing has been plummeting and sequencing a whole human genome will likely be &lt;$100 by 2015; this has been an incredible boon for basic research and our knowledge of the world, but so far the applications have been fairly minimal - but this may not be true forever, with new projects starting up tackling topics of the greatest magnitude like using thousands of genomes to search for the thousands of alleles which each affect intelligence a tiny bit. Were embryo selection for intelligence to become viable (as there is no reason to believe would not be possible once the right alleles have been identified) and every baby could be born with IQs &gt;130, society would change.</p>
<p>Does this apply to AI? At least two of the examples seem clear-cut examples of an ‚ÄòAmara effect‚Äô: Go-playing AIs were for decades toys easily beaten by bad amateurs until Monte Carlo Tree Search was introduced in 2006<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a>, while the first <a href="http://en.wikipedia.org/wiki/DARPA%20Grand%20Challenge" title="Wikipedia: DARPA Grand Challenge">DARPA Grand Challenge</a> in <a href="http://en.wikipedia.org/wiki/DARPA%20Grand%20Challenge%20%282004%29" title="Wikipedia: DARPA Grand Challenge (2004)">2004</a> was a debacle in which no car finished and the best car managed to travel a whopping 7 miles before getting stuck on a rock. 8 years later, and now the conversation has suddenly shifted from ‚Äúwill Go AIs ever reach human level?‚Äù or ‚Äúwill self-driving cars ever be able to cope with the real world?‚Äù to the simple question <em>when?</em>.</p>
<p>One of the ironies is that I am sure a ‚Äòpure‚Äô AI is possible; but the AI can‚Äôt be developed before the computing power is available (we humans are just not good enough at math &amp; programming to achieve it without running code), which means the AI will be developed either simultaneous with or <em>after</em> enough computing power becomes available. If the latter, if the AI is not run at the exact instant that there is enough processing power available, ever more computing power in excess of what is needed (by definition) builds up. It is like a dry forest roasting in the summer sun: the longer the wait until the match, the faster and hotter the wildfire will burn<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a>. Perhaps paradoxically, the longer I live without seeing an AI of any kind, the more schizophrenic my forecasts will appear to an outsider who hasn‚Äôt carefully thought about the issue - I will predict with increasingly high confidence that the future will be boring and normal (because the continued non-appearance makes it increasingly likely AI is impossible, see <a href="./docs/statistics/1994-falk" title="The Ups and Downs of the Hope Function In a Fruitless Search">the hope function</a> <a href="http://lesswrong.com/lw/5hq/an_inflection_point_for_probability_estimates_of/428t">&amp; AI</a>), that AI is more likely to be created in the next year (because the possibilities are being exhausted as time passes) <em>and</em> the changes I predict become ever more radical!</p>
<p>This set of estimates is obviously consistent with an appearance of stagnation: each year small advances build up, but no big breakthroughs appear - until they do.</p>
</section>
</section>
<section id="neo-luddism" class="level2">
<h2>Neo-Luddism</h2>
<blockquote>
<p>‚ÄúAlmost in the same way as earlier physicists are said to have found suddenly that they had too little mathematical understanding to be able to master physics; we may say that young people today are suddenly in the position that ordinary common sense no longer suffices to meet the strange demands life makes. Everything has become so intricate that for its mastery an exceptional degree of understanding is required. For it is not enough any longer to be able to play the game well; but the question is again and again: what sort of game is to be played now anyway?‚Äù ‚ÄìWittgenstein‚Äôs <em>Culture and Value</em>, MS 118 20r: 27.8.1937</p>
</blockquote>
<p>The idea of <a href="http://en.wikipedia.org/wiki/Luddite%23In%20contemporary%20thought" title="Wikipedia: Luddite#In contemporary thought">technological unemployment</a> - permanent <a href="http://en.wikipedia.org/wiki/structural%20unemployment" title="Wikipedia: structural unemployment">structural unemployment</a> and a <a href="http://en.wikipedia.org/wiki/jobless%20recovery" title="Wikipedia: jobless recovery">jobless recovery</a> - used to be dismissed contemptuously as the <a href="http://en.wikipedia.org/wiki/Luddite%20fallacy" title="Wikipedia: Luddite fallacy">Luddite fallacy</a>. (There are models where technology <em>does</em> produce permanent unemployment, and quite plausible ones too; see <a href="http://econ-www.mit.edu/files/569">Autor et al</a> and <a href="http://econ-www.mit.edu/files/5554">Autor &amp; Hamilton</a><a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a> and Krugman‚Äôs <a href="http://krugman.blogs.nytimes.com/2011/03/06/autor-autor/">commentary</a> pointing to <a href="http://www.nber.org/papers/w16082">recent data</a> showing the ‚Äòhollowing out‚Äô and ‚Äòdeskilling‚Äô predicted by the Autor model, which is also consistent with the <a href="http://www.federalreserve.gov/pubs/feds/2011/201141/index.html">long-term decline in teenage employment due to immigration</a>. Martin Ford has <a href="http://www.asymptosis.com/are-machines-replacing-humans-or-am-i-a-luddite.html">some graphs</a> explaining the complementation-substitution model.) But ever since the Internet bubble burst, it‚Äôs been looking more and more likely, with scads of evidence for it since the housing bubble like the otherwise peculiar changes in the value of college degrees<a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a>. (This is closely related to my grounds for believing in a distant Singularity.) When I look around, it seems to me that we have been suffering tremendous unemployment for a long time. When Alex Tabarrok writes ‚ÄúIf the Luddite fallacy were true we would all be out of work because productivity has been increasing for two centuries‚Äù, I think, isn‚Äôt that correct? If you‚Äôre not a student, you‚Äôre retired; if you‚Äôre not retired, you‚Äôre disabled<a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a>; if you‚Äôre not disabled, perhaps you are institutionalized; if you‚Äôre not that, maybe you‚Äôre on welfare, or just unemployed.</p>
<p>Compare now to most of human history, or just the 1300s:</p>
<ul>
<li>every kid in special ed would be out working on the farm; there would, if only from reduced <a href="http://en.wikipedia.org/wiki/moral%20hazard" title="Wikipedia: moral hazard">moral hazard</a><a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a> be fewer disabled than now (federal <a href="http://en.wikipedia.org/wiki/Supplemental%20Security%20Income" title="Wikipedia: Supplemental Security Income">Supplemental Security Income</a> alone supports 8 million Americans)</li>
<li><p>everyone in college would be out working (because the number of students was a rounding error and they didn‚Äôt spend very long in higher education to begin with)</p>
Indeed, education and healthcare are a huge chunk of the US economy - and both have serious questions about how much good, exactly, they do and whether they are grotesquely inefficient or just inefficient.</li>
<li>retirees didn‚Äôt exist outside the tiny nobility</li>
<li><p>‚Äòguard labor‚Äô - people employed solely to control and ensure productivity of the others has increased substantially (<a href="https://web.archive.org/web/20121031025512/http://www.international.ucla.edu/cms/files/jayadev_bowles.pdf" title="'Guard labor', Jayadev &amp; Bowles 2006">Bowles &amp; Jayadev 2006</a> claim US guard labor has gone from 6% of the 1890 labor force to 26% in 2002; this is not due to manufacturing declines<a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a>); examples of guard labor:</p>
<ul>
<li>standing militaries were unusual (although effective when needed<a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a>); the US maintains the <a href="http://en.wikipedia.org/wiki/List%20of%20countries%20by%20number%20of%20troops" title="Wikipedia: List of countries by number of troops">second-largest active</a> in the world - ~1.5m (~0.5% of the population), which employs millions more with its $700 billion budget<a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a> and is a key source of pork<a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a> and make-work</li>
<li>prisons were mostly for <em>temporary</em> incarceration pending trial or punishment<a href="#fn33" class="footnoteRef" id="fnref33"><sup>33</sup></a>; the US <a href="http://en.wikipedia.org/wiki/Incarceration%20in%20the%20United%20States" title="Wikipedia: Incarceration in the United States">currently</a> <a href="http://www.newyorker.com/arts/critics/atlarge/2012/01/30/120130crat_atlarge_gopnik?currentPage=all">has</a> ~2.3m (nearly 1% of the population!), and perhaps another 4.9m on parole/probation. (See also <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=970341" title="An Institutionalization Effect: The Impact of Mental Hospitalization and Imprisonment on Homicide in the United States, 1934-2001">the relationship</a> of psychiatric imprisonment with criminal imprisonment.) That‚Äôs impressive enough, but as with the military, consider how many people are tied down solely <em>because</em> of the need to maintain and supply the prison system - prison wardens, builders, police etc.</li>
</ul></li>
<li><p>people worked <em>hard</em>; the <a href="http://en.wikipedia.org/wiki/8-hour%20day" title="Wikipedia: 8-hour day">8-hour day</a> and 5-day workweek were major hard-fought changes (a plank of the <em><a href="http://en.wikipedia.org/wiki/Communist%20Manifesto" title="Wikipedia: Communist Manifesto">Communist Manifesto</a></em>!). Switching from a 16-hour to an 8-hour day means we are half-retired already and need many more workers than otherwise.</p></li>
</ul>
<p>In contrast, Americans now spend most of their lives not working.</p>
<p>The unemployment rate looks good - 9% is surely a refutation of the Luddite fallacy! - until you look into the meat factory and see that that is the best rate, for college graduates actively looking for jobs, and not the overall population one including those who have given up. Economist <a href="http://www.bloomberg.com/news/2011-03-31/why-unemployment-rose-so-much-dropped-so-fast-commentary-by-alan-krueger.html">Alan Krueger</a> writes of the ratio (which covers <em>only</em> 15-64 year olds):</p>
<blockquote>
<p>Tellingly, the <a href="http://en.wikipedia.org/wiki/Employment-to-population%20ratio" title="Wikipedia: Employment-to-population ratio">employment-to-population rate</a> has hardly budged since reaching a low of 58.2% in December 2009. Last month it stood at just 58.4%. Even in the expansion from 2002 to 2007 the share of the population employed never reached the peak of 64.7% it attained before the March-November 2001 recession.</p>
</blockquote>
<p>Let‚Äôs break it down by age group using <a href="http://en.wikipedia.org/wiki/Federal%20Reserve%20Economic%20Data" title="Wikipedia: Federal Reserve Economic Data">FRED</a>:</p>
<figure>
<img alt="Labor force participation rate 1993-2013, by age groups: 25-54yo, 20-24yo, &gt;55yo" height="378" src="./images/2013-fred-19932013laborforcerate.png" title="http://research.stlouisfed.org/fredgraph.png?g=gmF" width="630"/><figcaption>Labor force participation rate 1993-2013, by age groups: 25-54yo, 20-24yo, &gt;55yo</figcaption>
</figure>
<p><a href="http://en.wikipedia.org/wiki/Scott%20Sumner" title="Wikipedia: Scott Sumner">Scott Sumner</a> <a href="http://www.themoneyillusion.com/?p=16923" title="The employment to population ratio is nearly meaningless">correctly</a> <a href="http://www.themoneyillusion.com/?p=17076" title="About those employment to population ratios">points out</a> that the employment:population ratio itself doesn‚Äôt intrinsically tell us about whether things are going well or poorly - one could imagine a happy and highly automated country with a <a href="http://en.wikipedia.org/wiki/basic%20income%20guarantee" title="Wikipedia: basic income guarantee">basic income guarantee</a> where only 20% of the population works or an agricultural country where everyone works and is desperately poor. What matters more is wealth inequality combined with employment ratio: how many people are either rich enough that not having a job is not a disaster or at least can get a job?</p>
<p>What do you suppose the employment-to-population rate was in 1300 in the poorer 99% of the world population (remembering how homemaking and raising children is effectively a full-time job)? I‚Äôd bet it was a lot higher than the world record in 2005, Iceland‚Äôs 84%. And Iceland is a very brainy place. What are the merely average with IQs of 100-110 supposed to do? (Heck, what is the half of America with IQs in that region or below supposed to do? Learn C++ and statistics so they can work on Wall Street?) If you want to see the future, look at our youth; where are <a href="http://www.businessweek.com/printer/articles/54656-the-youth-unemployment-bomb" title="'The Youth Unemployment Bomb', by Peter Coy on February 02, 2011">summer jobs</a> these days? Gregory Clark comments sardonically (although he was likely not thinking of <a href="http://hanson.gmu.edu/uploads.html">whole brain emulation</a>) in <em><a href="http://en.wikipedia.org/wiki/Farewell%20to%20Alms" title="Wikipedia: Farewell to Alms">Farewell to Alms</a></em>:</p>
<blockquote>
<p>Thus, while in preindustrial agrarian societies half or more of the national income typically went to the owners of land and capital, in modern industrialized societies their share is normally less than a quarter. Technological advance might have been expected to dramatically reduce unskilled wages. After all, there was a class of workers in the preindustrial economy who, offering only brute strength, were quickly swept aside by machinery. By 1914 most horses had disappeared from the British economy, swept aside by steam and internal combustion engines, even though a million had been at work in the early nineteenth century. When their value in production fell below their maintenance costs they were condemned to the knacker‚Äôs yard.</p>
</blockquote>
<p>Technology may increase total wealth under many models, but there‚Äôs a key loophole in the idea of ‚ÄúPareto-improving‚Äù gains - <em>they don‚Äôt <strong>ever</strong> have to make some people better off</em>. And a Pareto-improvement is a good result! Many models don‚Äôt guarantee even that - it‚Äôs perfectly possible to become worse off (see the horses above and the fate of humans in <a href="http://en.wikipedia.org/wiki/Robin%20Hanson" title="Wikipedia: Robin Hanson">Robin Hanson</a>‚Äòs ‚Äôcrack of a future dawn‚Äô scenario). Such doctrinairism is not useful:</p>
<blockquote>
<p>‚ÄúLike experts in many fields who give policy advice, the authors show a preference for first-best, textbook approaches to the problems in their field, while leaving other messy objectives acknowledged but assigned to others. In this way, they are much like those public finance economists who oppose tax expenditures on principle, because they prefer direct expenditure programs, but do not really analyze the various difficulties with such programs; or like trade economists who know that the losers from trade surges need to be protected but regard this as not a problem for trade policy.‚Äù ‚Äì<a href="http://en.wikipedia.org/wiki/Lawrence%20H%20Summers" title="Wikipedia: Lawrence H Summers">Lawrence H Summers</a>, ‚ÄúComments on ‚ÄòThe Contradiction in China‚Äôs Gradualist Banking Reforms‚Äô‚Äù, <em>Brookings Papers on Economic Activity</em> 2006, 2, 149-162</p>
</blockquote>
<p>This is closely related to what I‚Äôve dubbed the ‚Äú‚ÄòLuddite fallacy‚Äô fallacy‚Äù (along the lines of the <a href="http://lesswrong.com/lw/z0/the_pascals_wager_fallacy_fallacy/">Pascal‚Äôs Wager Fallacy Fallacy</a>): technologists who are extremely intelligent and have worked most of their life only with fellow potential <a href="http://en.wikipedia.org/wiki/MENSA" title="Wikipedia: MENSA">Mensans</a> confidently say that ‚Äúif there is structural unemployment (and I‚Äôm being generous in granting you Luddites even this contention), well, better education and training will fix <em>that</em>!‚Äù It‚Äôs a little hard to appreciate what a stupendous mixture of availability bias, infinite optimism, and plain denial of intelligence differences this all is. <a href="http://online.wsj.com/article/SB10001424053111903480904576512250915629460.html">Marc Andreessen</a> offers an example in 2011:</p>
<blockquote>
<p>Secondly, many people in the U.S. and around the world lack the education and skills required to participate in the great new companies coming out of the software revolution. This is a tragedy since every company I work with is absolutely starved for talent. Qualified software engineers, managers, marketers and salespeople in Silicon Valley can rack up dozens of high-paying, high-upside job offers any time they want, while national unemployment and underemployment is sky high. This problem is even worse than it looks because many workers in existing industries will be stranded on the wrong side of software-based disruption and may never be able to work in their fields again. There‚Äôs no way through this problem other than education, and we have a long way to go.</p>
</blockquote>
<p>I see. So all we have to do with all the people with &lt;120 IQs, who struggled with algebra and never made it to calculus (when they had the self-discipline to learn it at all), is just to train them into world-class software engineers and managers who can satisfy Silicon Valley standards; and we have to do this for the first time in human history. Gosh, is that all? Why didn‚Äôt you say so before - we‚Äôll get on that <em>right away</em>! Or an anonymous ‚Äúdata scientist‚Äù recorded in the <a href="http://6thfloor.blogs.nytimes.com/2013/07/12/considering-the-horror-of-zuckerbergian-dystopias/"><em>NYT</em></a>: ‚ÄúHe found my concerns to be amusing. People can get work creating SEO-optimized niche blogs, he said. Or they can learn to code.‚Äù <a href="http://www.nytimes.com/2013/03/31/opinion/sunday/friedman-need-a-job-invent-it.html" title="Need a Job? Invent It">Thomas Friedman</a>:</p>
<blockquote>
<p>Every middle-class job today is being pulled up, out or down faster than ever. That is, it either requires more skill or can be done by more people around the world or is being buried - made obsolete - faster than ever. Which is why the goal of education today, argues Wagner, should not be to make every child ‚Äúcollege ready‚Äù but ‚Äúinnovation ready‚Äù - ready to add value to whatever they do‚Ä¶more than ever, our kids will have to ‚Äúinvent‚Äù a job. (Fortunately, in today‚Äôs world, that‚Äôs easier and cheaper than ever before.) Sure, the lucky ones will find their first job, but, given the pace of change today, even they will have to reinvent, re-engineer and reimagine that job much more often than their parents if they want to advance in it‚Ä¶ What does that mean for teachers and principals? [<a href="http://www.tonywagner.com/">Tony Wagner</a>:] ‚ÄúAll students should have digital portfolios to show evidence of mastery of skills like critical thinking and communication, which they build up right through K-12 and post-secondary. Selective use of high-quality tests, like the College and Work Readiness Assessment, is important. Finally, teachers should be judged on evidence of improvement in students‚Äô work through the year - instead of a score on a bubble test in May. We need lab schools where students earn a high school diploma by completing a series of skill-based ‚Äòmerit badges‚Äô in things like entrepreneurship. And schools of education where all new teachers have ‚Äòresidencies‚Äô with master teachers and performance standards - not content standards - must become the new normal throughout the system.‚Äù</p>
</blockquote>
<p>These sentiments or goals are so breathtakingly delusional (have these people ever met the average American?) that I find myself wondering (despite my personal injunctions against resorting to ad hominems) that ‚Äúsurely no one could believe such impossible things, either before or after breakfast; surely an award-winning <em>New York Times</em> columnist or a famous Harvard educational theorist, surely <em>these</em> people cannot seriously believe the claims they are supposedly making, and there is some more reasonable explanation - like they have been bribed by special interests, or are expounding propaganda designed to safeguard their lucrative profits from populist redistribution, or are pulling a prank in very bad taste, or (like President Reagan) are tragically in the grips of a debilitating brain disease?‚Äù</p>
<p>But moving on past Andreessen and Friedman. If it really is possible for people to rise to the demands of the New Economy, why is it not happening? <a href="http://public.econ.duke.edu/~psarcidi/prop209instfit.pdf" title="'Affirmative Action and University Fit: Evidence from Proposition 209', Arcidiacono et al 2012">For example</a> (emphasis added)</p>
<blockquote>
<p>As documented in <a href="http://www.nber.org/chapters/c10097.pdf" title="Going to College and Finishing College: Explaining Different Educational Outcomes">Turner (2004)</a>, Bound and Turner (<a href="http://cid.bcrp.gob.pe/biblio/Papers/NBER/2006/Agosto/w12424.pdf" title="Cohort crowding: How resources affect collegiate attainment">2007</a>, 2011), and <a href="http://cid.bcrp.gob.pe/biblio/Papers/NBER/2009/Diciembre/w15566.pdf" title="Why have College Completion Rates Declined? An Analysis of Changing Student Preparation and Collegiate Resources">Bound, Lovenheim and Turner (2010a)</a>, while the number of students attending college has increased over the past three decades in the U.S., college graduation rates (i.e., the fraction of college enrollees that graduate) and college attainment rates (i.e., the fraction of the population with a college degree) <em>have hardly changed since 1970</em> and the time it takes college students to complete a baccalaureate (BA) degree has increased (<a href="http://www.psc.isr.umich.edu/pubs/pdf/rr10-698.pdf" title="Increasing time to baccalaureate degree in the United States">Bound, Lovenheim and Turner, 2010b</a>). The disparities between the trends in college attendance and completion or time-to-completion of college degrees is all the more stark given that the earnings premium for a college degree relative to a high school degree nearly doubled over this same period (Goldin and Katz, 2008).</p>
<ul>
<li>Bound, John and Sarah Turner (2011). ‚ÄúDropouts and Diplomas: The Divergence in Collegiate Outcomes.‚Äù in <em>Handbook of the Economics of Education</em>, Vol. 4, E. Hanushek, S. Machin and L. Woessmann (eds.) Elsevier B.V., 573-613</li>
<li>Goldin, Claudia and Lawrence Katz (2008). <a href="http://www.amazon.com/The-Race-between-Education-Technology/dp/0674035305/?tag=gwernnet-20"><em>The Race between Education and Technology</em></a>. Cambridge: Harvard University Press</li>
</ul>
</blockquote>
<p>Or <a href="http://www.nytimes.com/2013/03/21/business/economy/as-men-lose-economic-ground-clues-in-the-family.html">‚ÄúStudy of Men‚Äôs Falling Income Cites Single Parents‚Äù</a>:</p>
<blockquote>
<p>The fall of men in the workplace is widely regarded by economists as one of the nation‚Äôs most important and puzzling trends. While men, on average, still earn more than women, the gap between them has narrowed considerably, particularly among more recent entrants to the labor force. For all Americans, it has become much harder to make a living without a college degree, for intertwined reasons including foreign competition, advancements in technology and the decline of unions. Over the same period, the earnings of college graduates have increased. Women have responded exactly as economists would have predicted, by going to college in record numbers. Men, mysteriously, have not. Among people who were 35 years old in 2010, for example, women were 17% more likely to have attended college, and 23% more likely to hold an undergraduate degree. ‚ÄúI think the greatest, most astonishing fact that I am aware of in social science right now is that women have been able to hear the labor market screaming out ‚ÄòYou need more education‚Äô and have been able to respond to that, and men have not,‚Äù said Michael Greenstone, an M.I.T. economics professor who was not involved in Professor Autor‚Äôs work. ‚ÄúAnd it‚Äôs very, very scary for economists because people should be responding to price signals. And men are not. It‚Äôs a fact in need of an explanation.‚Äù</p>
</blockquote>
<p>It‚Äôs always a little strange to read an economist remark that potential returns to education have been rising and so more people should get an education, but this same economist somehow not realize that the <em>continued presence of this free lunch indicates it is not free at all</em>. Look at how the trend of increasing education has stalled out:</p>
<figure>
<img alt="Education attainment climbed dramatically in the 20th century, but its growth has flattened recently (source: Census)" height="477" src="./images/2011-census-19472003uspopulationeducationalattainment.png" title="https://kaysteiger.files.wordpress.com/2011/09/screen-shot-2011-09-27-at-9-13-27-am.png http://www.theatlantic.com/business/archive/2011/09/the-value-of-college-is-a-growing-b-flat-c-falling-d-all-of-the-above/245746/" width="443"/><figcaption>‚ÄúEducation attainment climbed dramatically in the 20th century, but its growth has flattened recently (source: Census)‚Äù</figcaption>
</figure>
<p>Apparently markets work and people respond to incentives - <em>except</em> when it comes to education, and there people simply aren‚Äôt picking up those $100 bills laying on the ground and have been not picking them up for decades for some reason<a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a>, as the share of income accruing to ‚Äòlabor‚Äô falls both in <a href="http://www.clevelandfed.org/research/commentary/2012/2012-13.cfm" title="Labor's Declining Share of Income and Rising Inequality">the USA</a> and <a href="http://faculty.chicagobooth.edu/brent.neiman/research/KN.pdf" title="'The Global Decline of the Labor Share', Karabarbounis &amp; Neiman 2013">worldwide</a> I see. (In England, there‚Äôs evidence that college graduates were still being successfully absorbed in the ‚Äô90s and earlier, although apparently there weren‚Äôt relatively many during those periods<a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a>.) <a href="http://econlog.econlib.org/archives/2014/02/why_the_college.html">What do bad students know that good economists don‚Äôt?</a></p>
<p><a href="http://en.wikipedia.org/wiki/David%20Ricardo" title="Wikipedia: David Ricardo">David Ricardo</a> (inventor of the much-cited - by optimists and anti-neo-luddites - comparative advantage concept) <a href="http://www.econlib.org/library/Ricardo/ricP7.html#Ch.31,%20On%20Machinery" title="Chapter 31, 'On Machinery', _On the Principles of Political Economy and Taxation_">changed his mind</a> about whether technological unemployment was possible, but he thought it was possible only under certain conditions; <a href="./docs/2013-sachs.pdf" title="Smart Machines and Long-Term Misery">Sachs &amp; Kotlikoff 2013</a> gives a multi-generational model of suffering. Most economists, though, continue to dismiss this line of thought, saying that technological changes and structural unemployment <a href="http://www.igmchicago.org/igm-economic-experts-panel/poll-results?SurveyID=SV_0IAlhdDH2FoRDrm">are real</a> but things will work themselves out somehow. Robin Hanson, for example, <a href="http://www.overcomingbias.com/2009/10/take-both-econ-tech-seriously.html">seems to think that</a> and he‚Äôs a far better economist than me and has thought a great deal about AI and <a href="http://hanson.gmu.edu/aigrow.pdf" title="'Economic Growth Given Machine Intelligence', Hanson">the economic implications</a>. Their opposition to Neo-Luddism is about the only reason I remain uncertain, because otherwise, the data for the economic troubles starting in 2007, and especially the unemployment data, seem to match nicely. From a <a href="http://www.richmondfed.org/publications/research/economic_brief/2011/pdf/eb_11-09.pdf">Federal Reserve brief</a> (principally arguing that the data is better matched by a model in which the longer a worker remains unemployed, the longer they are likely to remain unemployed):</p>
<blockquote>
<p>For most of the post-World War II era, unemployment has been a relatively short-lived experience for the average worker. Between 1960 and 2010, the average duration of unemployment was about 14 weeks. The duration always rose during recessions, but relatively quick upticks in hiring after recessions kept the long-term unemployment rate fairly low. Even during the two ‚Äújobless recoveries‚Äù that followed the 1990-91 and 2001 recessions, the peak shares of long-term unemployment were 21% and 23%, respectively. But the 2007-09 recession represents a marked departure from previous experience: the average duration has increased to 40 weeks, and the share of long-term unemployment remains high more than two years after the official end of the recession.<a href="#fn36" class="footnoteRef" id="fnref36"><sup>36</sup></a> Never before in the postwar period have the unemployed been unemployed for so long.</p>
</blockquote>
<p>The <em>Economist</em> <a href="http://www.economist.com/blogs/babbage/2011/11/artificial-intelligence" title="Difference Engine: Luddite legacy">asked in 2011</a>:</p>
<blockquote>
<p>But here is the question: if the pace of technological progress is accelerating faster than ever, as all the evidence indicates it is, why has unemployment remained so stubbornly high - despite the rebound in business profits to record levels? Two-and-a-half years after the Great Recession officially ended, unemployment has remained above 9% in America. That is only one percentage point better than the country‚Äôs joblessness three years ago at the depths of the recession. The modest 80,000 jobs added to the economy in October were not enough to keep up with population growth, let alone re-employ any of the 12.3m Americans made redundant between 2007 and 2009. Even if job creation were miraculously nearly to triple to the monthly average of 208,000 that is was in 2005, it would still take a dozen years to close the yawning employment gap caused by the recent recession, says Laura D‚ÄôAndrea Tyson, an economist at University of California, Berkeley, who was chairman of the Council of Economic Advisers during the Clinton administration.</p>
</blockquote>
<p>And lays out the central argument for neo-Luddism, why ‚Äúthis time is different‚Äù:</p>
<blockquote>
<p>Thanks to tractors, combine harvesters, crop-picking machines and other forms of mechanisation, agriculture now accounts for little more than 2% of the working population. Displaced agricultural workers then, though, could migrate from fields to factories and earn higher wages in the process. What is in store for the Dilberts of today? Media theorist <a href="http://en.wikipedia.org/wiki/Douglas%20Rushkoff" title="Wikipedia: Douglas Rushkoff">Douglas Rushkoff</a> (<em>Program or Be Programmed</em> and <em>Life Inc</em>) would argue ‚Äúnothing in particular.‚Äù Put bluntly, few new white-collar jobs, as people know them, are going to be created to replace those now being lost-despite the hopes many place in technology, innovation and better education.</p>
<p>The argument against the Luddite Fallacy rests on two assumptions: one is that machines are tools used by workers to increase their productivity; the other is that the majority of workers are capable of becoming machine operators. What happens when these assumptions cease to apply - when machines are smart enough to become workers? In other words, when capital becomes labour. At that point, the Luddite Fallacy looks rather less fallacious‚Ä¶In his analysis [<em>Lights in the Tunnel</em>], Mr [Martin] Ford noted how technology and innovation improve productivity exponentially, while human consumption increases in a more linear fashion. In his view, Luddism was, indeed, a fallacy when productivity improvements were still on the relatively flat, or slowly rising, part of the exponential curve. But after two centuries of technological improvements, productivity has ‚Äúturned the corner‚Äù and is now moving rapidly up the more vertical part of the exponential curve. One implication is that productivity gains are now outstripping consumption by a large margin.</p>
</blockquote>
<p>The American oddities began before the current recession:</p>
<blockquote>
<p>Unemployment increased during the 2001 recession, but it subsequently fell almost to its previous low (from point A to B and then back to C). In contrast, job openings plummeted-much more sharply than unemployment rose-and then failed to recover. In previous recoveries, openings eventually outnumbered job seekers (where a rising blue line crosses a falling green line), but during the last recovery a labor shortage never emerged. The anemic recovery was followed in 2007 by an increase in unemployment to levels not seen since the early 1980s (the rise after point C). However, job openings fell only a little-and then recovered. The recession did not reduce hiring; it just dumped a lot more people into an already weak labor market.<a href="#fn37" class="footnoteRef" id="fnref37"><sup>37</sup></a></p>
</blockquote>
<p>And then there is the well-known example of Japan. Yet overall, both Japanese, American, and global wealth continue to grow. The hopeful scenario is that all we are suffering is temporary pains, which will eventually be grown out of, as <a href="http://en.wikipedia.org/wiki/John%20Maynard%20Keynes" title="Wikipedia: John Maynard Keynes">John Maynard Keynes</a> forecast in his 1930 essay <a href="http://www.econ.yale.edu/smith/econ116a/keynes1.pdf">‚ÄúOptimism in a Terrible Economy‚Äù</a>:</p>
<blockquote>
<p>At the same time technical improvements in manufacture and transport have been proceeding at a greater rate in the last ten years than ever before in history. In the United States factory output per head was 40 per cent greater in 1925 than in 1919. In Europe we are held back by temporary obstacles, but even so it is safe to say that technical efficiency is increasing by more than 1 per cent per annum compound‚Ä¶For the moment the very rapidity of these changes is hurting us and bringing difficult problems to solve. Those countries are suffering relatively which are not in the vanguard of progress. We are being afflicted with a new disease of which some readers may not yet have heard the name, but of which they will hear a great deal in the years to come‚Äìnamely, technological unemployment. This means unemployment due to our discovery of means of economising the use of labour outrunning the pace at which we can find new uses for labour. But this is only a temporary phase of maladjustment. All this means in the long run that mankind is solving its economic problem. I would predict that the standard of life in progressive countries one hundred years hence will be between four and eight times as high as it is to-day. There would be nothing surprising in this even in the light of our present knowledge. It would not be foolish to contemplate the possibility of afar greater progress still.</p>
</blockquote>
 
<section id="evaluation" class="level3">
<h3>Evaluation</h3>
<p>Of course, as plausible as this all looks, that doesn‚Äôt mean much. Anyone can cherrypick a bunch of quotes and citations. When <a href="Prediction%20markets#how-i-make-predictions">making predictions</a>, there are a few heuristics or principles I try to apply, and it might be worth applying a few here.</p>
<p>The specification seems fairly clear: the Neo-Luddite claim, in its simplest form predicts that ever fewer people will be able to find employment in undistorted free markets. We can see other aspects as either tangents (will people be able to consume due to a Basic Income or via capital ownership?) or subsets (the Autor thesis of polarization would naturally lead to an overall increase in unemployment). The due date is not clear, but we can see the Neo-Luddite thesis as closely linked to artificial intelligences, and 2050 would be as good a due date as any inasmuch as I expect to be alive then &amp; AI will have matured substantially (if we date serious AI to 1960 then 2012 is a bit past halfway) &amp; many predictions like Ray Kurzweil‚Äôs will have been verified or falsified.</p>
<p>The probability part of a prediction is the hard part. Going in order (the latter heuristics aren‚Äôt helpful):</p>
<ol type="1">
<li><p>‚ÄúWhat does the prediction about the future world imply about the present world?‚Äù</p>
<p>What would we expect in a world in which the Neo-Luddite thesis were true?</p>
<ul>
<li>first and foremost, we would expect both software &amp; hardware to continue improving. Both are true: Moore‚Äôs law continues despite the breakdown in chip frequencies, and AI research forges on with things like deep neural networks being deployed at scale by companies such as Google. If we did not see improvement, that would be extremely damaging to the thesis. However, this is a pretty boring retrodiction to make: technology has improved for so many centuries now that it would be surprising if the improvements had suddenly stopped, and if it had, why would anyone be taking this thesis seriously? It‚Äôs not like anyone worries over the implications of a philosopher‚Äôs stone for forex.</li>
<li><p>More meaningfully: capital &amp; labor increasingly cease to be complements, and become substitutes. We would expect gradually rising disemployment as algorithms &amp; software &amp; hardware were refined and companies learned when employees could be replaced by technological substitutes, with occasional jumps as idiosyncratic breakthroughs were made for particular tasks. We would expect returns on capital to increase, and we would expect that employees with un-substitutable skills or properties would increase wealth. This seems sort of true: STEM-related salaries in particular fields seem to be steady and tech companies continue to complain that good software engineers are hard to find (and Congress should authorize ever more H-1B visas) with consequences such as skyrocketing San Francisco real estate as tech companies flock there to find the rare talent they require, which is the sort of ‚Äúsuperstar effect‚Äù we would expect if human beings with certain properties were intrinsically rare &amp; valuable and the remainder just so much useless dross that hold back a business or worse. This is particularly striking when we note that it has never been cheaper or easier to become a software engineer as adequate computer hardware is dirt cheap &amp; <em>all</em> necessary software is available for free online &amp; instructional materials likewise, and it‚Äôs unclear how barriers like certification could matter when programmers are producing objective products - either a website is awesome and works, or it doesn‚Äôt.</p>
On the other hand, I also read of booming poor economies like China or Africa where wages are rising in general and unemployment seems to be less of a concern. This might fit the Autor model of polarization if we figure that those booming economies are pricing human labor so cheap that it outcompetes software/robots/etc, in which case we would expect to see these countries hit a ‚Äúwall‚Äù where only a part of their populations can pass the ‚Äòvalley of death‚Äô to reach the happy part of the polarized economy but the rest of the population is now struggling to be cheap enough to compete with the capital-alternatives. I‚Äôm not sure I see this. Yes, there are a lot of robotic factories being set up in China now, but does that really mean anything important on China‚Äôs scale? What‚Äôs a few million robots in a country of 1.3 billion people? If China does wind up falling into what looks like a <a href="http://en.wikipedia.org/wiki/middle%20income%20trap" title="Wikipedia: middle income trap">middle income trap</a>, that would be consistent with the Autor model, I think, and strengthen this retrodiction.</li>
<li>As technology is mobile and can easily be sold or exported, we would expect to see this general trend in many wealthy Western countries. This is a serious weak point of my knowledge thus far: I simply don‚Äôt know what it really looks like in eg Japan or England or Germany. Are they seeing similar things to my factoids about the USA?</li>
</ul></li>
<li><p>‚ÄúBase rates‚Äù here is essentially applying the <a href="http://wiki.lesswrong.com/wiki/Outside_view">Outside View</a></p>
<p>The main problem here is that it‚Äôs very difficult to rebut the Outside View: the Luddite thesis has, it seems, failed many times in the past; why expect <em>this</em> time to be any different? The historical horse example is amusing, certainly, but there could be many factors separating horses from ordinary people. To this, I don‚Äôt have any good reply. Even if the thesis is ‚Äúright‚Äù from the perspective of 1000 years from now, there is good reason to be chary of expecting it to happen in my lifetime. Computers themselves furnish a great many examples of people who, with vision and deep insight not shared by the people who ridiculed them as techno-utopians, correctly foresaw things like the personal computer or the Internet or online sales - and started their companies too early. The best I can say is that software/AI seems completely &amp; qualitatively different from earlier technologies like railroads or assembly lines, in that they are performing deeply human mental functions that earlier technologies did not come anywhere near: the regulator of a steam engine is solving a problem <em>so</em> much simpler than an autonomous car solves that it‚Äôs hard to even see them as being even theoretically related in exerting control on processes by feedback processes. The dimmest human could productively use contemporary technologies, where today we struggle to find subsidized jobs for the mentally handicapped where they are even just not a net loss.</p></li>
</ol>
<p>From these musings, I think we can extract a few warning signs which would indicate the Neo-Luddite thesis breaking down:</p>
<ul>
<li>global economic growth stopping</li>
<li>AI research progress stopping</li>
<li>Moore‚Äôs law in terms of FLOPS/$ breaking down</li>
<li>decreased wealth inequality (eg. Gini) in the First World</li>
<li>increases in population working</li>
</ul>
<p>Daniel Kahneman has an interesting thinking technique he calls the ‚Äúpre-mortem‚Äù, where you ask yourself: ‚Äúassume it‚Äôs the future, and my confident predictions have completely failed to come true. What went wrong?‚Äù Looking back, if the Neo-Luddite thesis fails, I think the most likely explanation for what I‚Äôve seen in the USA would be something related to globalization &amp; China in particular: the polarization, increased disemployment, increasing need for technical training etc, all seem explainable by those jobs heading overseas, exacerbated by other factors such as domestic politics (Bush‚Äôs tax cuts on the rich?) and maybe things like the structural unemployment relating to existing workers having difficulty switching sectors or jobs but new workers being able to adapt. If this is so, then I think we would expect the trends to gradually ameliorate themselves: older workers will die off &amp; retire, new workers will replace them, new niches and jobs will open up as the economy adapts, China‚Äôs exponential growth will result in catchup being completed within 2 or 3 decades, and so on.</p>
</section>
<section id="external-links" class="level3">
<h3>External links</h3>
<ul>
<li><a href="http://www.theatlantic.com/business/archive/2013/01/the-end-of-labor-how-to-protect-workers-from-the-rise-of-the-robots/267135/">‚ÄúThe End of Labor: How to Protect Workers From the Rise of Robots‚Äù</a>, Noah Smith</li>
<li><a href="http://www.motherjones.com/print/223026">‚ÄúWelcome, Robot Overlords. Please Don‚Äôt Fire Us?‚Äù</a>, Kevin Drum</li>
<li><a href="http://www.forbes.com/sites/modeledbehavior/2013/05/13/inequality-in-the-robot-future/">‚ÄúInequality In The Robot Future‚Äù</a>, Karl Smith</li>
<li><a href="http://www.overcomingbias.com/2013/05/robot-econ-primer.html">‚ÄúRobot Econ Primer‚Äù</a>, Robin Hanson</li>
<li><a href="http://lesswrong.com/lw/hh4/the_robots_ai_and_unemployment_antifaq/">‚ÄúThe Robots, AI, and Unemployment Anti-FAQ‚Äù</a>, Eliezer Yudkowsky</li>
</ul>
</section>
</section>
<section id="iq-race" class="level2">
<h2>IQ &amp; race</h2>
<p><a href="http://en.wikipedia.org/wiki/Race%20and%20intelligence" title="Wikipedia: Race and intelligence">This one</a> may be even more inflammatory than supporting nicotine, but it‚Äôs an important entry on any honest list. I never doubted that IQ was in part hereditary (Stephen Jay Gould aside, this is too obvious - what, everything from drug responses to skin and eye color would be heritable <em>except</em> the most important things which would have a huge effect on reproductive fitness?), but all the experts seemed to say that diluted over entire populations, any tendency would be non-existent. Well, OK, I could believe that; visible traits consistent over entire populations like skin color might differ systematically because of sexual selection or something, but why not leave IQ following the exact same bell curve in each population? There was no specific thing here that made me start to wonder, more a gradual undermining (Gould‚Äôs work like <em><a href="http://en.wikipedia.org/wiki/The%20Mismeasure%20of%20Man" title="Wikipedia: The Mismeasure of Man">The Mismeasure of Man</a></em> being <a href="http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1001071">completely dishonest</a> is one example - with enemies like that‚Ä¶) as I continued to read studies and wonder why Asian model minorities did so well, and a lack of really convincing counter-evidence like one would expect the last two decades to have produced - given the politics involved - if the idea were false. And one can always ask oneself: suppose that intelligence was meaningful, and did have a large genetic component, and the likely genetic ranking East Asians &gt; Caucasian &gt; Africans; in what way would the world, or its history (eg the growth of the Asian tigers vs Africa, or the different experiences of discriminated-against minorities in the USA), look different than it does now?</p>
<section id="mu" class="level3">
<h3><em>Mu</em></h3>
<p>It‚Äôs worth noting that the IQ wars are a rabbit hole you can easily dive down. The literature is vast, spans all sorts of groups, all sorts of designs, from test validities to sampling to statistical regression vs causal inference to forms of bias; every point is hotly debated, the ways in which studies can be validly critiqued are an education in how to read papers and look for how they are weak or make jumps or some of the data just looks wrong, and you‚Äôll learn every technical requirement and premise and methodological limitation because the opponents of that particular result will be sure to bring them up if it‚Äôll at all help their case.</p>
<p>In this respect, it‚Äôs a lot like the feuds in biblical criticism over issues like <a href="http://en.wikipedia.org/wiki/Historicity%20of%20Jesus" title="Wikipedia: Historicity of Jesus">whether Jesus existed</a>, or the long philosophical debate over the <a href="http://en.wikipedia.org/wiki/existence%20of%20God" title="Wikipedia: existence of God">existence of God</a>. There too is an incredible amount of material to cover, by some really smart people (what did geeks do before science and modernity? well, for the most part, they seem to have done theology; consider how much time and effort Isaac Newton reportedly <a href="http://en.wikipedia.org/wiki/Isaac%20Newton%27s%20occult%20studies%23Alchemical%20research" title="Wikipedia: Isaac Newton's occult studies#Alchemical research">spent on alchemy</a> and <a href="http://en.wikipedia.org/wiki/Isaac%20Newton%27s%20religious%20views" title="Wikipedia: Isaac Newton's religious views">his own Biblical studies</a>, or the sheer brainpower that must‚Äôve been spent over the centuries in rabbinical studies). You could learn a lot about the ancient world or the incredibly complex chain of transmission of the Bible‚Äôs constituents in their endless varieties and how they are put together into a single canonical modern text, or the other countless issues of <a href="http://en.wikipedia.org/wiki/textual%20criticism" title="Wikipedia: textual criticism">textual criticism</a>. An awful lot, indeed. One could, and people as smart or smarter than you <em>have</em>, lose one‚Äôs life in exploring little back-alleys and details.</p>
<p>If, like most people, you‚Äôve only read a few papers or books on it, your opinion (whatever that is) is worthless and you probably don‚Äôt even realize how worthless your opinion is, how far you are from actually grasping the subtleties involved and having a command of all the studies and criticisms of said studies. I exempt myself from this only inasmuch as I have realized how little I still know after all my reading. No matter how tempting it is to think that you may be able to finally put together the compelling refutation of God‚Äôs existence or to demonstrate that Jesus‚Äôs divinity was a late addition to his gospel, you won‚Äôt make a dent in the debate. In other words, these can become forms of <a href="http://xkcd.com/356/">nerd sniping</a> and intellectual crack. ‚ÄúIf only I compile a few more studies, make a few more points - <em>then</em> my case will become clear and convincing, and people on the Internet will stop <a href="http://xkcd.com/386/">being wrong</a>!‚Äù</p>
<p>But having said that, and admiring things like Plantinga‚Äôs free will defense, and the subtle logical issues in formulating it and the lack of any really concrete evidence for or against Jesus‚Äôs existence, do I take the basic question of God seriously? No. The theists‚Äô rearguard attempts and ever more ingenious explanations and indirect pathways of reasons and touted miracles fundamentally do not add up to an existing whole. The universe does not look anything like a omni-benevolent/powerful/scient god was involved, a great deal of determined effort has failed to provide any convincing proof, there not being a god is consistent with all the observed processes and animal kingdom and natural events and material world we see, and so on. The persistence of the debate reflects more what motivated cognition can accomplish and the weakness of existing epistemology and debate. Unfortunately, this could be equally well-said by someone on the other side of the debate, and in any case, I cannot communicate my <em>gestalt</em> impression of the field to anyone else. I don‚Äôt expect anyone to be the least bit swayed by what I‚Äôve written here.</p>
<p>So why be interested in the topics at all? If you cannot convince anyone, if you cannot learn the field to a reasonable depth, and you cannot even communicate well what convinced you, <em>why bother</em>? In the spirit of <a href="http://www.paulgraham.com/identity.html">keeping one‚Äôs identity small</a>, I say: it‚Äôs not clear at all. So you should know in advance whether you want to take the red pill and see how far down the rabbit hole you go before you finally give up, or you take the blue pill and be an onlooker as you settle for a high-level overview of the more interesting papers and issues and accept that you will only have that and a general indefensible assessment of the state of play.</p>
<p>My own belief is that as interesting as it is, you should take the blue pill and not adopt any strong position but perhaps (if it doesn‚Äôt take too much time) point out any particularly naive or egregious holes in argument, by people who are simply wrong or don‚Äôt realize how little they know or how slanted a view they have received from the material they‚Äôve read. It‚Äôs sad to not reach agreement with other people, dangerous to ignore critics, tempting to engage trolls - but life is too short to keep treading the same ground.</p>
<p>The reason for IQ is this: yes, <a href="http://humanvarieties.org/2013/03/29/cryptic-admixture-mixed-race-siblings-social-outcomes/">Murray failed to organize</a> a definitive genetic study. It hasn‚Äôt happened yet even though it‚Äôs more important than most of the trivialities that get studied in population genetics (like historical movements of random groups). I don‚Äôt need to explain why this would be the case even if people on the environmentalist side of the IQ wars were confident they were right. But the massive fall in genome sequencing costs (projected to be <a href="http://lesswrong.com/lw/9em/open_thread_january_1531_2012/5s7c">&lt;$1000 by ~2014</a>) means that large human datasets <em>will</em> be produced, and the genetics directly examined, eliminating entire areas of objections to the previous heredity studies. And at some point, some researcher will manage the study - some group inside or outside the USA will fund it, at some point a large enough genetic database will be cross-referenced against IQ tests and existing racial markers. We already see some of this in research: <a href="./docs/2013-rietveld.pdf" title="GWAS of 126,559 Individuals Identifies Genetic Variants Associated with Educational Attainment">Rietveld et al 2013</a> found 3 SNPs simply by pooling existing databases of genetics data &amp; correlating against schooling. I don‚Äôt know when the definitive paper will come out, if it‚Äôll be this year, or by 2020, although I would be surprised if there was still nothing by 2030; but it will happen and it will happen relatively soon (for a debate going on for the past century or more). Genome sequencing is simply going to be too cheap for it to not happen. By 2030 or 2040, I expect the issue will be definitively settled in the same way earlier debates about the validity of IQ tests were eventually settled (even if the public hasn‚Äôt yet gotten the word, the experts all concede that IQ tests are valid, reliable, not biased, and meaningful predictors of a wide variety of real-world variables).</p>
</section>
<section id="value-of-information" class="level3">
<h3>Value of Information</h3>
<p>What is the direct <a href="Zeo#value-of-information-voi">value</a> of learning about IQ? Speaking of it in terms of money may not be the best approach, so instead we can split the question up into a few different sub-questions:</p>
<ol type="1">
<li><p>how much do your efforts lead to additional information?</p>
In this case, not much. I would have to be very arrogant to think I can go through a large fraction of the literature and evaluate it better than the existing authorities like Nisbett or Flynn or Jensen. I have no advantages over them.</li>
<li><p>would this information-gathering be expensive?</p>
Yes. A single paper can take an hour to read well, and a technical book weeks. There are hundreds of papers and dozens of books to learn. The mathematics and statistics are nontrivial, and sooner or later, one will have to learn them in order to evaluate the seriousness of criticisms for oneself. The time spent will not have been throw-away recreational time, either, like slumming on the couch watching TV, but will be one‚Äôs highest-quality time, which could have been spent learning other difficult material, working, meaningfully interacting with other people, and so on. Given the <a href="DNB%20FAQ#aging">decline with age</a> of fluid intelligence, one may be wasting a non-trivial fraction of one‚Äôs lifetime learning.</li>
<li><p>will new information come in the absence of your efforts?</p>
Yes. My interest does not materially affect when the final genetic studies will be conducted.</li>
<li><p>what decisions or beliefs would the additional information change?</p>
<p>Suppose the environmentalists were 100% right and the between-race genetics were a negligibly small factor. Regardless, the topic of IQ and its correlates and what it predicts does not live and die based on there being a genetic factor to average IQ differences between groups; if the admixture and genetics studies turn in a solid estimate of 0, IQ will still predict lifetime income, still predict crime rates, still predict educational scores, and so on.</p>
<p>In contrast, some of the other topics have very concrete immediate implications. Switching from occultism/theism to atheism implies many changed beliefs &amp; choices; a near vs far Singularity has considerable consequences for retirement planning, if nothing else; while Neo-Luddism has implications for both career choice and retirement planning; attitudes towards fiction and nicotine also cash out in obvious ways. Of the topics here, perhaps only Communism and the American Revolution are as sterile in practical application.</p></li>
</ol>
<p>So, I try not to spend too much time thinking about this issue: the results will come in regardless of my opinion, and unlike other issues here, does not materially affect my worldview or suggest action. Given this, there‚Äôs no reason to invest your life in the topic! It has no practical ramifications for you, discussing the issue can only lead to negative consequences - and on the intellectual level, no matter how much you read, you‚Äôll always have nagging doubts, so you won‚Äôt get any satisfaction. You might as well just wait patiently for the inevitable final answer.</p>
</section>
<section id="further-reading" class="level3">
<h3>Further reading</h3>
<p>For people who are <em>not</em> content to wait and see who was right after all, here are some random online entry points into the IQ wars:</p>
<ul>
<li>Jensen‚Äôs original 1969 <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.138.980&amp;rep=rep1&amp;type=pdf">‚ÄúHow Much Can We Boost IQ and Scholastic Achievement?‚Äù</a>; for comparison, note carefully what <a href="http://scottbarrykaufman.com/wp-content/uploads/2012/01/Nisbett-et-al.-2012.pdf" title="Intelligence: New Findings and Theoretical Developments">Nisbett et al 2012</a> review actually says compared to what many laymen believe, and consider also whether its description of DNB would be recognizable to a thorough reader of my <a href="DNB%20FAQ" title="Go to wiki page: DNB%20FAQ">DNB FAQ</a>/<a href="DNB%20meta-analysis">meta-analysis</a>.</li>
<li>the <a href="http://www.udel.edu/educ/gottfredson/reprints/1997mainstream.pdf">consensus reply to <em>The Bell Curve</em></a>, the <a href="http://en.wikipedia.org/wiki/Mainstream%20Science%20on%20Intelligence" title="Wikipedia: Mainstream Science on Intelligence">Mainstream Science on Intelligence</a> statement</li>
<li><a href="http://www.udel.edu/educ/gottfredson/reprints/1997whygmatters.pdf">‚ÄúWhy <em>g</em> Matters: The Complexity of Everyday Life‚Äù</a>, Gottfredson 1997</li>
<li>Cosma Shalizi on how <a href="http://vserver1.cscs.lsa.umich.edu/~crshalizi/weblog/520.html" title="Yet More on the Heritability and Malleability of IQ">the ‚Äòheredity‚Äô metric doesn‚Äôt mean what you naively think it does</a> (but note that Shalizi seems to spend more time trying to <a href="http://vserver1.cscs.lsa.umich.edu/~crshalizi/weblog/495.html" title="Those Voices Again">come up with holes</a> allowing for the possibility of IQ being meaningless or entirely causally unrelated to genetics due to the weakness of correlational/observational methods - criticisms which fall into a general category of critiques which <a href="http://www.hss.cmu.edu/philosophy/glymour/glymour1998.pdf" title="What Went Wrong? Reflections on Science by Observation and the Bell Curve">Glymour 1998</a> points out applies to almost all results in social science. On the other hand, Shalizi‚Äôs essay <a href="http://vserver1.cscs.lsa.umich.edu/~crshalizi/weblog/523.html">‚Äú<em>g</em>, a Statistical Myth‚Äù</a> seems, as far as I understand it, to be essentially irrelevant since the point of <em>g</em> is that positive correlations exist at all, not that tests with a factor can be devised; see <a href="http://humanvarieties.org/2013/04/03/is-psychometric-g-a-myth/">‚ÄúIs psychometric <em>g</em> a myth?‚Äù</a>).</li>
<li><a href="http://occidentalascent.wordpress.com/2012/06/10/the-facts-that-need-to-be-explained/">‚ÄúThe facts that need to be explained‚Äù</a></li>
<li><a href="http://humanvarieties.org/2013/01/15/100-years-of-testing-negro-intelligence/">collection of black-white gap datapoints</a> (<a href="http://humanvarieties.org/2013/01/15/secular-changes-in-the-black-white-cognitive-ability-gap/">graphed over time</a>)</li>
<li><a href="http://web.archive.org/web/20130114194332/http://squid314.livejournal.com/350090.html">‚ÄúEpistemic learned helplessness‚Äù</a></li>
<li><a href="http://squid314.livejournal.com/346391.html">‚ÄúThe Biodeterminist‚Äôs Guide to Parenting‚Äù</a></li>
<li><a href="http://www.paulgraham.com/say.html">‚ÄúWhat You Can‚Äôt Say‚Äù</a>, Paul Graham</li>
</ul>
</section>
</section>
</section>
<section id="see-also" class="level1">
<h1>See Also</h1>
<ul>
<li><a href="Prediction%20markets">Predictions</a></li>
</ul>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<section id="miller-on-neo-luddism" class="level2">
<h2>Miller on neo-Luddism</h2>
<p>From chapter 13 of <a href="http://www.amazon.com/Singularity-Rising-Surviving-Thriving-Dangerous/dp/1936661659/?tag=gwernnet-20"><em>Singularity Rising</em></a>, James Miller 2012:</p>
<blockquote>
<blockquote>
<p>‚ÄúThere‚Äôs this stupid myth out there that AI has failed, but AI is everywhere around you every second of the day. People just don‚Äôt notice it. You‚Äôve got AI systems in cars, tuning the parameters of the fuel injection systems. When you land in an airplane, your gate gets chosen by an AI scheduling system. Every time you use a piece of Microsoft software, you‚Äôve got an AI system trying to figure out what you‚Äôre doing, like writing a letter, and it does a pretty damned good job. Every time you see a movie with computer-generated characters, they‚Äôre all little AI characters behaving as a group. Every time you play a video game, you‚Äôre playing against an AI system.‚Äù ‚ÄìRodney Brooks, Director, MIT Computer Science and AI Laboratory 288</p>
</blockquote>
<p>‚Ä¶In the next few decades, all of my readers might have their market value decimated by intelligent machines. Should you be afraid? Fear of job-destroying technology is nothing new. During the eighteenth century, clothing manufacturers in England replaced some of their human laborers with machines. In response, a gang supposedly led by one Ned Ludd smashed a few machines owned by a sock maker. Ever since then, people opposing technology have been called Luddites. Luddites are correct in thinking that machines can cause workers to lose their jobs. But overall, in the past, job-destroying machine production has overall greatly benefited workers. ‚ÄúDestroying jobs‚Äù sounds bad - like something that should harm an economy. But the benefits of job destruction become apparent when you realize that an economy‚Äôs most valuable resource is human brains. If a businessman figured out how to make a product using less energy or fewer materials, we would applaud him because the savings could be used to produce additional goods. The same holds true when we figure out how to make something using less labor. If you used to need 1,000 workers to run your sock factory but you can now produce the same number of socks by employing only 900 workers, then you probably would (and perhaps even should) fire the other 100. Although in the short run these workers will lack jobs, in the long run they will likely find new employment and expand the economy.</p>
<p>The obliteration of most agricultural jobs has been a huge source of economic growth for America. In 1900, farmers made up 38% of the Americans workforce, whereas now they constitute less than 2% of it.289 Most of the displaced agricultural laborers found work in cities. Yet despite the massive decrease in farming jobs, the United States has steadily produced more and more food since 1900. Agricultural technology gave the American people a ‚Äúfree lunch,‚Äù in which we got more food with less effort, making obesity a greater threat to American health than calorie deprivation. Technology raises wages by increasing worker productivity. In a free-market economy, the value of the goods an employee produces for his employer roughly determines his wage. A farmer with a tractor produces more food than one with just a hoe. Consequently, modern farmers earn higher wages than they would if they lived in a world deprived of modern agricultural technology. In rich nations, wages have risen steadily over the last two hundred years because technology keeps increasing worker productivity. But will this trend continue? Past technologies never completely eliminated the need for humans, so fired sock workers usually found other employment. But a sufficiently advanced AI possessing a robot body might outperform people at every single task.</p>
<p>‚Ä¶If a Kurzweilian merger doesn‚Äôt occur, sentient AIs might compete directly with people in the labor market. Let‚Äôs now explore what happens to human wages if these AIs become better than humans at every task. Adam Smith, the great eighteenth-century economist, explained that everyone benefits from trade if each participant makes what he is best at. So, for example, if I‚Äôm better at making boots than you are, but you have more skill at making candles, then we would both become richer if I produced your boots and you made my candles. But what if you‚Äôre more skilled at making both boots and candles? What if, compared to you, I‚Äôm worse at doing everything? Adam Smith never answered this question, but nineteenth-century economist David Ricardo did. This question is highly relevant to our future, as an AI might be able to produce every good and service at a lower cost than any human could, and if we turn out to have no economic value to the advanced artificial intelligences, then they might (at best) ignore us, depriving humanity of any benefits of their superhuman skills.</p>
<p>Most people intuitively believe that mutually beneficial trades take place only when each person has an area of absolute excellence. But Ricardo‚Äôs theory of comparative advantage shows that trade can make everyone better off regardless of a person‚Äôs absolute skill because everyone has an area of comparative advantage. I‚Äôll illustrate Ricardo‚Äôs nineteenth-century theory with a twenty-first-century example involving donuts and anti-gravity flying cars. Let‚Äôs assume that humans can‚Äôt make flying cars, but an AI can; and although people can make donuts, an AI can make them much faster than we can. Let‚Äôs pretend that at least one AI likes donuts, where donuts represent anything a human can make that an AI would want. Here‚Äôs how a human and AI could both benefit from trade: a human could offer to give an AI many donuts in return for a flying car. The trade could clearly benefit the human. If it gets enough donuts, the AI also benefits from the trade. To see how this could work, imagine that (absent trade) it takes an AI one second to make a donut. The AI could build a flying car in one minute.</p>
<ul>
<li>Time needed for an AI to make a donut: one second</li>
<li>Time needed for an AI to make a flying: car one minute</li>
</ul>
<p>A human then offers the following deal to the AI: Build me a flying car and I will give you one hundred donuts. It will take you one minute to make me a flying car. In return for this flying car you get something that would cost you 100 seconds to make. Consequently, our trade saves you 40 seconds. As the AI‚Äôs powers grew, people could still gain from trading with it. If, say, it took the AI only one nanosecond to make a donut and 60 nanoseconds to make a flying car, then it would still become better off by trading 100 donuts for 1 flying car. 292 In general, as an AI becomes more intelligent, trading with humans will save it less time, but what the AI can do with this saved time goes up, especially since a smarter AI would probably gain the capacity to create entirely new categories of products. An AI might trade 100 donuts for a flying car, but an ‚ÄúAI+‚Äù would trade this number of donuts for a wormhole generator. Modern economists use Ricardo‚Äôs theory of comparative advantage to show how rich and poor countries can benefit by trading with each other. Understanding Ricardo‚Äôs theory causes almost all economists to favor free trade. If we substitute ‚Äúhumanity‚Äù for ‚Äúpoor countries‚Äù and AI for ‚Äúrich countries,‚Äù then Ricardo gives us some hope for believing that even self-interested advanced artificial intelligences would want to take actions that bestow tremendous economic benefits on mankind.</p>
<p>MAGIC WANDS</p>
<p>In the previous scenario, I implicitly assumed that producing donuts doesn‚Äôt require the use of some ‚Äúfactor of production.‚Äù A factor of production is an essential nonhuman element needed to create a good. Factors of production for donuts include land, machines, and raw materials, and without these factors, a person (no matter how smart and hardworking) can‚Äôt make donuts. Instead of using the intimidating and boring term ‚Äúfactor of production,‚Äù I‚Äôm going to say that to make a good or produce a service you need the right ‚Äúmagic production wand,‚Äù with the wand being the appropriate set of factors of production. For example, a donut maker needs a donut wand.</p>
<p>If a relatively small number of wands existed and no more could be created, then all of the wands would go to AIs. Let‚Äôs say donuts sell for $1 each and an AI could use a donut wand to produce one million donuts, whereas a human using the same wand could make only a thousand donuts. A human would never be willing to pay more than $1,000 for the wand, whereas an AI would earn a huge profit if it bought a donut wand for, say, $10,000. Even if a human initially owned a donut wand, he would soon sell it to an AI. Human wand owners in this situation would benefit from AIs because AIs would greatly raise the market value of wands. Human workers who had never had a wand would become impoverished because they couldn‚Äôt produce anything.</p>
<p>The Roman Republic‚Äôs conquests in the first century BC effectively stripped many Roman citizens of their production wands. In the early Republic, poor citizens had access to wands, as they were often hired to farm the land of the nobility. But after the Republic‚Äôs conquests brought in a huge number of slaves, the noblemen had their slaves use almost all of the available land wands. Cheap slave labor enriched the landowning nobility by reducing their production costs. But abundant slave labor impoverished non-landowning Romans by depriving them of wands. Cheap slave labor contributed to the fall of the Roman Republic. As Roman inequality increased, common soldiers came to rely on their generals for financial support. The troops put loyalty to their generals ahead of loyalty to the Roman state. Generals such as Sulla and Julius Caesar took advantage of their increased influence over their troops to propel themselves to absolute political power. Caesar sought to reduce the social instability caused by slaves by giving impoverished free Roman citizens new lands from the territories Rome had recently conquered. Caesar essentially created many new wands and gave them to his subjects.</p>
<p>Although AIs will use wands, they will also likely help create them. For example, using nanotechnology, they might be able to build dikes to reclaim land from the ocean. Or perhaps they‚Äôll figure out how to terraform Mars, making Martian land cheap enough for nearly any human to afford. AIs could also figure out better ways to extract raw materials from the earth or invent new ways to use raw materials, resulting in each product needing fewer wands. The future of human wages might come down to a race between the number of AIs and the quantity of wands. Economist and former artificial-intelligence programmer Robin Hanson has created a highly counterintuitive theory of why (in the long run) AIs will destroy nearly all human jobs: they will end up using all of the production wands (<a href="http://hanson.gmu.edu/aigrow.pdf">‚ÄúEconomic Growth Given Machine Intelligence‚Äù</a>).</p>
<p>‚Ä¶What I‚Äôve written so far about the economics of emulations probably seems correct to most readers. After all, if we can make copies of extraordinarily bright and productive people and employ multiple copies of them in science and industry, then we should all get richer. The results would be similar to what would happen if a select few nursery schools became so fantastically good that each year they turned ten thousand toddlers into von Neumann-level geniuses who then immediately entered the workforce.</p>
<p>Robin Hanson, however, isn‚Äôt willing to rely on mere intuition when analyzing the economics of emulations. Robin realizes that if, after we have emulations, the price of computing power continues to fall at an exponential rate, then emulations will soon become extraordinarily cheap. If you combine extremely inexpensive emulations with a bit of economic theory, you get a seemingly crazy result, something that you might think is too absurd to ever happen. But Robin, ever the bullet-eater, refuses to turn away from his conclusion. Robin thinks that in the long run, emulations will drive wages down to almost zero, pushing most of the people who are unfortunate enough to rely on their wages into starvation-because emulations will kick us back into a ‚ÄúMalthusian trap.‚Äù Arguably, humanity‚Äôs greatest accomplishment was escaping the Malthusian trap. Thomas Malthus, a nineteenth-century economist, believed that starvation would ultimately strike every country in the entire world. Malthus wrote that if a population is not facing starvation, people in that population will have many children who grow up, get married, and have even more children. A country with an abundance of food, Malthus wrote, is one with an increasing population. Unfortunately, in Malthus‚Äôs time, as the size of a country‚Äôs population went up, it became more difficult to feed everyone in the country. Eventually, when the population got large enough, many starved. Only when lots of people were dying of starvation would the country‚Äôs population stabilize. Consequently, Malthus believed that all countries were trapped in one of two situations:</p>
<ol type="1">
<li>Many people are starving.</li>
<li>The population is growing, and so many will eventually starve.</li>
</ol>
<p>‚Ä¶Pretend that someone emulates Robin and places the software in the public domain. Anyone can now freely copy e-Robin, although it still costs something to buy enough computing power to run him on, say a hundred thousand dollars a year. A profit-maximizing business would employ an e-Robin if the e-Robin brought the business more than $100,000 a year in revenue. After Moore‚Äôs law pushes the annual hardware costs of an e-Robin down to a mere $1, then a company would hire e-Robins as long as each brought the business more than $1 per annum. What happens to the salary of bio-Robin if you can hire an e-Robin for only a dollar? David Ricardo implicitly knew the answer to that question. Ricardo wrote that if it costs 5,000 pounds to rent a machine, and this machine could do the work of 100 men, the total wages paid to 100 men will never be greater than 5,000 pounds because if the total wages were higher, manufacturers would fire the workers and rent the machine.296 Applying Ricardo‚Äôs theory to an economy with emulations tells us that, if an emulation can do whatever you can do, your wage will never be higher than what it costs to employ the emulation. The question now is whether, if it‚Äôs extremely cheap to run an e-Robin, these e-Robins would still earn high salaries and therefore allow the original Robin to bring home a decent paycheck. Unfortunately, the answer is no because if an e-Robin were earning much more than what it costs to run an e-Robin, then it would be profitable for businesses to create many more of them. Companies will keep making copies of their emulations until they no longer make a profit by producing the next copy. A general rule of economics is that the more you have of something, the smaller its value. For example, even though water is inherently much more useful than diamonds because there is so much more water than diamonds, the price of water is much lower. If anyone can freely copy e-Robin, then the free market would drive the wage of an e-Robin down to what it costs to run one.</p>
<p>‚Ä¶Even if the emulations push wages to almost zero, lots of bio-humans would be much richer than they would be in a world without emulations. Though ancient Rome was in a Malthusian trap, its landowning nobility was rich. When you have lots of people and little land, the land is extremely valuable because it‚Äôs cheap to hire people to work the land and there is great demand for the food the land produces. Similarly, if there are a huge number of emulations and relatively few production wands, then the wands become extraordinarily valuable. True, the emulations will increase the number of production wands. But because it‚Äôs so cheap to copy software, if the price of hardware is low enough, there will always be a lot more labor than wands. Consequently, bio-people who own wands will become fantastically rich. Even though you would lose your job, the value of your stock portfolio might jump a thousandfold‚Ä¶Since bio-humans could earn almost nothing by working, our prosperity would depend on our owning property or receiving welfare payments. If bio-humans became masters of an emulation-filled Malthusian world, keeping most of the wealth for ourselves, then we would live like a landed aristocracy that receives income from taxing others and renting out our agricultural lands to poor peasants. Eliezer Yudkowsky doubts this possibility:</p>
<blockquote>
<p>‚ÄúThe prospect of biological humans sitting on top of a population of [emulations] that are smarter, much faster, and far more numerous than bios while having all the standard human drives, and the bios treating the [emulations] as standard economic [value] to be milked and traded around, and the [emulations sitting] still for this for more than a week of bio time - this does not seem historically realistic.‚Äù301</p>
</blockquote>
<p>Carl Shulman, one of the most knowledgeable people I‚Äôve spoken to about Singularity issues, goes even further than Yudkowsky. He writes that since obsolescence would frequently kill entire categories of emulations, bio-humans could maintain total control of the government and economy only if the emulations regularly submitted ‚Äúto genocide, even though the overwhelming majority of the population expects the same thing to happen to it soon.‚Äù302</p>
<p>‚Ä¶Robin thinks that if we behaved intelligently and maintained good relations with the emulations, bio-humans could safely take up to around 5% of the world‚Äôs economic output without having the emulations seek to destroy us. By appropriating 5% rather than the preponderance of the world‚Äôs income, we would ensure that the emulations would have less to gain from killing us and taking our stuff. But as power flows from money, having less income would make us less able to defend ourselves from any emulations that did wish to strip us of our wealth. Robin is optimistic about our ability to keep this 5%. He correctly notes that many times in history wealthy but weak groups have managed to keep their property for long periods of time. For example, many Americans over the age of seventy are rich even though they no longer contribute to economic production. These Americans, if standing without allies, would not have the slightest chance of prevailing in a fight in which Americans in their twenties joined together to steal the property of seniors. Yet it‚Äôs almost inconceivable that this would happen. Similarly, in many societies throughout human history, rich senior citizens have enjoyed secure property rights even though they would quickly lose their wealth if enough younger men colluded to take it from them. Even senior citizens whom dementia has made much less intelligent than most of their countrymen are still usually able to retain their property. Robin mentioned to me that tourists from rich countries are generally secure when they travel to poor nations even when the tourists are clearly undefended wealthy outsiders. A wealthy white American wearing expensive Western clothes could probably walk safely through most African villages even if the villagers knew that the American earns more in a day than a villager does in a year.</p>
<p>‚Ä¶As Robin points out, throughout human history most revolts broke out when conditions had improved for the poorest in society.305 And great revolutions have almost invariably been led by the rich. George Washington and Thomas Jefferson were wealthy landowners; Lenin‚Äôs father, born a serf, had risen through government service to the rank of a nobleman and married a woman of wealth, and Trotsky‚Äôs father was an illiterate but prosperous landlord; Julius Caesar was the first- or second-wealthiest individual alive at the time he overthrew the Roman Republic; and the mutiny on the Bounty was led by an officer. Perhaps bio-humans would have more to fear from the small number of wealthy emulations than from the emulations facing starvation.</p>
</blockquote>
<!--

## Bitcoin

I have read a number of books dealing with the history of the computing industry (eg. in no particular order, [_iWoz_](http://www.amazon.com/Iwoz-Invented-Personal-Computer-Along/dp/140015328X), [_Steve Jobs_](http://www.amazon.com/Steve-Jobs-ebook/dp/B004W2UBYW/), [_The Making of Prince of Persia_](http://www.amazon.com/Making-Prince-Persia-Jordan-Mechner-ebook/dp/B005WUE6Q2), [_It's Behind You_](http://bizzley.com/), [_What the Dormouse Said_](http://www.amazon.com/What-Dormouse-Said-Counterculture-Personal/dp/0143036769), [_The Media Lab: Inventing the Future_](http://www.amazon.com/The-Media-Lab-Inventing-Future/dp/0140097015) all come to mind; for discussion, see [Book reviews]()), and so many people in those eras seem to have almost stumbled into becoming millionaires or billionaires, ludicrously lucked out on fortunes. Pennies at the right time and place would become thousands of dollars; when Microsoft IPOed, how much growth was left to its stock? Quite a bit. If a kindly time traveler offered to take you back to the 1970s, wouldn't it be tempting? To go back and see if one could become one of the lucky people who rode the famous names like Apple & Microsoft & Oracle to great wealth? Of course, the down side is that in reading those books, one realizes just what an agonizing experience going back could be. What must it have been like to be [Douglas Engelbart](!Wikipedia), giving the [Mother of All Demos](!Wikipedia) and not seeing the technologies go mainstream for 30 years and even then in a half-assed form? What must it have been like to be [Alan Kay](!Wikipedia), working on ideas for high-powered tablets and interactive graphical applications, ideas which would be only *partially* realized in the 2010s with the iPads? But no, we would be worse off than them, because we would know exactly how things would be in the future without any doubt, and we would have had experience with powerful high-level systems. It must be a special kind of Sisyphean suffering to spend one's days writing assembler, exploiting platform-specific quirks, doing the insane tricks that programmers in the '70s *had* to, and doing so in the full awareness of how these perversions are forced by hardware limits, how their code will be discarded without a thought when another platform comes around. And of course, if one took the time traveler's offer and went back and endured the working conditions, there is still no guarantee that one will grab the winning lottery ticket. It's an interesting hypothetical for the unusual tradeoff involved. But it is just a hypothetical. There is no way to go back in time, and no way for one to know how well one would have fared, and whether one would have successfully discerned the gold from the pyrite. One can always dream about it. Like a more realistic version of winning the lottery.

On the other hand, we can imagine a similar scenario for the not-so-distant 1990s - it had the same combination of extraordinary fortunes being made simultaneous with revolting constraints on things like bandwidth or web browser complexity. As it happens, I was in elementary school for much of the 1990s, so for me, the counterfactual of becoming wealth in the 1990s is just as out of reach as imaging myself in the 1970s. So the 1990s and before are right out. But that still leaves the 2000s and 2010s: vast fortunes are still being made - YouTube, Facebook, Tumblr, Dropbox, Twitter, to lazily pick a few. I have not been involved in any of these. So does that crush the fantasy? No, not really, because even if these companies are contemporary with me, I would have had to essentially found them in order to made the fortunes. The fantasy is more like buying at the Microsoft or Apple IPOs, not being the company founder. Each of those named companies reached their values privately (Dropbox) or by being acquired while still private (YouTube, Tumblr) or had IPOs priced so high that shares were no better an investment than any other companies' (Facebook and likely Twitter). Even someone gifted in 2005 with a vision of Facebook's future would have had remarkable difficulty profiting off this knowledge: Facebook hired few employees and the early employees were generally picked for web development skills which the person may well not have, Facebook took relatively little outside investment and mostly from angels who could kick in large sums, and the IPO offered minimal reward for investors as Facebook was able to auction the shares off so highly priced that its original shareholders ate most of the surplus (a canny move underappreciated by most observers). Fundamentally, the startup market is very illiquid. This illiquidity can salve my self-esteem.

This final defense fails.

It fails because there *was* a startup with extreme liquidity and ease of investment, where the returns were measured in thousands of percent, where I was uniquely placed to understand and invest in it, which I did in fact understand, which was contemporary with me, where acquaintances did realize serious sums. The fantasy fails: I had my chance and I blew it. I can't tell myself that in past decades I would have been the winner, because I have already proven myself one of the losers.

The chance was Bitcoin.

Satoshi Nakamoto began thinking up Bitcoin in 2006 or so, sent the earliest emails in 2007/2008 and it began in earnest in 3 January 2009 when Satoshi kicked off the genesis block with [a quote from a British newspaper](/docs/2011-davis#chancellor-on-brink-of-second-bailout-for-banks). 8 days later ($0/‡∏ø), techie [Hal Finney](!Wikipedia "Hal Finney (cypherpunk)") (who I knew from LessWrong and even [wrote a poem about](/fiction/Dying Outside)) [commented on](http://www.mail-archive.com/cryptography@metzdowd.com/msg10152.html) the Cryptography mailing list where Bitcoin had originally been announced:

> As an amusing thought experiment, imagine that Bitcoin is successful and becomes the dominant payment system in use throughout the world. Then the total value of the currency should be equal to the total value of all the wealth in the world. Current estimates of total worldwide household wealth that I have found range from \$100 trillion to \$300 trillion. With 20 million coins, that gives each coin a value of about \$10 million. So the possibility of generating coins today with a few cents of compute time may be quite a good bet, with a payoff of something like 100 million to 1! Even if the odds of Bitcoin succeeding to this degree are slim, are they really 100 million to one against? Something to think about...

It would be more accurate to compare Bitcoin to the turnover of all transactions rather than wealth, but this still serves as an indicator that even then, the potential of Bitcoin as a truly [disruptive innovation](!Wikipedia) was recognizable. It was a oblique approach to the impossible cypherpunk dream of a truly decentralized digital currency, accomplished through a tradeoff that even the people on the Cryptography mailing list found both remarkable and distasteful. Besides the simple fact - a decentralized digital currency which solved double-spends! - Bitcoin included machinery for complicated transactions like escrows or time-delayed payments which as of 2013 still have not been exploited on a wide scale.

Hal was interested in Bitcoin, played with the client, kept the network running by mining some coins, received a few transactions from Satoshi Nakamoto but didn't become involved. He saw value in Bitcoin, though, and held on to the few thousand worthless coins he had. A little while later, [he developed ALS](https://bitcointalk.org/index.php?topic=155054.0 "Bitcoin and me (Hal Finney)"), even as the Bitcoin economy kept growing from nothing. His coins will be his estate and support his children.

While I don't directly involve myself much in cryptography, I still occasionally read about it and try to follow the news through Bruce Schneier and others. Bitcoin made waves, and eventually they reached me. Did they reach me in the second, or the first bubble in 2013? No. Checking my logs, I see I learned of Bitcoin prior to 6 January 2011 - which is when I shared an interesting blog post by [Aaron Swartz](!Wikipedia), ["Squaring the Triangle: Secure, Decentralized, Human-Readable Names"](http://www.aaronsw.com/weblog/squarezooko). That day, a bitcoin cost \$0.3 on MtGox.

I must have been reading up on Bitcoin at that point, because my post does not scoff at the concept or call it a Ponzi or doomed to fail. A month later, 11 February ($1/‡∏ø), I was discussing it freely on `#lesswrong` with an even earlier adopter, and I pointed out the same thing that Finney did (although I would not read his email for years to come) - that Bitcoin was a disruptive innovation which disrupted huge multi-billion-dollar payment systems or even entire currencies:

    20:04 gwern> chelz: Wei Dai is a pretty cool dude. one of the LWers I admire most
    ...
    12:54 gwern> ...insurance. given the existing success of
        bitcoin, there's a nontrivial (somewhere between 0.01 and 0%) chance bitcoin
        will replace a major fiat currency in which case a few bitcoins would be very
        valuable
    13:03 gwern> (I mean, figure said currency has outstanding, say,
        just 10 billion in USD equivalent and is replaced by bitcoin, that'd put each
        bitcoin at 476 units-usd)
    13:03 gwern> (so each bitcoin would have an EV of 4.76USD, 0.01 * 476)

As of 8 November 2013 ($340/‡∏ø), the market cap of Bitcoin has reached \$4b and is already not far from that estimate. A better estimate might be to compare Bitcoin to PayPal: PayPal is privately held by eBay and the market cap of its stock hard to estimate, but the estimates range from [>$12b](http://blogs.reuters.com/breakingviews/2010/03/18/ebay-spinoff-of-paypal-looks-inevitable/) to [$40b](http://www.quora.com/If-Paypal-were-a-standalone-company-how-much-would-it-be-worth) and so my estimate of \$10b there was really quite low.

A few days later, on 16 February 2011 ($1.05/‡∏ø), the topic came up [on LW](http://lesswrong.com/lw/4cs/making_money_with_bitcoin/) as a potential good investment for tech-types willing to make expected-value calculation and understand innovations before the rest of the world. [I wrote](http://lesswrong.com/lw/4cs/making_money_with_bitcoin/3ljd)

> After thinking about it and looking at the current community and the surprising amount of activity being conducted in bitcoins, I estimate that bitcoin has somewhere between 0 and 0.1% chance of eventually replacing a decent size fiat currency, which would put the value of a bitcoin at anywhere upwards of \$10,000 a bitcoin. (Match the existing outstanding number of whatever currency to 21m bitcoins. Many currencies have billions or trillions outstanding.)
>
> Cut that in half to \$5000, and call the probability an even 0.05% (average of 0 and 0.1%), and my expected utility/value for possessing a coin is \$25 a bitcoin (5000*0.005)...

I was interested in mining as a way to acquire some Bitcoins without spending some of my little cash (I was essentially unemployed at this point):

>> If you value 1 BTC at \$25, you should just buy BTC with cash directly.
>
> I should, but I'm not confident enough of my analysis to spend anything but some electricity & time, and I'm very low on money anyway.

On 24 March 2011 ($0.9/‡∏ø), an early adopter acquaintance gave me ‡∏ø1 on condition I spent it on [Witcoin](https://en.bitcoin.it/wiki/Witcoin), an interesting twist on Reddit where karma was measured in bitcoins and so one profited off good comments and submissions. I did profit and earned ‡∏ø3. That acquaintance, incidentally, despite his many other flaws, had boundless faith in Bitcoin and had accumulated ~‡∏ø400 he planned to hold onto, come hell or high water - which he has. ("There is nothing so disturbing to one's well-being and judgment as to see a friend get rich")

http://bitcoincharts.com/charts/mtgoxUSD#rg60zczsg2011-01-6zeg2013-11-09ztgSzm1g10zm2g25zv
http://lesswrong.com/lw/4cs/making_money_with_bitcoin/3ljp
http://lesswrong.com/lw/4cs/making_money_with_bitcoin/3ljd?context=1#3ljd
https://encrypted.google.com/search?num=100&q=gwern+bitcoin+site:predictionbook.com&sei=AJZ9Ur1I8cbgA_HlgZAP&gbv=2#gbv=2&q=gwern+bitcoin+-%22most+recent%22+site:predictionbook.com%2Fpredictions%2F

09:19:16 <@gwern> newherePC: I know this because I've never had security issues with my laptop, I have easily bought-and-held shares on prediction markets for
                  long periods, I have a fair amount of confidence in my calibration and betting skills, I've never been scammed using bitcoin whether on silk
                  road or #bitcoin-otc, or burned by a shutdown because I distrust all bitcoin services and keep fudns with them for as short a time as
                  possible. so basically, if I had ...
09:19:16 <@gwern> ... bought $10 of bitcoin in february 2011 because I thought it might go to $10, I would still have it

-->
</section>
</section>
<section class="footnotes">
<hr/>
<ol>
<li id="fn1"><p><a href="http://en.wikipedia.org/wiki/Char%20Aznable" title="Wikipedia: Char Aznable">Char Aznable</a>, <em><a href="http://en.wikipedia.org/wiki/Mobile%20Suit%20Gundam" title="Wikipedia: Mobile Suit Gundam">Mobile Suit Gundam</a></em>; this line stayed with me after watching <em><a href="http://en.wikipedia.org/wiki/Otaku%20no%20Video" title="Wikipedia: Otaku no Video">Otaku no Video</a></em> - one does not care, indeed.<a href="#fnref1">‚Ü©</a></p></li>
<li id="fn2"><p>‚Äú‚Ä¶Once we have taken on a definite form, we do not lose it until death.‚Äù ‚ÄìChapter 2 of the <em><a href="http://en.wikipedia.org/wiki/Chuang-tzu" title="Wikipedia: Chuang-tzu">Chuang-tzu</a></em>; <a href="http://en.wikipedia.org/wiki/Thomas%20Cleary" title="Wikipedia: Thomas Cleary">Thomas Cleary</a>‚Äôs translation in <a href="http://www.amazon.com/Vitality-Energy-Spirit-Sourcebook-Shambhala/dp/1590306880/?tag=gwernnet-20"><em>Vitality, Energy, Spirit: A Taoist Sourcebook</em></a> (1991), ISBN 978-0877735199<a href="#fnref2">‚Ü©</a></p></li>
<li id="fn3"><p>One of the few good bits of Kathryn Schulz‚Äôs 2011 book <a href="http://www.amazon.com/Being-Wrong-Adventures-Margin-Error/dp/0061176052/?tag=gwernnet-20"><em>Being Wrong</em></a> (part 1) is where she does a more readable version of Wittgenstein‚Äôs observation (<em>PI</em> Pt II, p.¬†162), ‚ÄúOne can mistrust one‚Äôs own senses, but not one‚Äôs own belief. If there were a verb meaning‚Äùto believe falsely,&quot; it would not have any [meaningful] first person, present indicative.&quot; Her version goes:</p>
<blockquote>
<p>But before we can plunge into the experience of being wrong, we must pause to make an important if somewhat perverse point: there <em>is</em> no experience of being wrong.</p>
<p>There is an experience of <em>realizing</em> that we are wrong, of course. In fact, there is a stunning diversity of such experiences. As we‚Äôll see in the pages to come, recognizing our mistakes can be shocking, confusing, funny, embarrassing, traumatic, pleasurable, illuminating, and life-altering, sometimes for ill and sometimes for good. But by definition, there can‚Äôt be any particular feeling associated with simply <em>being</em> wrong. Indeed, the whole reason it‚Äôs possible to be wrong is that, while it is happening, you are oblivious to it. When you are simply going about your business in a state you will later decide was delusional, you have no idea of it whatsoever. You are like the coyote in the <a href="http://en.wikipedia.org/wiki/Wile%20E.%20Coyote%20and%20Road%20Runner" title="Wikipedia: Wile E. Coyote and Road Runner"><em>Road Runner</em></a> cartoons, after he has gone off the cliff but before he has looked down. Literally in his case and figuratively in yours, you are already in trouble when you feel like you‚Äôre still on solid ground. So I should revise myself: it does feel like something to be wrong. It feels like being right.</p>
</blockquote>
<a href="#fnref3">‚Ü©</a></li>
<li id="fn4"><p>‚ÄúYou‚Äôre only as young as the last time you changed your mind.‚Äù ‚Äì<a href="http://en.wikipedia.org/wiki/Timothy%20Leary" title="Wikipedia: Timothy Leary">Timothy Leary</a> (quoted in <a href="http://www.amazon.com/Office-Yoga-Simple-Stretches-People/dp/0811826856/?tag=gwernnet-20"><em>Office Yoga: Simple Stretches for Busy People</em></a> (2000) by Darrin Zeer, p.¬†52)<a href="#fnref4">‚Ü©</a></p></li>
<li id="fn5"><p>‚ÄúEveryone thinks they‚Äôve won the Magical Belief Lottery. Everyone thinks they more or less have a handle on things, that they, as opposed to the billions who disagree with them, have somehow <em>lucked</em> into the one true belief system.‚Äù ‚Äì<a href="http://en.wikipedia.org/wiki/R.%20Scott%20Bakker" title="Wikipedia: R. Scott Bakker">R. Scott Bakker</a>, <a href="http://www.amazon.com/Neuropath-R-Scott-Bakker/dp/0765361574/?tag=gwernnet-20"><em>Neuropath</em></a><a href="#fnref5">‚Ü©</a></p></li>
<li id="fn6"><p>From <a href="http://en.wikipedia.org/wiki/E.T.%20Jaynes" title="Wikipedia: E.T. Jaynes">E.T. Jaynes</a>‚Äôs <a href="http://bayes.wustl.edu/etj/articles/general.background.ps.gz">‚ÄúBayesian Methods: General Background‚Äù</a>:</p>
<blockquote>
<p>As soon as we look at the nature of inference at this many-moves-ahead level of perception, our attitude toward probability theory and the proper way to use it in science becomes almost diametrically opposite to that expounded in most current textbooks. We need have no fear of making shaky calculations on inadequate knowledge; for if our predictions are indeed wrong, then we shall have an opportunity to improve that knowledge, an opportunity that would have been lost had we been too timid to make the calculations.</p>
<p>Instead of fearing wrong predictions, we look eagerly for them; it is only when predictions based on our present knowledge fail that probability theory leads us to fundamental new knowledge.</p>
</blockquote>
<p>From Wittgenstein‚Äôs <a href="http://www.amazon.com/Culture-Value-Ludwig-Wittgenstein/dp/0226904350/?tag=gwernnet-20"><em>Culture and Value</em></a>, MS 117 168 c: 17.2.1940:</p>
<blockquote>
<p>You can‚Äôt be reluctant to give up your lie &amp; still tell the truth.</p>
</blockquote>
<a href="#fnref6">‚Ü©</a></li>
<li id="fn7"><p>As quoted in page 207 of Genna Sosonko‚Äôs <a href="http://www.amazon.com/Russian-Silhouettes-Genna-Sosonko/dp/9056912933/?tag=gwernnet-20"><em>Russian Silhouettes</em></a> of <a href="http://en.wikipedia.org/wiki/Mikhail%20Botvinnik" title="Wikipedia: Mikhail Botvinnik">Mikhail Botvinnik</a>:</p>
<blockquote>
<p>He did not dissolve and he did not change. On the last pages of the book he is still the same Misha Botvinnik, pupil of the 157<sup>th</sup> School of United Workers in Leningrad and Komsomol member. He had not changed at all for seventy years, and, listening to his sincere and passionate monologue, one involuntarily thinks of Confucius: ‚ÄúOnly the most clever and the most stupid cannot change.‚Äù</p>
</blockquote>
<a href="#fnref7">‚Ü©</a></li>
<li id="fn8"><p>Socrates, Plato‚Äôs <em><a href="http://en.wikipedia.org/wiki/Gorgias%20%28dialogue%29" title="Wikipedia: Gorgias (dialogue)">Gorgias</a></em>, 458a (Zeyl translation)<a href="#fnref8">‚Ü©</a></p></li>
<li id="fn9"><p>I have a similar absence of story for my generally <a href="http://en.wikipedia.org/wiki/transhumanist" title="Wikipedia: transhumanist">transhumanist</a> beliefs, since I was born hearing-impaired and grew up using hearing aids. That technology could improve my natural condition, that the flesh was imperfect, or that scientific &amp; technological advancement was a good thing were not things I ever had to be argued into believing: I received those lessons daily when I took off my hearing aids, the world went silent, and I could no longer understand anyone around me. Or for that matter, when my hearing aids were periodically replaced &amp; upgraded. And how could I regard cyborgs as bizarre or horrible things when I myself verged on the cyborg?<a href="#fnref9">‚Ü©</a></p></li>
<li id="fn10"><p>One such argument is that miracles don‚Äôt work because we are too skeptical or don‚Äôt believe faithfully enough (this argument is also used in parapsychology, oddly enough). This is absurd, since much of the point of miracles in the Bible (and especially by saints &amp; missionaries) was to convert infidels or skeptics or wavering believers; and especially rings hollow when people like <a href="http://www.theatlantic.com/national/archive/2013/04/why-are-there-so-few-resurrected-corpses-in-the-united-states/274681/" title="Why Are There So Few Resurrected Corpses in the United States? Pat Robertson thinks it's because of the Ivy League">Pat Robertson</a> attempt to explain why miracles are generally reported from poor, superstitious, and uneducated <em>contemporary</em> areas like Africa:</p>
<blockquote>
<p>Cause people overseas didn‚Äôt go to Ivy League schools! [chuckles] Well, we‚Äôre so sophisticated. We think we‚Äôve got everything figured out. We know about evolution, we know about Darwin, we know about all these things that say God isn‚Äôt real. We know about all this stuff and if we‚Äôve been in many schools, the more advanced schools, we have been inundated with skepticism and secularism. And overseas they‚Äôre simple, humble, you tell them God loves them and they say ‚Äúokay he loves me.‚Äù And you tell them God will do miracles and they say ‚Äúokay, we believe you.‚Äù And that‚Äôs what God‚Äôs looking for. That‚Äôs why they have miracles.</p>
</blockquote>
<a href="#fnref10">‚Ü©</a></li>
<li id="fn11"><p>Whatever the truth may be, I stand staunchly by this point: a kid sees so much evidence and belief in God that he <em>ought</em> to rationally believe. <a href="http://unqualified-reservations.blogspot.com/2008/01/how-i-stopped-believing-in-democracy.html">Mencius Moldbug</a>:</p>
<blockquote>
<p>Most people are theists not because they were ‚Äòreasoned into‚Äô believing in God, but because they applied Occam‚Äôs razor at too early an age. Their simplest explanation for the reason that their parents, not to mention everyone else in the world, believed in God, was that God actually existed. The same could be said for, say, Australia. Dennett‚Äôs approach, which of course is probably ineffective in almost all cases, is to explain why, if God doesn‚Äôt exist, everyone knows who He is. How did this whole God thing happen? Why is it not weird that people believed in Him for 2000 years, but actually they were wrong?</p>
</blockquote>
<a href="#fnref11">‚Ü©</a></li>
<li id="fn12"><p>From a theology blog, <a href="https://web.archive.org/web/20131013084016/http://prosblogion.ektopos.com/archives/2012/03/trust-in-testim.html">‚ÄúTrust in testimony and miracles‚Äù</a>:</p>
<blockquote>
<p>‚Ä¶Harris found that children do not fall into either pattern. Pace the Humean account, he found that young children are readily inclined to believe extraordinary claims, such as that there are invisible organisms on your hands that can make you ill and that you need to wash off, and that there is a man who visits you each 24th December to bring presents and candy if you are nice (see e.g., <a href="http://www.gse.harvard.edu/news/features/harris/Harris%26Koenig2006.pdf" title="Trust in Testimony: How Children Learn About Science and Religion">Harris &amp; Koenig, 2006</a>, <em>Child Development</em>, 77, 505-524). But children are not blindly credulous either, as Reid supposed. In a series of experiments, Harris could show that even children of 24 months pay attention to the reliability of the testifier. When they see two people, one of which systematically misnames known objects (e.g., saying ‚Äúthat‚Äôs a bear‚Äù, while presenting a bottle), toddlers are less likely to trust later utterances by these unreliable speakers (when they name unfamiliar objects), and more likely to trust people who systematically gave objects their correct names (see e.g., <a href="http://isites.harvard.edu/fs/docs/icb.topic783896.files/Young%20childrens%20selective%20trust%20in%20informants.pdf" title="Young children's selective trust in informants">Paul L. Harris and Kathleen H. Corriveau</a> <em>Phil. Trans. R. Soc.</em> B 2011 366, 1179-1187.) Experiments by Mills and Keil show that 6-year-olds already take into account a testifier‚Äôs self-interest: they are more likely to believe someone who says he lost a race than someone who says he won it (<a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3034135/" title="The Development of Cynicism">Candice M. Mills and Frank C. Keil</a> <em>Psychological Science</em> 2005 16: 385).</p>
</blockquote>
<a href="#fnref12">‚Ü©</a></li>
<li id="fn13"><p>I sometimes wonder if this had anything to do with my later philosophy training; atheists make up something like 70% of respondents to the <a href="http://philpapers.org/surveys/results.pl">Philpapers survey</a>, and a critical ‚Äòreflective‚Äô style both correlates with and causes <a href="http://www.apa.org/pubs/journals/releases/xge-ofp-shenhav.pdf">lower belief in God</a>; another interesting correlation is that people on the autism spectrum (which I have often been told I must surely be on) seem to be <a href="http://csjarchive.cogsci.rpi.edu/proceedings/2011/papers/0782/paper0782.pdf">heavily agnostic or atheistic</a>.</p>
<p>For discussion of these points, see:</p>
<ol type="1">
<li><a href="http://lesswrong.com/r/discussion/lw/7o4/atheism_the_autism_spectrum/">‚ÄúAtheism &amp; the autism spectrum‚Äù</a></li>
<li><a href="http://lesswrong.com/lw/7rh/cognitive_style_tends_to_predict_religious">‚ÄúCognitive style tends to predict religious conviction‚Äù</a></li>
<li><a href="http://lesswrong.com/lw/aq6/on_the_etiology_of_religious_belief/">‚ÄúOn the etiology of religious belief‚Äù</a></li>
</ol>
<a href="#fnref13">‚Ü©</a></li>
<li id="fn14"><p>One of my favorite books on modern religion is Luhrmann‚Äôs 2012 <a href="http://www.amazon.com/When-God-Talks-Back-Understanding/dp/0307277275/?tag=gwernnet-20"><em>When God Talks Back</em></a>. It‚Äôs not that it‚Äôs fantastically written or researched, although I do like books where the author has done research themselves on the topic and cite a reasonable number of claims. I like it because, as the last chapter says, it provides a large part of the answer to the ‚Äúnonbeliever‚Äôs question‚Äù: in an age of zero miracles beyond the risible (‚Äúmy tumor went away after I prayed!‚Äù), with no gods thundering to crowds, and with the best philosophical arguments contenting themselves with the <em>logical possibility</em> of the god of the philosophers, how could anyone sincerely believe in supernatural beings and why isn‚Äôt a sort of practical agnosticism (‚Äúyeah, I don‚Äôt really believe, but church is where all my social activities are‚Äù) universal? Why are there so many fervent believers and some religions spreading rapidly while holding fairly constant in highly developed industrialized countries? The book answers that this absence is partly illusory: they do hear God‚Äôs voice, through a variety of auto-suggestive meditative practices which collectively constitute the <em><a href="http://en.wikipedia.org/wiki/sensus%20divinitatis" title="Wikipedia: sensus divinitatis">sensus divinitatis</a></em> that atheists are accused of sadly lacking, which combined with the other factors (the intuitiveness of supernatural beings pace the research in kids and evo-psych reasoning, the suppression of analytic thought, the social benefits, etc) maintains religion at its historical popularity in developed countries and spurs new growth in developing countries. (Africa is growing fantastically for both Islam and Christianity; and other developing countries experience their own versions of Japan‚Äôs ‚Äúrush hour of the gods‚Äù.)</p>
<p>Excerpts from Luhrmann 2012: <a href="https://plus.google.com/u/0/103530621949492999968/posts/8tQdy7kNfKE">preface</a> / <a href="https://plus.google.com/u/0/103530621949492999968/posts/G6Q4Z6ANMRb">1</a> / <a href="https://plus.google.com/u/0/103530621949492999968/posts/dgdcF6Qmc9B">2</a> / <a href="https://plus.google.com/u/0/103530621949492999968/posts/eXhmZUpBmCm">3</a> / <a href="https://plus.google.com/u/0/103530621949492999968/posts/5AvbwfuNmPy">4</a> / <a href="https://plus.google.com/u/0/103530621949492999968/posts/dbFEadqScJH">5</a> / <a href="https://plus.google.com/u/0/103530621949492999968/posts/47YUHeASv9w">6</a> / <a href="https://plus.google.com/u/0/103530621949492999968/posts/AThvaCXCSp2">7</a> / <a href="https://plus.google.com/u/0/103530621949492999968/posts/1ZKVCxrSwTR">8</a> / <a href="https://plus.google.com/u/0/103530621949492999968/posts/SH2SqzCtVE2">9</a> / <a href="https://plus.google.com/u/0/103530621949492999968/posts/5f7z4KYwE6Z">10</a><a href="#fnref14">‚Ü©</a></p></li>
<li id="fn15"><p>pg 120, <a href="http://www.amazon.com/When-London-Was-Capital-America/dp/0300178131/?tag=gwernnet-20"><em>When London was Capital of America</em></a>, Julie Flavell 2011:</p>
<blockquote>
<p>The British government hoped that a west sealed off from encroachments by whites, and where traders had to operate under the watchful eye of a British army detachment, would bring about good relations with the Indians. To the great discontent of speculators, in 1761 it was announced that all applications for land grants now had to go to London; no colonial government could approve them. The Proclamation of 1763 banned westward settlement altogether and instead encouraged colonists who wanted new lands to settle to the north in Quebec, and to the south in Florida. Within just a few years British ministers would be retreating from the Proclamation and granting western lands.</p>
</blockquote>
<a href="#fnref15">‚Ü©</a></li>
<li id="fn16"><p>From Wikipedia: ‚ÄòIt remains the deadliest war in American history, resulting in the deaths of 620,000 soldiers and an undetermined number of civilian casualties. According to John Huddleston, ‚Äú10% of all Northern males 20-45 years of age died, as did 30% of all Southern white males aged 18-40.‚Äù‚Äô<a href="#fnref16">‚Ü©</a></p></li>
<li id="fn17"><p><em>An Inquiry into the Nature and Causes of the Wealth of Nations </em>, <a href="http://www.marxists.org/reference/archive/smith-adam/works/wealth-of-nations/book04/ch02.htm">‚ÄúBook IV: On Systems of Political Economy‚Äù</a>:</p>
<blockquote>
<p>When the Act of Navigation was made, though England and Holland were not actually at war, the most violent animosity subsisted between the two nations‚Ä¶They are as wise, however, as if they had all been dictated by the most deliberate wisdom. National animosity at that particular time aimed at the very same object which the most deliberate wisdom would have recommended, the diminution of the naval power of Holland, the only naval power which could endanger the security of England.</p>
<p>The Act of Navigation is not favourable to foreign commerce, or to the growth of that opulence which can arise from it. The interest of a nation in its commercial relations to foreign nations is, like that of a merchant with regard to the different people with whom he deals, to buy as cheap and to sell as dear as possible. But it will be most likely to buy cheap, when by the most perfect freedom of trade it encourages all nations to bring to it the goods which it has occasion to purchase; and, for the same reason, it will be most likely to sell dear, when its markets are thus filled with the greatest number of buyers. The Act of Navigation, it is true, lays no burden upon foreign ships that come to export the produce of British industry. Even the ancient aliens‚Äô duty, which used to be paid upon all goods exported as well as imported, has, by several subsequent acts, been taken off from the greater part of the articles of exportation. But if foreigners, either by prohibitions or high duties, are hindered from coming to sell, they cannot always afford to come to buy; because coming without a cargo, they must lose the freight from their own country to Great Britain. By diminishing the number of sellers, therefore, we necessarily diminish that of buyers, and are thus likely not only to buy foreign goods dearer, but to sell our own cheaper, than if there was a more perfect freedom of trade. As defence, however it is of much more importance than opulence, the Act of Navigation is, perhaps, the wisest of all the commercial regulations of England.</p>
</blockquote>
<a href="#fnref17">‚Ü©</a></li>
<li id="fn18"><p>Nietzsche writes this summary of traditional philosophers to mock them, but isn‚Äôt there a great deal of truth in it? ‚ÄúHow could anything originate out of its opposite? Truth out of error or the pure and sunlike gaze of the sage out of lust? Such origins are impossible; whoever dreams of them is a fool.‚Äù<a href="#fnref18">‚Ü©</a></p></li>
<li id="fn19"><p>Friedrich Hayek, <a href="http://www.amazon.com/The-Constitution-Liberty-Definitive-Collected/dp/0226315398/?tag=gwernnet-20"><em>The Constitution of Liberty</em></a> (1960)<a href="#fnref19">‚Ü©</a></p></li>
<li id="fn20"><p>The rhetoric in the 1990s and early 2000s is amazing to read in retrospect; some of the claims were about as wrong as it is possible to be. For example, the CEO of <a href="http://en.wikipedia.org/wiki/Millennium%20Pharmaceuticals" title="Wikipedia: Millennium Pharmaceuticals">Millennium Pharmaceuticals</a> - not at all a small or fly-by-night pharmacorp - said <a href="http://www.fool.com/specials/2000/sp000322levin.htm">in 2000 it had high hopes for 6 drugs in human trials</a> and <a href="http://www.thestreet.com/story/10006531/1/the-upshot-the-kids-in-the-hall-at-hq.html">claimed in 2002</a> that thanks to genetic research it would have 1-2 drugs entering trials every year within 3 years, for 6-12 new drugs by 2011. As of October 2011, it has exactly 1 approved drug.<a href="#fnref20">‚Ü©</a></p></li>
<li id="fn21"><p>The subsequently cited review covers this; almost all of the famous increase in longevity by decades is due to the young:</p>
<blockquote>
<p><a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3163136/table/tab1/">Table 1</a> shows the average number of years of life remaining from 1900 to 2007 from various ages, combining both sexes and ethnic groups. From birth, life expectancy increased from 49.2 years (previously estimated at 47.3 years in these same sources) in 1900 to 77.9 in 2007, a gain of life expectancy of nearly 29 years and a prodigious accomplishment. The increase was largely due to declines in perinatal mortality and reduction in infectious diseases which affected mainly younger persons. Over this period, developed nations moved from an era of acute infectious disease to one dominated by chronic illness. As a result, life extension from age 65 was increased only 6 years over the entire 20th century; from age 75 gains were only 4.2 years, from age 85 only 2.3 years and from age 100 a single year. From age 65 over the most recent 20 years, the gain has been about a year <a href="http://www.ncbi.nlm.nih.gov/books/NBK44745/" title="Health, United States, 2009: With Special Feature on Medical Technology">[16]</a>.</p>
<p>Much confusion in longevity predictions comes from using projections of life expectancy at birth to estimate future population longevity <a href="http://info-centre.jenage.de/assets/pdfs/library/vaupel_NATURE_2010.pdf" title="'Biodemography of human ageing', Vaupel 2010">[18]</a>. For example, ‚ÄúIf the pace of increase in life expectancy (from birth) for developed countries over the past two centuries continues through the 21st century, most babies born since 2000 will celebrate their 100th birthdays‚Äù <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2810516/" title="'Ageing populations: the challenges ahead', Christensen et al 2009">29</a>. Note from the 100-year line of <a href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3163136/table/tab1/">Table 1</a> that life expectancies for centenarians would be projected to rise only one year in the 21st century, as in the 20th. Such attention-grabbing statements follow from projecting from birth rather than age 65, thus including infant and early life events to project ‚Äúsenior‚Äù aging, using data from women rather than both genders combined, cherry-picking the best data for each year, neglecting to compute effects of in-migration and out-migration, and others.</p>
</blockquote>
<p>Remarkably, some groups show a <em>decrease</em> in longevity; a centenarian in 1980 has an average remaining lifespan of 2.7 years, but in 2000, that has fallen to 2.6. There was an even larger reversal in 1940 (2.1) to 1960 (1.9). Younger groups show larger gains (eg. 85-year-olds had 6.0 years in 1980 and 6.3 in 2000), evidence for <a href="http://en.wikipedia.org/wiki/compression%20of%20morbidity" title="Wikipedia: compression of morbidity">compression of morbidity</a>.<a href="#fnref21">‚Ü©</a></p></li>
<li id="fn22"><p>If exponentials &amp; sigmoids really do explain Amara‚Äôs observation, that implies that there ought to be some sort of ‚Äúreverse‚Äù Amara effect: where an observer is at the top of the sigmoid and naively extrapolates that the long run will look very different from now - and it turns out that the long run looks identical to right now. Identifying reverse Amara effects is easier than regular Amara effects because one has the benefit of hindsight. For example, nuclear energy: it was initially a puzzling physics anomaly and a research problem at most - uranium was important to the world economy because it was used in things like the <a href="http://en.wikipedia.org/wiki/Haber-Bosch%20process" title="Wikipedia: Haber-Bosch process">Haber-Bosch process</a> which helped enable World War I. Even in the 1930s, it was more interesting than useful. Then suddenly nuclear reactors were demonstrated and atomic bombs transform the world in the 1940s, leading to widespread futurism predictions of ubiquitous nuclear energy ‚Äútoo cheap to meter‚Äù, but further use of nuclear technologies suddenly breaks down; with the honorable exception of nuclear medicine, the world in the 2010s looks pretty much identical to the 1950s. Given this, one would not be <em>very</em> surprised if in 2112, nuclear technologies were nothing but refinements of 2012 nuclear technologies.<a href="#fnref22">‚Ü©</a></p></li>
<li id="fn23"><p>Monte Carlo trees are very similar to the techniques used in one of the computable implementations of <a href="http://wiki.lesswrong.com/wiki/AIXI">AIXI</a>, incidentally: <a href="http://arxiv.org/abs/0909.0801">MC-AIXI</a> (<a href="http://www.vetta.org/2009/09/monte-carlo-aixi/">background</a>).<a href="#fnref23">‚Ü©</a></p></li>
<li id="fn24"><p>This is known as the ‚Äòoverhang‚Äô argument. The development and canonical form of it is unclear; it may simply be Singulitarian folklore-knowledge. Eliezer Yudkowsky, from the 2008 <a href="http://lesswrong.com/lw/wf/hard_takeoff/">‚ÄúHard Takeoff‚Äù</a>:</p>
<blockquote>
<p>Or consider the notion of sudden resource bonanzas. Suppose there‚Äôs a semi-sophisticated Artificial General Intelligence running on a cluster of a thousand CPUs. The AI has not hit a wall - it‚Äôs still improving itself - but its self-improvement is going so <em>slowly</em> that, the AI calculates, it will take another fifty years for it to engineer / implement / refine just the changes it currently has in mind. Even if this AI would go FOOM eventually, its current progress is so slow as to constitute being flatlined‚Ä¶</p>
<p>So the AI turns its attention to examining certain blobs of binary code - code composing operating systems, or routers, or DNS services - and then takes over all the poorly defended computers on the Internet. This may not require what humans would regard as genius, just the ability to examine lots of machine code and do relatively low-grade reasoning on millions of bytes of it. (I have a saying/hypothesis that a <em>human</em> trying to write <em>code</em> is like someone without a visual cortex trying to paint a picture - we can do it eventually, but we have to go pixel by pixel because we lack a sensory modality for that medium; it‚Äôs not our native environment.) The Future may also have more legal ways to obtain large amounts of computing power quickly.</p>
<p>‚Ä¶A subtler sort of hardware overhang, I suspect, is represented by modern CPUs have a 2GHz <em>serial</em> speed, in contrast to neurons that spike 100 times per second on a good day. The ‚Äúhundred-step rule‚Äù in computational neuroscience is a rule of thumb that any postulated neural algorithm which runs in realtime has to perform its job in less than 100 <em>serial</em> steps one after the other. We do not understand how to efficiently use the computer hardware we have now, to do intelligent thinking. But the much-vaunted ‚Äúmassive parallelism‚Äù of the human brain, is, I suspect, <a href="http://lesswrong.com/lw/k5/cached_thoughts/">mostly cache lookups</a> to make up for the sheer awkwardness of the brain‚Äôs <em>serial</em> slowness - if your computer ran at 200Hz, you‚Äôd have to resort to all sorts of absurdly massive parallelism to get anything done in realtime. I suspect that, if <em>correctly designed</em>, a midsize computer cluster would be able to get high-grade thinking done at a serial speed much faster than human, even if the total parallel computing power was less.</p>
<p>So that‚Äôs another kind of overhang: because our computing hardware has run so far ahead of AI <em>theory</em>, we have incredibly fast computers we don‚Äôt know how to use <em>for thinking</em>; getting AI <em>right</em> could produce a huge, discontinuous jolt, as the speed of high-grade thought on this planet suddenly dropped into computer time.</p>
<p>A still subtler kind of overhang would be represented by human <a href="http://lesswrong.com/lw/qk/that_alien_message/">failure to use our gathered experimental data efficiently</a><a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a>.</p>
</blockquote>
<p><a href="http://www.aleph.se/andart/archives/2010/10/why_early_singularities_are_softer.html">Anders Sandberg &amp; Carl Shulman</a> gave a 2010 talk on it; from the blog post:</p>
<blockquote>
<p>We give an argument for why - if the AI singularity happens - an early singularity is likely to be slower and more predictable than a late-occurring one‚Ä¶.</p>
<p>If you are on the hardware side, how much hardware do you believe will be available when the first human level AI occurs? You should expect the first AI to be pretty close to the limits of what researchers can afford: a project running on the future counterpart to Sequoia or the Google servers. There will not be much extra computing power available to run more copies. An intelligence explosion will be bounded by the growth of more hardware.</p>
<p>If you are on the software side, you should expect that hardware has continued to increase after passing ‚Äúhuman equivalence‚Äù. When the AI is finally constructed after all the human and conceptual bottlenecks have passed, hardware will be much better than needed to just run a human-level AI. You have a ‚Äúhardware overhang‚Äù allowing you to run many copies (or fast or big versions) immediately afterwards. A rapid and sharp intelligence explosion is possible.</p>
<p>This leads to our conclusion: if you are an optimist about software, you should expect an early singularity that involves an intelligence explosion that at the start grows ‚Äújust‚Äù as Moore‚Äôs law (or its successor). If you are a pessimist about software, you should expect a late singularity that is very sharp. It looks like it is hard to coherently argue for a late but smooth singularity.</p>
<p>‚Ä¶Note that sharp, unpredictable singularities are dangerous. If the breakthrough is simply a matter of the right insights and experiments to finally cohere (after endless disappointing performance over a long time) and then will lead to an intelligence explosion nearly instantly, then most societies will be unprepared, there will be little time to make the AIs docile, there are strong first-mover advantages and incentives to compromise on safety. A recipe for some nasty dynamics.</p>
</blockquote>
<p><a href="http://hplusmagazine.com/2011/07/08/future-technology-merger-or-trainwreck/">Jaan Tallinn</a> in 2011:</p>
<blockquote>
<p>It‚Äôs important to note that with every year the AI algorithm remains unsolved, the hardware marches to the beat of Moore‚Äôs Law - creating a massive hardware overhang. The first AI is likely to find itself running on a computer that‚Äôs several orders of magnitude faster than needed for human level intelligence. Not to mention that it will find an Internet worth of computers to take over and retool for its purpose.</p>
</blockquote>
<a href="#fnref24">‚Ü©</a></li>
<li id="fn25"><p>Or Robin Hanson‚Äôs paper, <a href="http://hanson.gmu.edu/aigrow.pdf">‚ÄúEconomic Growth Given Machine Intelligence‚Äù</a></p>
<blockquote>
<p>Machines complement human labor when they become more productive at the jobs they perform, but machines also substitute for human labor by taking over human jobs. At first, expensive hardware and software does only the few jobs where computers have the strongest advantage over humans. Eventually, computers do most jobs. At first, complementary effects dominate, and human wages rise with computer productivity. But eventually substitution can dominate, making wages fall as fast as computer prices now do. An intelligence population explosion makes per-intelligence consumption fall this fast, while economic growth rates rise by an order of magnitude or more.</p>
</blockquote>
<a href="#fnref25">‚Ü©</a></li>
<li id="fn26"><p>Intuitively, one would guess that the value of education and changes in it value would follow some sort of linear or exponential - more is better, less is worse. If the value of a high school diploma increases, an undergraduate ought to increase more, and postgraduate degrees increase even more, right? A ‚Äòhollowing-out‚Äô model, on the other hand, would seem to predict that there would be a sort of U-curve where the mediocre education is not worth what it costs and one would be better off not bothering with getting more education or sticking it out and getting a ‚Äòreal‚Äô degree. With that in mind, it is interesting <a href="http://blogs.wsj.com/economics/2011/09/19/only-advanced-degree-holders-see-wage-gains/">to look at the Census data</a>:</p>
<blockquote>
<p>In fact, new Census Bureau data show that if you divide the population by education, <em>on average</em> wages have risen only for those with graduate degrees over the past 10 years. (On average, of course, means that some have done better and some have done worse.) Here (thanks to economist Matthew Slaughter of Dartmouth College‚Äôs Tuck School of Business) are changes in U.S. workers wages as reported in the latest Census Bureau report, adjusted for inflation using the CPI-U-RS measure recommended by the Bureau of Labor Statistics:</p>
<figure>
<img alt="Change between 2000 and 2010 in inflation-adjusted average earnings by educational attainment" height="331" src="./images/2011-census-earningsbyeducation.jpg" title="http://si.wsj.net/public/resources/images/NA-BN371_Census_G_20110919135704.jpg" width="705"/><figcaption>‚ÄúChange between 2000 and 2010 in inflation-adjusted average earnings by educational attainment‚Äù</figcaption>
</figure>
</blockquote>
<a href="#fnref26">‚Ü©</a></li>
<li id="fn27"><p>Charles Murray reportedly cites statistics in <a href="http://www.amazon.com/Coming-Apart-State-America-1960-2010/dp/030745343X/?tag=gwernnet-20"><em>Coming Apart: The State of White America 1960-2010</em></a> that the disability rate for men - working class - was 2% in 1960; with more than half a century of medical progress, the rate has not fallen but risen to 10%.<a href="#fnref27">‚Ü©</a></p></li>
<li id="fn28"><p>And there have always been rumors that the moral hazard is <em>substantial</em>; eg. the psychiatrist Steve Balt, <a href="http://thoughtbroadcast.com/2011/09/04/how-to-retire-at-age-27/">‚ÄúHow To Retire At Age 27‚Äù</a> <a href="http://thelastpsychiatrist.com/2011/09/how_to_be_mean_to_your_kids.html">and commentary</a>.<a href="#fnref28">‚Ü©</a></p></li>
<li id="fn29"><p>From pg 13/340 of Bowles &amp; Jayadev 2006:</p>
<blockquote>
<p>Other differences in technology (or different distributions of labor across sectors of the economy) may account for some of the differences. However, the data on supervision intensity by manufacturing sector in five sub-Saharan African countries shown in Table 4 suggest large country effects independent of the composition of output. Supervisory intensities in Zambia‚Äôs ‚Äòwood and furniture‚Äô and ‚Äòfood processing‚Äô industries, are twice and five times Ghana‚Äôs respectively. A country-and-industry fixed effects regression indicates that Zambia‚Äôs supervision intensity conditioned on industrial structure is two and a half times Ghana‚Äôs. Of course these differences could reflect within sector variation among countries in output composition or technologies, but there is no way to determine how much (if any) of the estimated country effects are due to this. We also explored if supervision intensity was related to more advanced technologies generically. However, in the advanced economy dataset (shown in Table 2) the value added of knowledge intensive sectors as a share of gross value added was substantially uncorrelated with the supervisory ratio (<em>r</em> = 0.14).</p>
<p>While the data are inadequate to provide a compelling test of the hypothesis, we thus find little evidence that the increase in guard labor in the U.S. or the differences across the countries is due to differences in output composition and technology. A more likely explanation is what we term ‚Äòenforcement specialization‚Äô. Economic development proceeds through a process of specialization and increasing division of labor; the work of perpetuating a society‚Äôs institutions is no exception to this truism‚Ä¶.Our data indicate that the United States devotes well over twice as large a fraction of its labor force to guard labor as does Switzerland. This may occur in part because peer monitoring and informal sanctioning play a larger role in Switzerland, as well as the fact that ordinary Swiss citizens have military defense capacities and duties and are not counted in our data as soldiers.</p>
</blockquote>
<a href="#fnref29">‚Ü©</a></li>
<li id="fn30"><p>A key advantage of the <a href="http://en.wikipedia.org/wiki/Byzantine%20Empire" title="Wikipedia: Byzantine Empire">Byzantine Empire</a>, according to <a href="http://en.wikipedia.org/wiki/Edward%20Luttwak" title="Wikipedia: Edward Luttwak">Edward Luttwak</a>, was that it had an efficient tax system which enabled it to support a standing military, which was able to be trained in horse-archery all the way up to steppe-nomad standards - a task which took years for the trainees who could manage it at all. (In contrast, the US military is happy to send many soldiers into combat with only a few months of training.)<a href="#fnref30">‚Ü©</a></p></li>
<li id="fn31"><p>If you think that‚Äôs the <em>whole</em> military-industrial-intelligence budget, you are quite naive.<a href="#fnref31">‚Ü©</a></p></li>
<li id="fn32"><p>Witness the massive fights over the <a href="http://en.wikipedia.org/wiki/Base%20Realignment%20and%20Closure" title="Wikipedia: Base Realignment and Closure">Base Realignment and Closure</a> and unusual measures required; the Congressmen aren‚Äôt stupid, they understand how valuable the military-industrial welfare is for their communities.<a href="#fnref32">‚Ü©</a></p></li>
<li id="fn33"><p>Imprisonment as a <em>permanent</em> punishment was <a href="http://en.wikipedia.org/wiki/Prison%23History" title="Wikipedia: Prison#History">used rarely</a> prior to the Industrial Revolution, and what prisons there were often were primarily a mine or other facility of that kind; it is very expensive to imprison and only imprison someone, which is why techniques like fines (eg. Northern Europe), torture (China), exile (Greece) or <a href="http://en.wikipedia.org/wiki/penal%20transportation" title="Wikipedia: penal transportation">penal transportation</a> (England &amp; Australia), or execution (everyone) were the usual methods.<a href="#fnref33">‚Ü©</a></p></li>
<li id="fn34"><p>For example, MIT economist ‚Äú<a href="http://en.wikipedia.org/wiki/Daron%20Acemoglu" title="Wikipedia: Daron Acemoglu">Daron Acemoglu</a> <a href="https://web.archive.org/web/20130121075633/http://thebrowser.com/interviews/daron-acemoglu-on-inequality?page=full">on Inequality</a>‚Äù has all the pieces but somehow escape the obvious conclusion:</p>
<blockquote>
<p><em>Let‚Äôs go through your books. Your first choice is <strong>The Race between Education and Technology</strong>, published by Harvard University Press. You mentioned in an earlier email to me that it is ‚Äúa must-read for anyone interested in inequality‚Äù. Tell me more.</em></p>
<p>This is a really wonderful book. It gives a masterful outline of the standard economic model, where earnings are proportional to contribution, or to productivity. It highlights in a very clear manner what determines the productivities of different individuals and different groups. It takes its cue from a phrase that the famous Dutch economist, Jan Tinbergen coined. The key idea is that technological changes often increase the demand for more skilled workers, so in order to keep inequality in check you need to have a steady increase in the supply of skilled workers in the economy. He called this ‚Äúthe race between education and technology‚Äù. If the race is won by technology, inequality tends to increase, if the race is won by education, inequality tends to decrease.</p>
<p><em>The authors, Claudia Goldin and Larry Katz, show that this is actually a pretty good model in terms of explaining the last 100 years or so of US history. They give an excellent historical account of how the US education system was formed and why it was very progressive, leading to a very large increase in the supply of educated workers, in the first half of the century. This created greater equality in the US than in many other parts of the world.</em></p>
<p>They also point to three things that have changed that picture over the last 30 to 40 years. One is that technology has become even more biased towards more skilled, higher earning workers than before. So, all else being equal, that will tend to increase inequality. Secondly, we‚Äôve been going through a phase of globalisation. Things such as trading with China - where low-skill labour is much cheaper - are putting pressure on low wages. Third, and possibly most important, is that the US education system has been failing terribly at some level. We haven‚Äôt been able to increase the share of our youth that completes college or high school. It‚Äôs really remarkable, and most people wouldn‚Äôt actually guess this, but in the US, the cohorts that had the highest high-school graduation rates were the ones that were graduating in the middle of the 1960s. Our high-school graduation rate has actually been declining since then. If you look at college, it‚Äôs the same thing. This is hugely important, and it‚Äôs really quite shocking. It has a major effect on inequality, because it is making skills much more scarce then they should be.</p>
<p><em>Do Goldin and Katz go into the reasons why education is failing in the US?</em></p>
<p>They do discuss it, but nobody knows. It‚Äôs not a monocausal, simple story. It‚Äôs not that we‚Äôre spending less. In fact, we are spending more. It‚Äôs certainly not that college is not valued, it‚Äôs valued a lot. The college premium - what college graduates earn relative to high-school graduates - has been increasing rapidly. It‚Äôs not that the US is not investing enough in low-income schools. There has been a lot of investment in low-income schools. Not just free lunches, but lots of grants and other forms of spending from both states and the federal government.</p>
</blockquote>
<p>The failure of education to increase may be <a href="http://econlog.econlib.org/archives/2012/03/the_myth_of_the_7.html" title="'The Myth of the Education Plateau', by Bryan Caplan">masked by the dying of the uneducated elderly</a>, but that is an effect that can only last so long. And then we will see something like that looks <a href="http://econlog.econlib.org/archives/2012/03/goldin-katz_and.html" title="'Goldin-Katz and the Education Plateau', by Bryan Caplan">more like this</a>, a log graph which may begin petering out soon (and which looks like a diminishing returns graph - every time unit sees less and less increase squeezed out as additional efforts or larger returns are applied to the populace):</p>
<figure>
<img alt="Log-Relative Supply of College/non-College Labor, 1963-2008" height="308" src="./images/2008-katz-relativefractionuscollegenocollege.jpg" title="http://econlog.econlib.org/katz.jpg by Lawrence F. Katz" width="425"/><figcaption>Log-Relative Supply of College/non-College Labor, 1963-2008</figcaption>
</figure>
<p>Or economist <a href="http://en.wikipedia.org/wiki/Alex%20Tabarrok" title="Wikipedia: Alex Tabarrok">Alex Tabarrok</a>, in a <a href="http://www.econtalk.org/archives/2011/12/tabarrok_on_inn.html">podcast</a>, who identifies the problem and blames it on a decrease in teacher quality!</p>
<blockquote>
<p>You argue that the American education system, both K-12 and at the college levels, has got some serious problems. Let‚Äôs talk about it. What‚Äôs wrong with it? And of course, as a result, education is a key part of innovation and productivity. If you don‚Äôt have a well-educated populace you are not going to have a very good economy. What‚Äôs wrong with our education system?</p>
<p>Let‚Äôs talk about K-12. Here‚Äôs two remarkable facts, which have just blown me away. Right now, in the United States, people 55-64 years old, they are more likely to have had a high school education than 25-34 year olds. Just a little bit, but they are more likely. So, you look everywhere in the world and what do you see? You see younger people having more education than older people. Not true in the United States. That is a shocking claim. Incredible. And the reason is that the drop-out rate has increased? Exactly. So, the high school dropout rate has increased. Now, 25% of males in the United States drop out of high school. And that‚Äôs increased since the 1960s, even as the prospects for a high school dropout have gotten much worse. We‚Äôve seen an increase, 21st century‚Äì25% of males not graduating high school. That‚Äôs mind-boggling. Why? One of the underlying facts relating to education, which is [?], which is that the more education you get on average‚Äìand I‚Äôm going to talk about why on average can be very misleading‚Äìhigh school graduates do better than high school dropouts; people with some college do better than high school graduates; people graduating from college do better than people with some college; people with graduate degrees do better than college grads. And the differences are large. Particularly if you compare a college graduate to a high school dropout, there is an enormous difference.</p>
<p>So, normally we would say: Well, this problem kind of solves itself. There‚Äôs a natural incentive to stay in school, and I wouldn‚Äôt worry about it. Why should we be worrying about it? It doesn‚Äôt seem to be working. Why isn‚Äôt it working and what could be done? I think there‚Äôs a few problems. One is the quality of teachers I think has actually gone down. So I think that‚Äôs a problem. This is a case of every silver lining has a cloud, or something like that, in that in 1970s about half of college-educated women became teachers. This is at a time when there‚Äôs maybe 4% are getting an MBA, less than 10% are going to medical school, going to law school. These smart women, they are becoming teachers. Well, as we‚Äôve opened up, by 1980 you‚Äôve got 30% or so of the incoming class of MBAs, doctors, lawyers, are women. Which is great. Their comparative advantage, moving into these fields, productivity, and so forth. And yet that is meant that on average, the quality of teachers, the quality pool we are drawing from, has gone down in terms of their SAT levels and so forth. So, I think we need to fix that.</p>
</blockquote>
<p>Also relevant: <a href="http://www.insidehighered.com/news/2014/01/22/see-how-liberal-arts-grads-really-fare-report-examines-long-term-data">‚ÄúLiberal Arts Grads Win Long-Term‚Äù</a>.<a href="#fnref34">‚Ü©</a></p></li>
<li id="fn35"><p><a href="http://ideas.repec.org/p/iza/izadps/dp2442.html">‚ÄúOver-Education and the Skills of UK Graduates‚Äù</a> (Chevalier &amp; Lindley 2006):</p>
<blockquote>
<p>Before the Eighties, Britain had one of the lowest participation rates in higher education across OECD countries. Consequently, increasing participation in higher education became the mantra of British governments. The proportion of school leavers reaching higher education began to slowly increase during the early Eighties, until it suddenly increased rapidly towards the end of the decade. As illustrated in Figure 1, the proportion of a cohort participating in higher education doubled over a five year period, from 15% in 1988 to 30% by 1992‚Ä¶we analyse the early labour market experience of the 1995 cohort, since these people graduated at the peak of the higher education expansion period. We find a reduction in the proportion of matched graduates, compared to the 1990 cohort. This suggests that the labour market could not fully accommodate the increased inflow of new graduates, although this did not lead to an increased wage penalty associated with over-education. Hence, the post-expansion cohort had the appropriate skills to succeed in the labour market. Secondly, we are the first to investigate whether the over-education wage penalty remains even after controlling for observable graduate skills, skill mismatch, as well as unobservable characteristics. We find some evidence that genuinely over-educated individuals lack ‚Äògraduate skills‚Äô; mostly management and leadership skills. Additionally, the longitudinal element of the dataset is used to create a measure of time-invariant labour market unobservable characteristics which are also found to be an important determinant of the probability to be over-educated. Over-education impacts negatively on the wages of graduates, over and above skill levels (observed or not) which suggests that the penalty cannot be solely explained by a lack of skills but also reflects some job idiosyncratic characteristics. It also increases unemployment by up to three months but does not lead to an increase in job search, as the numbers of job held since graduation is not affected by the current over-education status.</p>
<p>‚Ä¶Most of the UK literature has relied on self-assessment of over-education, and typically finds that 30% of graduates are overeducated<sup>4</sup>. Battu et al. (2000) provide one of the most comprehensive studies of over-education. The average proportion of over-educated individuals across the 36 estimates of their analysis was around one-quarter, with estimates ranging between one-fourteenth and as high as two-thirds. For the UK, Battu et al (2000) concluded that over-education has not increased in the early Nineties.</p>
<p>This result is supported by Groot and Maassen van den Brink (2000) whose meta-analysis of 25 studies found no tendency for a world-wide increase in the incidence of over-education despite the general improvement in the level of education, although they do suggest it has become increasingly concentrated among lower ability workers, suggesting the over-education is not solely due to mismatch of workers and jobs. Freeman‚Äôs pioneering work on over-education (1976) suggests that over-education is a temporary phenomenon due to friction in the labour market, although UK evidence is contrary to this assumption. Dolton and Vignoles (2000) found that 38% of 1980 UK graduates were over-educated in their first job and that 30% remained in that state six years later. Over a longer period there is also evidence that over-education is a permanent feature of some graduates‚Äô career (Dolton and Silles, 2003). For graduates the wage penalty associated with over-education ranges between 11 and 30%, however, contrary to Freeman‚Äôs view over-education has not led to a decrease in the UK return to education in general (Machin, 1999 and Dearden et al., 2002) even if recent evidence by Walker and Zhu (2005) report lower returns for the most recent cohort of graduates.</p>
<p>The general consensus is that after controlling for differences in socio-economic and institutional factors, over-education is a consequence of unobservable elements such as heterogeneous ability and skills. There is evidence to support this from studies by B√ºchel and Pollmann-Schult (2001), Bauer (2002), Chevalier (2003) and Frenette (2004). Most over-educated workers are efficiently matched into appropriate jobs and after accounting for the unobserved heterogeneity, the wage penalty for over-education is reduced. However, a remaining group of workers appear over-skilled for their jobs and suffer from substantial wage penalties.</p>
</blockquote>
<a href="#fnref35">‚Ü©</a></li>
<li id="fn36"><p>The Bureau of Labor Statistics revised its data-collection method for unemployment duration in January 2011. Based on the previous method, the average unemployment duration would be about 37 weeks rather than 40 weeks. For more information, see <a href="http://www.bls.gov/cps/duration.htm">‚ÄúChanges to data collected on unemployment duration‚Äù</a>.<a href="#fnref36">‚Ü©</a></p></li>
<li id="fn37"><p><a href="http://www.brookings.edu/opinions/2011/0909_jobs_winship.aspx">‚ÄúA Decade of Slack Labor Markets‚Äù</a>, Scott Winship, <a href="http://en.wikipedia.org/wiki/Brookings%20Institution" title="Wikipedia: Brookings Institution">Brookings Institution</a> Fellow; other good quotes:</p>
<blockquote>
<p>From 1951 through 2007, there were never more than three unemployed workers for each job opening, and it was rare for that figure even to hit two-to-one. In contrast, there have been more than three jobseekers per opening in every single month since September 2008. The ratio peaked somewhere between five-to-one and seven-to-one in mid-2009. It has since declined but we have far to go before we return to ‚Äúnormal‚Äù levels.</p>
<p>The bleak outlook for jobseekers has three immediate sources. The sharp deterioration beginning in early 2007 is the most dramatic feature of the above chart (the rise in job scarcity after point C in the chart, the steepness of which depends on the data source used). But two less obvious factors predated the recession. The first is the steepness of the rise in job scarcity during the <em>previous</em> recession in 2001 (from point A to point B), which rivaled that during the deep downturn of the early 1980s. The second is the failure between 2003 and 2007 of jobs per jobseeker to recover from the 2001 recession (the failure of point C to fall back to point A).</p>
</blockquote>
<a href="#fnref37">‚Ü©</a></li>
</ol>
</section>
</div>
</div>
<div id="footer">
<p>Still bored? Then try my <a href="https://plus.google.com/103530621949492999968/posts" title="Google+ posts">Google+ news feed</a>.</p>
<a href="https://docs.google.com/spreadsheet/viewform?formkey=dE5GLWpfX3RhX1c2Q1phcEo3U3VDVEE6MQ">Send anonymous feedback</a>
<br/>
<div id="license">
<p xmlns:dct="http://purl.org/dc/terms/" xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#">
<a rel="license" href="http://creativecommons.org/publicdomain/zero/1.0/">
<img src="http://i.creativecommons.org/p/zero/1.0/88x31.png" style="border-style: none;" alt="CC0" height="31" width="88"/>
</a>
</p>
</div>
</div>
 
<script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
 
<script type="text/javascript" src="./static/js/footnotes.js"></script>
 
<script type="text/javascript" src="./static/js/abalytics.js"></script>
<script type="text/javascript">
      window.onload = function() {
      ABalytics.applyHtml();
      };
    </script>
 
<script id="googleAnalytics" type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-18912926-1']);

      ABalytics.init({
      indent: [
      {
      name: "none",
      "indent_class1": "<style>p + p { text-indent: 0.0em; margin-top: 0 }</style>"
      },
      {
      name: "indent0.1",
      "indent_class1": "<style>p + p { text-indent: 0.1em; margin-top: 0 }</style>"
      },
      {
      name: "indent0.5",
      "indent_class1": "<style>p + p { text-indent: 0.5em; margin-top: 0 }</style>"
      },
      {
      name: "indent1.0",
      "indent_class1": "<style>p + p { text-indent: 1.0em; margin-top: 0 }</style>"
      },
      {
      name: "indent1.5",
      "indent_class1": "<style>p + p { text-indent: 1.5em; margin-top: 0 }</style>"
      },
      {
      name: "indent2.0",
      "indent_class1": "<style>p + p { text-indent: 2.0em; margin-top: 0 }</style>"
      }
      ],
      }, _gaq);

      _gaq.push(['_trackPageview']);
      (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
 
<script id="outboundLinkTracking" type="text/javascript">
      $(function() {
      $("a").on('click',function(e){
      var url = $(this).attr("href");
      if (e.currentTarget.host != window.location.host) {
      _gat._getTrackerByName()._trackEvent("Outbound Links", e.currentTarget.host.replace(':80',''), url, 0);
      if (e.metaKey || e.ctrlKey || (e.button == 1)) {
      var newtab = true;
      }
      if (!newtab) {
      e.preventDefault();
      setTimeout('document.location = "' + url + '"', 100);
      }
      }
      });
      });
    </script>
 
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
 
<script type="text/javascript" src="./static/js/footnotes.js"></script>
 
<script type="text/javascript" src="./static/js/tablesorter.js"></script>
<script type="text/javascript" id="tablesorter">
      $(document).ready(function() {
      $("table").tablesorter();
      }); </script>
 
<div id="disqus_thread"></div>
<script type="text/javascript">
      if (document.title != 'Essays') { <!-- avoid Disqus comments on front page -->
      (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://disqus.com/forums/gwern/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
      var disqus_shortname = 'gwern';
      (function () {
      var s = document.createElement('script'); s.async = true;
      s.src = 'http://disqus.com/forums/gwern/count.js';
      (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
      }());
      }</script>
<noscript><p>Enable JavaScript for Disqus comments</p></noscript>
</body>
</html>

