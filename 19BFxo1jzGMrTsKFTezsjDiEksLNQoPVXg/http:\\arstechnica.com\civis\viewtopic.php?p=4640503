http://arstechnica.com/civis/viewtopic.php?p=4640503
Why do I think and computers? - Ars Technica OpenForum
Welcome to the Ars OpenForum.
Register
Login
Posting Guidelines | Contact Moderators
Ars Technica > Forums > Ars Lykaion > The Observatory
Jump to:
Select a forum
------------------
Hardware & Tweaking
Audio/Visual Club
Case and Cooling Fetish
CPU & Motherboard Technologia
Mobile Computing Outpost
Networking Matrix
Other Hardware
Agora Classifieds
Operating Systems & Software
Battlefront
Microsoft OS & Software Colloquium
Linux Kung Fu
Windows Technical Mojo
Distributed Computing Arcana
Macintoshian Achaia
Programmer's Symposium
The Server Room
Ars Lykaion
Gaming, Extra Strength Caplets
The Lounge
The Soap Box
The Boardroom
The Observatory
Ars Help & Feedback
Ars Subscription Member Areas
Image Galleries
Why do I think and computers?
201 posts • 123 ... 6 Next
flyingember
Ars Legatus Legionis
Tribus: Kansas City
Registered: Jan 15, 2001Posts: 19330
Posted: Wed Sep 19, 2007 7:46 pm
I love thinking about the brain and how it relates to computing. Immersing myself in Searle vs Lycan is the only reason I passed my philosophy class.I have to wonder where the science of AI is taking us? What parts of out lives have been effected by this area of science and how much is just good programming? Are there any good examples of work done in AI put into a common product that everyone knows about?
.劉煒
""A New World Man""
Ars Legatus Legionis
et Subscriptor
Tribus: Here, Now
Registered: Nov 10, 1999Posts: 38635
Posted: Wed Sep 19, 2007 8:08 pm
'REAL' AI? Or hierarchical trees and decision making processes?
Hellburner
Ars Legatus Legionis
Registered: Feb 22, 1999Posts: 10623
Posted: Wed Sep 19, 2007 8:15 pm
What do you call AI?Sheer computational horsepower leads to Intelligence Amplification orders of magnitude beyond that of human v1.0.Almost any complex product has heavily utilized computers in the design process.
EpitomeOfAGeek
"Cipher Simian"
Ars Tribunus Militum
et Subscriptor
Tribus: Mesa, AZ
Registered: Aug 19, 2003Posts: 2113
Posted: Wed Sep 19, 2007 8:16 pm
I remember reading, and I think this was actually a short piece of fiction, but they posited that true AI won't come into existence until we've built fully viable quantum computers. Expert systems, on the other hand, can make your life a lot easier in many fields. They're essentially incredibly complex decision trees and logic statements but when restricted to one or two areas of expertise they can be very effective.One area of AI research I've been interested in is the concept of emergent behavior. Complex interactions deriving from a small subset of simpler behaviors.
andyfatbastard
"lol donkaments"
Ars Legatus Legionis
et Subscriptor
Tribus: 19BFxo1jzGMrTsKFTezsjDiEksLNQoPVXg
Registered: Oct 17, 2000Posts: 19944
Posted: Wed Sep 19, 2007 8:18 pm
.milFox: I think he's referring to modern neural networks?If so, they have some excellent uses, but we're nowhere near creating anything big or fast enough to become anything close to an AI at the moment. Quantum computing may change that, but as you'll read from Noble Intent, we're very far behind on that one too.Re: Why do you think.You think because you understand and speak language. Language provides a framework for rational and logical thought, and allows for significantly more complex thought processes beyond basic instincts and social interaction.
omniron
Ars Praefectus
Registered: Aug 11, 2002Posts: 4333
Posted: Wed Sep 19, 2007 8:31 pm
quote:Originally posted by clichekiller:One area of AI research I've been interested in is the concept of emergent behavior. Complex interactions deriving from a small subset of simpler behaviors. For AI, as most people think about it, an emergent system is the only viable environment.
EpitomeOfAGeek
"Cipher Simian"
Ars Tribunus Militum
et Subscriptor
Tribus: Mesa, AZ
Registered: Aug 19, 2003Posts: 2113
Posted: Wed Sep 19, 2007 8:33 pm
quote:Originally posted by omniron:quote:Originally posted by clichekiller:One area of AI research I've been interested in is the concept of emergent behavior. Complex interactions deriving from a small subset of simpler behaviors. For AI, as most people think about it, an emergent system is the only viable environment. Well in a lot of ways we're really one big emergent system ourselves.
onkeljonas
Ars Tribunus Angusticlavius
Registered: Aug 31, 2003Posts: 6078
Posted: Wed Sep 19, 2007 8:38 pm
quote:Originally posted by Hellburner:What do you call AI?Sheer computational horsepower leads to Intelligence Amplification orders of magnitude beyond that of human v1.0.Almost any complex product has heavily utilized computers in the design process. I think there is an important difference between what we have now – systems amplifying as you say our own intelligence ("merely" tools), and actual artificial intelligence – something independent of us – something more than a tool.Personally I doubt we will ever create something like that, but I don't consider it impossible per se – if intelligent life didn't need a "hand of god" to evolve, I see no reason why the process couldn't be replicated.What makes me most pessimistic about our ability to do it, is the fact that it took nature longer than the human race has existed. A million years is quite a long time for us...
bluezen
Wise, Aged Ars Veteran
Registered: Apr 28, 2002Posts: 158
Posted: Wed Sep 19, 2007 9:45 pm
As onkeljonas mentioned, computers are tools. They are merely an extension of ourselves. Fundamentally, they handle all of the heavy computation that we'd rather not do by hand. It's like how a high-level programmer that lets the computer, the tool, handle all of the low-level tasks. We have barely even begun in simulating a model that is as complex as the human brain. And we're not even close. Many insist that the true, end-all be-all definiton of AI is complex, independent thought. So far, we're only able to program mimics of complex thought. In the end, there is a programmer, and the system's output is entirely dependent on its input. In my opinion, true AI would also have to include the good with the bad; independent, rational and irrational computation, just like us droids I mean humans. Until then, we're still just programming systems that happen to be pretty smart. Quantum physics seems to be future for AI, as the utter complexity is already present in what scientists have and haven't found in that field. But who knows, someone might figure it out, hopefully before our race is done for.As noted in every sci-fi book ever published that touched on AI, this seems dangerous. A programmer in the future may have his hands in the development of a buggy, rational/irrational system, and if he gives it more than just autonomy and the ability to process more than just input->output, he will be a God, or creator of "life", or what-have-you, with a funky, buggy, independent system on his hands.We're pretty fucked up people, as a whole; maybe we don't need to create fucked up devices. I'd rather not allow something we've created to overreact, overcompute, or not do anything at all. Spouses seem to do this enough as it is.Johnny 5 is alive!
kcisobderf
Ars Legatus Legionis
et Subscriptor
Tribus: Southgate, MI, USA
Registered: Jun 2, 2000Posts: 11488
Posted: Wed Sep 19, 2007 10:03 pm
When considered in the whole, the Chinese Room does understand chinese. Your brain works the same way.Think about how you go through your day, and the people you interact with. It's surprising how little cognition is required for many everyday acts.
bluezen
Wise, Aged Ars Veteran
Registered: Apr 28, 2002Posts: 158
Posted: Wed Sep 19, 2007 10:19 pm
But cognition is a state in which one learns how to think and interact at a low level. It creates patterns of thought in our brain based on previous results from internal and external input and/or dictated by pre-programmed or externally programmed rules. We're able to teach computational systems to do this, and they're great at it. But, once a computer "decides" to change his program, just like a human could (with therapy, drugs, friends, input), that's where the real distinction between AI and cognition begins.
ringobob
Ars Praefectus
Registered: Dec 8, 1999Posts: 3984
Posted: Thu Sep 20, 2007 8:49 am
I find it interesting that people doubt the possibility of "true" AI (whether absolutely or within a reasonable time frame). It seems so simple to me, that I have to believe that it will be accomplished in my lifetime.Granted, my knowledge on the subject is far from expert. But I've worked with ANNs, I've read papers, the more complicated the research gets the more it feels to me like they're missing the forest for the trees.
norton_I
Ars Praefectus
Registered: Apr 30, 2001Posts: 3397
Posted: Thu Sep 20, 2007 9:56 am
quote:Originally posted by clichekiller:I remember reading, and I think this was actually a short piece of fiction, but they posited that true AI won't come into existence until we've built fully viable quantum computers. So, I can say that most people interested in quantum computers consider this pretty much bunk. Not that it is a bad idea for a short story, just there isn't currently any good reason to think the two are related.The idea that "brains a fundamentally quantum" enjoyed a fair amount of popularity back in the day, primarily because we had relatively little understanding of either quantum mechanics or the biology of the brain, and it is easy to equate two poorly understood things. That isn't really the case any more, and most scientists I know think there is no reason to believe anything quantum mechanical is relevant at the "interesting" scales of consciousness. Obviously, quantum mechanics is king at the molecular level, in much the same way that it describes the behavior of electrons in a semiconductor, but in both cases the quantum properties are washed out very quickly.One might argue that a quantum computer with its promised dramatically faster computation might help, but despite a lot of effort, there really aren't that many algorithms a hypothetical quantum computer can actually do faster than a classical computer.
Yoweigh
Ars Scholae Palatinae
Tribus: New Orleans, LA
Registered: Jan 6, 2002Posts: 722
Posted: Thu Sep 20, 2007 10:49 am
I think that someday, given enough time, robust artificial intelligences could be developed. (but that doesn't necessarily mean that they will) I do not think it's possible to provide any kind of reliable estimate of when that might happen.What I think will be interesting to see is where the inspiration comes from (if it ever does) and what other unforeseen developments it could lead to. The two leading inspiration candidates look like biology and math, and they seem pretty interrelated to me. Will AI be born through a thorough biological understanding of human consciousness, or will that understanding come after the development of the mathematical basis of AI?
river-wind
Ars Praefectus
et Subscriptor
Registered: Nov 20, 2001Posts: 5547
Posted: Thu Sep 20, 2007 11:01 am
I've been under the impression for a while now that the emergent complexity that appears to be needed for the development of consciousness cannot occur in computers until we merge CPUs and RAM.Thousands of simple, in order CPUs all networked together, and able to detect the state of their neighbors directly. The state of the CPU would work as the binary bit in terms of memory, so each CPU would also act as one or more memory bits.Lately, I've been wondering if such a system could be mimicked in software, but I'm uncertain. The isolationist design of most CPUs is a key hinderance here, IMO.(warning: use of anthropomorphic phrasing follows. Please read quoted terms as analogies)A modern CPU is "aware" of what is actively being worked on (held in its capacitors), and only "knows" of the existence of data in their on-die memory. All other data is a black box to them, until they "forget" something and replace it with a new piece of data from RAM/HD. The CPU has access to the map which ties everything into a cohesive whole (File System, file format defs, etc), but can only look at that map when it isn't actively looking at the data itself.Either they 'know' where a thing needs to go but are blind to the actual thing itself, or they can "see" the thing but in isolation from its context.edit: There is an article in the latest Dr. Dobbs (Issue 401, Oct '07 "AI: It's OK Again!") that covers some history and bit of the current state of things in the AI area. I seems to be largely touting the beneficial effects of the application of AI research into everyday products and computer systems, but doesn't go much into the current fields of study. It almost reads as if the authors is not telling us why AI is ok, but trying to get us to say it along with him as an affirmation.
StarKruzr
"et Ars Scholae Palatinae"
Ars Tribunus Angusticlavius
Tribus: Taxation without Representation
Registered: Aug 9, 1999Posts: 6392
Posted: Thu Sep 20, 2007 1:58 pm
"Why do I think and computers?" Emkorial
Ars Legatus Legionis
Registered: May 11, 2000Posts: 33592
Posted: Thu Sep 20, 2007 3:04 pm
Why do people think consciousness is just an emergent property of raw computational speed and power?
Hawkear
Ars Tribunus Angusticlavius
et Subscriptor
Tribus: OC
Registered: Aug 11, 2000Posts: 7042
Posted: Thu Sep 20, 2007 3:36 pm
quote:Originally posted by Emkorial:Why do people think consciousness is just an emergent property of raw computational speed and power? It's also a matter of information collection and filtering.What do you think it is?
ringobob
Ars Praefectus
Registered: Dec 8, 1999Posts: 3984
Posted: Thu Sep 20, 2007 3:40 pm
quote:I've been under the impression for a while now that the emergent complexity that appears to be needed for the development of consciousness cannot occur in computers until we merge CPUs and RAM. I don't see why the hardware matters that much at all, unless it's simply a matter of speed to simulate the system in real time. quote: Lately, I've been wondering if such a system could be mimicked in software, but I'm uncertain. The isolationist design of most CPUs is a key hinderance here, IMO. I don't see why not, though again speed may be an issue to accomplish it in real time, it shouldn't be a barrier to accomplishing it at all.The only true hardware difference that I would see as an honest to goodness barrier would be if it did require quantum computers, but norton_I covered that.
QuantumET
Ars Scholae Palatinae
Registered: Jun 27, 2000Posts: 1184
Posted: Thu Sep 20, 2007 5:10 pm
Well, as I understand the philosophical side of the AI field, there really are quite a few influential people who believe a Von Neumann machine cannot produce consciousness. And no, they don't make any claim about quantum mechanics being the issue. My philosophy being somewhat weak, I have trouble understanding their arguments, which often seem to be Chinese Room related.Personally - I don't see how this work. Presuming that the function of a neuron can be described well by strictly classical processes (And that seems to be the current belief), then a computer can simulate those processes at some rate.And if it can simulate one, there's no reason it couldn't simulate many (besides engineering concerns, of course, which are huge), and eventually simulate a whole brain's worth.At which point the computer should be simulating a consciousness.The only way I see out of the above is to argue that a Von Neumann computer can never simulate a neuron. And I don't see any reason why it couldn't.
ringobob
Ars Praefectus
Registered: Dec 8, 1999Posts: 3984
Posted: Thu Sep 20, 2007 7:50 pm
Honestly I don't care if a Von Neumann machine can "produce consciousness", as the idea of consciousness isn't clearly defined when it comes to an artificial entity.But it should certainly be possible to simulate consciousness so closely that it doesn't make a practical difference. quote: Personally - I don't see how this work. Presuming that the function of a neuron can be described well by strictly classical processes (And that seems to be the current belief), then a computer can simulate those processes at some rate.And if it can simulate one, there's no reason it couldn't simulate many (besides engineering concerns, of course, which are huge), and eventually simulate a whole brain's worth.At which point the computer should be simulating a consciousness.The only way I see out of the above is to argue that a Von Neumann computer can never simulate a neuron. And I don't see any reason why it couldn't. I agree with all of this, I just think it's too low level. People are too focused on simulating the low level processes of the brain and getting those to perform the high level processes, and not focusing enough on the idea that the human brain is, primarily and at a high level, a huge context processing engine. And, as such, a computer should be able to mimic it.
Citrus538
Ars Legatus Legionis
Registered: Jul 21, 2000Posts: 12726
Posted: Thu Sep 20, 2007 8:23 pm
quote:Originally posted by Hawkear:quote:Originally posted by Emkorial:Why do people think consciousness is just an emergent property of raw computational speed and power? It's also a matter of information collection and filtering.What do you think it is? Hey now, none of that philosophy stuff! This is the science forum!But seriously, I think the biggest problem people have with AI being "conscious" is the hard problem of consciousness. Sure we know some stuff about the brain and cognition, but how do all those neurotransmitters floating around in our skull produce our actual conscious experience?
MooseAreFun
"I'm not Canadian. (No-one is)."
Ars Legatus Legionis
et Subscriptor
Registered: Feb 10, 2000Posts: 10820
Posted: Sat Sep 22, 2007 5:21 pm
quote:When considered in the whole, the Chinese Room does understand Chinese. The Chinese Room contains both a rule book that simplifies understanding Chinese down to basic rules, and a sentient being interpreting those rules.When you combine two understanding systems in one room, it's not surprising they can demonstrate understanding.As I see it, it either reduces to:A rule book that can be followed mechanically and a Turing machine to execute the rules - this is saying "a rule book which understands Chinese understands Chinese." (and the creation of such a rule book is left as an exercise for the reader).orA rule book that requires conciousness to follow properly and a person to interpret the rules - this is saying "conciousness plus something can demonstrate conciousness".
ringobob
Ars Praefectus
Registered: Dec 8, 1999Posts: 3984
Posted: Sat Sep 22, 2007 7:14 pm
I wasn't aware of the Chinese Room (as a formal concept, at least; the basic idea is pretty apparent when thinking about AI) until reading this thread, but I more or less agree.
QtDevSvr
Ars Tribunus Angusticlavius
et Subscriptor
Tribus: California Floristic Province, Madrean Region, Holarctic Kingdom
Registered: Apr 28, 2001Posts: 8942
Posted: Sun Sep 23, 2007 12:52 am
quote: Originally posted by MooseAreFun:The Chinese Room contains both a rule book that simplifies understanding Chinese down to basic rules, and a sentient being interpreting those rules. It's exactly wrong to say that the person in the Chinese Room "interprets" the rules. The person follows the rules, in exaclty the same sense that a computer chip follows a program: mechanically. The whole force of the Chinese Room argument flows from the stipulation that the "operator" does no interpretation whatsoever. He just matches shapes with other shapes, according to some instructions. The whole process is necessarily purely formal, purely syntactic.quote:Originally posted by QuantumET:Well, as I understand the philosophical side of the AI field, there really are quite a few influential people who believe a Von Neumann machine cannot produce consciousness. And no, they don't make any claim about quantum mechanics being the issue. My philosophy being somewhat weak, I have trouble understanding their arguments, which often seem to be Chinese Room related.Personally - I don't see how this work. Presuming that the function of a neuron can be described well by strictly classical processes (And that seems to be the current belief), then a computer can simulate those processes at some rate. The Chinese Room argument is essentially the claim that purely syntactic processes do not amount to semantic states. The argument gets what force it has by making the greatest semantic engine we know, the human mind, a constitutive part of the syntactic operations which directly result in passage of the Turing test, while simultaneously yielding no semantic grasp whatsoever—no understanding—on the part of the mind in the machine.There are problems with that portrayal and the conclusion it leads to, but that's not what concerns me most. The problem I see for your analysis is that there is some presumably unintentional but nonetheless fatal equivocation. The claim within the Chinese Room argument is the claim that purely syntactic processes do not, as such, produce understanding. Alas, you go on to talk not about a computer causing states of understanding, but rather of it "simulating brain processes". That's an equivocation. What you need is an argument to the effect that the machinery of an AI which has consciousness produces understanding, not that it merely "simulates" it, whatever that might mean.Mind you I don't take that to be some sort of impossible burden. Neither, for that matter, does John Searle, the guy who invented the Chinese Room argument. In the original paper in which he presented the argument he basically acknowledged that a complete AI, which had motor and perceptual capabilities in addition to computational ones, could in principle do all the things that the "system" which is a fluent Chinese speaker can do and that, therefore, it could be said to understand Chinese. On his terms he still wins the argument though, because this more complete AI embraces a hell of a lot more than the mere mechanical manipulation of uninterpreted symbols which is a digital computer.This last point is really where most novices go wrong in critiquing the Chinese Room argument: they fail to understand that it was not directed at the possibility of an artificial mind, it was directed at the notion that computation as such was wholly constitutive of advanced mental states such as states of understanding. It was this now anachronistic idea, the idea that computation was the essence of the mental, which Searle was attacking. quote: Originally posted by Citrus538:But seriously, I think the biggest problem people have with AI being "conscious" is the hard problem of consciousness. Sure we know some stuff about the brain and cognition, but how do all those neurotransmitters floating around in our skull produce our actual conscious experience? Perhaps. I personally think that consciousness is somewhat over-rated, at least as a problem in the philosophy of mind. I mean, it seems all but obvious to me at this point that the "mystery" of consciousness per se is no mystery at all. The brain is massively parallel. Our "advanced" consciousness is to more "mundane" and merely sentient creatures as the man who can juggle five plates while playing the ukelele is to the baby who is only just learning to grasp. You get the various memory-prediction cortices going all at once and it puts on a pretty amazing show, but all that "going on at once" is not the impressive part.No sir. What mystifies me at this point in my own philosophy of mind is not consciousness, but sentience. I want to understand, at least in principle, how sensory cortices produce a representation of their sense domain. I don't have trouble understanding in principle the idea of something like a photo-sensitive set of cells which yield a set of stimulus-response behaviors to light. What I don't understand, even in principle, is how more complex neuro-anatomies yield a mental model. (The fact that the mental model, at least with advanced neuro-anatomies such as found in mammals, yields such tight sensory and predictive binding to reality is not a puzzle to me. It seems that standard evolutionary processes would all but necessarily lead to such tight bindings, given the genetic possibilities.) The part about mind that puzzles me is much closer to the beginning than it is to the latest evolutionary advance, human consciousness. I don't understand how to think about what is involved in that gap between brute stimulus-response and primitive perception.I feel like if I got this part of the puzzle, the rest of the philosophical puzzle about consciousness would fall together rather quickly.People have been working on vision for a long time, so maybe I'm just ignorant of useful theories in this area? David Marr?
ringobob
Ars Praefectus
Registered: Dec 8, 1999Posts: 3984
Posted: Sun Sep 23, 2007 9:13 am
quote:Mind you I don't take that to be some sort of impossible burden. Neither, for that matter, does John Searle, the guy who invented the Chinese Room argument. In the original paper in which he presented the argument he basically acknowledged that a complete AI, which had motor and perceptual capabilities in addition to computational ones, could in principle do all the things that the "system" which is a fluent Chinese speaker can do and that, therefore, it could be said to understand Chinese. On his terms he still wins the argument though, because this more complete AI embraces a hell of a lot more than the mere mechanical manipulation of uninterpreted symbols which is a digital computer.This last point is really where most novices go wrong in critiquing the Chinese Room argument: they fail to understand that it was not directed at the possibility of an artificial mind, it was directed at the notion that computation as such was wholly constitutive of advanced mental states such as states of understanding. It was this now anachronistic idea, the idea that computation was the essence of the mental, which Searle was attacking. Thanks for this explanation. Without the larger context, it's easy to get stuck in the ridiculousness of the basic premise that a strictly rules based processor can be an effective simulator. quote:No sir. What mystifies me at this point in my own philosophy of mind is not consciousness, but sentience. I want to understand, at least in principle, how sensory cortices produce a representation of their sense domain. OK, I'm going to fall back on my previously claimed non-expert status and ask for a clarification here. What you're talking about is an entity forming a subjective unit based on sensory input... basically a sense of distinctness from "other" (strictly based on sensory input, i.e. these things are what "I" sense)? Is this correct? quote: The part about mind that puzzles me is much closer to the beginning than it is to the latest evolutionary advance, human consciousness. I don't understand how to think about what is involved in that gap between brute stimulus-response and primitive perception. If I understand what you're getting at, I really think the answer is kind of what I was talking about earlier. Perception arises from context. Stimulus-response necessarily ignores context, and with its introduction into the system, more complex cognition necessarily arises.But perhaps I'm putting the cart before the horse.
QtDevSvr
Ars Tribunus Angusticlavius
et Subscriptor
Tribus: California Floristic Province, Madrean Region, Holarctic Kingdom
Registered: Apr 28, 2001Posts: 8942
Posted: Sun Sep 23, 2007 11:31 am
quote:Originally posted by ringobob:OK, I'm going to fall back on my previously claimed non-expert status and ask for a clarification here. What you're talking about is an entity forming a subjective unit based on sensory input... basically a sense of distinctness from "other" (strictly based on sensory input, i.e. these things are what "I" sense)? Is this correct? Not quite. Leave off the bit about "other" and it's very close. (I think that the concept of "other" is actually fairly advanced and not really needed for more primitive forms of sentience, so I'd prefer to leave it off of the immediate discussion.) Moving on:quote:quote: The part about mind that puzzles me is much closer to the beginning than it is to the latest evolutionary advance, human consciousness. I don't understand how to think about what is involved in that gap between brute stimulus-response and primitive perception. If I understand what you're getting at, I really think the answer is kind of what I was talking about earlier. Perception arises from context. Stimulus-response necessarily ignores context, and with its introduction into the system, more complex cognition necessarily arises. Hmmm. I think you have some things in mind when you mention "context", but I might need you to un-pack them. Perception is certainly context-sensitive (both logically and physically), and yes, there's a causal chain which begins with a creature's environmental context and ends with the creature's perception of same. Are these the sorts of facts what you have in mind when you say that "perception arises from context"?Getting back to your earlier remark: yes, what puzzles me is how to think about the very brute states of mind constituted by simple perceptions, of light, say, though we could equally talk about noise or maybe even touch. What I seek a clearer understanding of is the conceptual differences between a creature that is merely photo-sensitive, a plant, say, and one which, however crudely, perceives light. The latter creature (forgive my zoology: there must be some examples, I just don't know any off-hand), but not the plant, has a subjective state. I want to know how to think about the logical differences between those two conditions because, from my perspective, those differences are the biggest puzzle in the philosophical topic of consciousness.Again, I have no trouble imagining an evolutionary story which gets us from the crude, brute perception of light from dark to the wonders of the human eye. Given the tremendous selective advantage of an organ such as the modern eye, it seems all but inevitable to me that you'd end up with one, given time.Nor am I puzzled in any big way about philosophical issues surrounding the subjectivity/objectivity of perception. Consider this passage, for example, from the Singularity Institute for Artificial Intelligence:quote: Nobody actually lives in external reality, and we couldn't understand it if we did; too many quarks flying around. When we walk through a hall, watching the floor and walls and ceiling moving around us, we're actually walking through our visual cortex. That's what we see, after all. We don't see the photons reflected by the walls, and we certainly don't see the walls themselves; every single detail of our perception is there because a neuron is firing somewhere in the visual system. If the wrong neuron fired, we'd see a spot of color that wasn't there; if a neuron failed to fire, we wouldn't see a spot of color that was there. From this perspective, the actual photons are almost irrelevant. Furthermore, all the colors in the hall you're walking through are technically incorrect due to that old color-space thing. Heck, you might even walk past something purple.This is the point where the philosopher usually goes off the solipsistic deep end. "It's all arbitrary! Nothing is real! Everything is true! I can say whatever I want and nobody can do a thing about it, bwahaha!" I hate this whole line of thinking. If I ever start sounding like this, check my forehead for lobotomy scars.The Consensus usually has an extremely tight sensory, predictive, and manipulative binding to external reality. No, it doesn't work 100% of the time, but it works 99.99% of the time, so the rules are just as strict. Just because you can't see external reality directly doesn't mean it isn't there.Everything you see is illusion, the Veil of Maya. Where Eastern philosophy goes wrong is in assuming that the Veil of Maya is hiding something big and important. What lies behind the illusion of a brick is the actual brick. I have some major objections to aspects of this passage (veridical perceptions at least are real, and you do actually see the hall—to suppose otherwise is to misunderstand the logical nature of perception), but they are not relevant to present concerns. As a statement of the fact that conscious experience of external reality is created by nervous systems, in context, it's pretty good IMO.What I seek a better conceptual understanding of is not some grand philosophical puzzle about subjectivity/objectivity, nor how it is that at least some sense faculties have such stunning sensory, predictive, and manipulative binding to reality. What I seek is a clearer understanding of what's involved in the possession of a primitive sentience.Imagine a creature that is crudely photo-sensitive, but only in a stimulus-response sort of way. Let's suppose it's photo-averse: when it's crude photo-sensitive cells detect light of sufficient strength, the creatures squirms away.Now imagine the same crude photo-sensitivity—the mere ability to detect light from dark—but this time in a creature which sees the difference.What does such seeing consist in?
QuantumET
Ars Scholae Palatinae
Registered: Jun 27, 2000Posts: 1184
Posted: Sun Sep 23, 2007 12:33 pm
I was under the impression that there were people who are claiming that a Von Neumann machine can never produce a conscious mind.For example, David Gelentner in a recent article in the MIT Tech Review: Artificial Intelligence Is Lost in the Woods - A conscious mind will never be built out of software, argues a Yale University professor. It's a long article, but at one point he says:quote:David Gelernter said:The "Chinese Room" argument, proposed in 1980 by John Searle, a philosophy professor at the University of California, Berkeley, is intended to show that no computer running software could possibly manifest understanding or be conscious. His arguments don't make much sense to me. He seems to basically say that "A computer mechanistically executes instructions. It isn't aware of itself, and why would executing many, many instructions really fast make it suddenly exhibit consciousness?"Which in my view is missing the point. The computer is just a substrate to run a mind on, like the laws of physics are a substrate for our brains and minds to run on. On the topic of 'basic sentience': I just read the (bleak, but quite good SF) Blindsight which explores some of that in a hard-SF setting. When people who are blind due to brain injury can still catch balls thrown at them, it makes one wonder about exactly what goes on in the distributed processing of one's brain..
QtDevSvr
Ars Tribunus Angusticlavius
et Subscriptor
Tribus: California Floristic Province, Madrean Region, Holarctic Kingdom
Registered: Apr 28, 2001Posts: 8942
Posted: Sun Sep 23, 2007 3:54 pm
quote:Originally posted by QuantumET:I was under the impression that there were people who are claiming that a Von Neumann machine can never produce a conscious mind. Indeed there are, but in discussing such matters we need to be careful. For example, Gelentner wisely eschews such an absolute claim ("never"), and rather more carefully argues for something less absolute than your impossibility claim: quote: I believe it is hugely unlikely, though not impossible, that a conscious mind will ever be built out of software. His reasons boil down to a belief that those who claim otherwise have given no good reason in support of their views. They just have an intuition that minds are to brains as software is to hardware.quote:His arguments don't make much sense to me. He seems to basically say that "A computer mechanistically executes instructions. It isn't aware of itself, and why would executing many, many instructions really fast make it suddenly exhibit consciousness?"Which in my view is missing the point. The computer is just a substrate to run a mind on, like the laws of physics are a substrate for our brains and minds to run on. What, exactly, does the claim that the mind "runs" on the brain consist in? What evidence is there for the view? What is this "mind" thing that is so independent of the brain "substrate" as to be capable of merely "running" on that "substrate"?What you have to understand is that philosophers like Searle are exactly taking such claims seriously, and their arguments are exactly an effort to explain what they find false and misleading in such talk.For my own part, I'll just say for now that claims about "minds" "running on" brains strike me as steeped in metaphysical dualism, and thereby and to that degree flawed, fatally.The mind does not "run on" the living brain and it's associated sensory anatomy, it's caused by it. My brain isn't sitting around right now, running some experience-of-Qt-posting-on-Ars program, it's causing that experience, complete with the imaginary background music of Feith's "1,2,3,4", the feel of the chair I'm on, and all the rest. Moreover, the mind is a physical manifestation of a physical system. Sensations, perceptions, feelings, thoughts—all physical. We colloquially call such things "mental" and so far as no heavy metaphysical dualism is meant by such talk it's harmless—useful, even, so far as we intend to label a class of phenomena. But, metaphysically speaking, the mental is the physical.I won't say "never", "impossible", or the like, but when I hear talk of minds "running on" brains—and, soon, computers!—I wonder just what it is that people who talk about such things take minds to be.quote:On the topic of 'basic sentience': I just read the (bleak, but quite good SF) Blindsight which explores some of that in a hard-SF setting. When people who are blind due to brain injury can still catch balls thrown at them, it makes one wonder about exactly what goes on in the distributed processing of one's brain.. Thanks, I'll give that a read when I can.
Citrus538
Ars Legatus Legionis
Registered: Jul 21, 2000Posts: 12726
Posted: Sun Sep 23, 2007 4:05 pm
quote:Originally posted by QuantumET:On the topic of 'basic sentience': I just read the (bleak, but quite good SF) Blindsight which explores some of that in a hard-SF setting. When people who are blind due to brain injury can still catch balls thrown at them, it makes one wonder about exactly what goes on in the distributed processing of one's brain.. One of my favorite books about this topic is Phantoms in the Brain. It's really a great read, even if you know nothing about biology, psychology, or cognitive science. It even addresses a phenomenon like the one you described.
SO1OS
"Urban (under-)Achiever"
Ars Praefectus
Tribus: raleighboy
Registered: Aug 1, 2000Posts: 4553
Posted: Sun Sep 23, 2007 4:31 pm
quote:Originally posted by QtDevSvr:For my own part, I'll just say for now that claims about "minds" "running on" brains strike me as steeped in metaphysical dualism, and thereby and to that degree flawed, fatally. The "fatality" of dualism is oft overstated, especially in Berkeley. Many people believe that humans are distinguished with minds, but OTOH we're merely one species capable of mental activity. So, the point is already conceded that minds are separated from brains, it's just usually only thought of in a Cartesian frame.
ringobob
Ars Praefectus
Registered: Dec 8, 1999Posts: 3984
Posted: Sun Sep 23, 2007 6:32 pm
quote:Hmmm. I think you have some things in mind when you mention "context", but I might need you to un-pack them. I'll do my best. I have concrete ideas about how to accomplish what I'm talking about in an artificial system, but I haven't completely made it coherent yet in my mind, so it might be difficult to explain it. quote: Perception is certainly context-sensitive (both logically and physically), and yes, there's a causal chain which begins with a creature's environmental context and ends with the creature's perception of same. Are these the sorts of facts what you have in mind when you say that "perception arises from context"? That's a part of it. quote: What I seek a clearer understanding of is the conceptual differences between a creature that is merely photo-sensitive, a plant, say, and one which, however crudely, perceives light. The latter creature (forgive my zoology: there must be some examples, I just don't know any off-hand), but not the plant, has a subjective state. I want to know how to think about the logical differences between those two conditions because, from my perspective, those differences are the biggest puzzle in the philosophical topic of consciousness. Gotcha. quote: Imagine a creature that is crudely photo-sensitive, but only in a stimulus-response sort of way. Let's suppose it's photo-averse: when it's crude photo-sensitive cells detect light of sufficient strength, the creatures squirms away.Now imagine the same crude photo-sensitivity—the mere ability to detect light from dark—but this time in a creature which sees the difference.What does such seeing consist in? In my mind, it consists in the existence of a larger context. The creature isn't just responding to stimulus moment to moment, it has a larger context in which it recognizes a range of levels of light, and can place the current level in that range. It then reacts not strictly in a stimulus-response manner, but it sees light on one end and dark on another and chooses to move away from the light... and, given time and evolution, probably eventually chooses to move towards the dark.
QuantumET
Ars Scholae Palatinae
Registered: Jun 27, 2000Posts: 1184
Posted: Mon Sep 24, 2007 12:34 am
quote:Originally posted by QtDevSvr:I won't say "never", "impossible", or the like, but when I hear talk of minds "running on" brains—and, soon, computers!—I wonder just what it is that people who talk about such things take minds to be. I'll readily admit that I'm highly likely to be out of my philosophical depth, here, as clearly the argument is questioning connections that have to me seemed so self-evident as not to require any discussion.The verb 'caused by' here seems interesting to me. What is the fundamental difference between something 'running on' a substrate and being 'caused by' the substrate. (I realize substrate again implies a duality, but I haven't yet thought of a general word that works well). A poor EE's analogy follows:I can write a program that runs on a general purpose machine, or I can take the same program and convert it to load into an FPGA. The FPGA chip actually physically rewires itself to be the program - the colloquial phrase is still that it 'runs a program', but the logic gates and wires of the FPGA _are_ the program. Is there an essential difference, there? In one, a clear separation exists between the program and the CPU, in the other they are a single hardware unit. So if I took my massive software system that simulates a brain, and compiled onto an FPGA, where the software is no longer separated, what then? But getting back to the leap of logic, that somehow brains and minds are qualitatively different from computers and software, what is the evidence one needs? Clearly a brain is the physical structure that encompasses our mind. It's composed of a complex network of neurons, which appear to signal to each other using electrical potentials. Assuming there is no metaphysical soul, and no quantum-mechanical basis for consciousness, I don't understand what there is to prove. I just don't see what it is that is special about the few kilograms of gray stuff in our skulls, that it cannot even in principle be recreated in simulation on a simple abacus. That's reductionist in me, I suppose.Perhaps I should read a few dozen books and think about this for a while.
QuantumET
Ars Scholae Palatinae
Registered: Jun 27, 2000Posts: 1184
Posted: Mon Sep 24, 2007 12:40 am
quote:Originally posted by QtDevSvr:Now imagine the same crude photo-sensitivity—the mere ability to detect light from dark—but this time in a creature which sees the difference.What does such seeing consist in? It would seem to me that memory is one basic requirement for such a thing. To me, even primitive sentience would imply that the same action is not always taken in the presence of the same stimulus, and that requires memory.Of course, memory itself seems quite a messy concept, so perhaps this thought doesn't help much.
andyfatbastard
"lol donkaments"
Ars Legatus Legionis
et Subscriptor
Tribus: 19BFxo1jzGMrTsKFTezsjDiEksLNQoPVXg
Registered: Oct 17, 2000Posts: 19944
Posted: Mon Sep 24, 2007 1:58 am
I think people put too much importance on this idea of 'consciousness' that's been overdone in Star Trek during their formative years. Data is alive, dammit!
ringobob
Ars Praefectus
Registered: Dec 8, 1999Posts: 3984
Posted: Mon Sep 24, 2007 6:45 am
quote:It would seem to me that memory is one basic requirement for such a thing. To me, even primitive sentience would imply that the same action is not always taken in the presence of the same stimulus, and that requires memory. I think even "memory" is too high level a concept for this, since it implies higher level understanding about past/present time relationships.That's why I went for context, as it just means that the entity in question has some concept that the sensory input they are currently receiving is not all that there is.Going a bit further, I suppose it requires, given multiple sensory inputs, that the entity can distinguish between different kinds of stimulus and classify stimulus based on which sense it triggered, and can relate multiple sensory inputs that happen simultaneously (e.g. fire is hot [touch] and bright [sight] and loud [hearing] and has an odor [smell]). The fewer sensory inputs, the more limited sentience becomes, and can't probably move beyond a very primitive model with a single input.This gets to something else I've thought of when dealing with AI: if you're mapping human sensory input to a computer, it's probably useful to go with four senses instead of five, combining smell and taste into one as they are, for all intents and purposes, the same sense calibrated to different levels of sensitivity (smell can pull a few particles out of the air to "taste", taste needs much more material and much closer contact to do the same). This, of course, ignores that taste and smell aren't nearly as commonly simulated inputs as sight, hearing, and touch.
QtDevSvr
Ars Tribunus Angusticlavius
et Subscriptor
Tribus: California Floristic Province, Madrean Region, Holarctic Kingdom
Registered: Apr 28, 2001Posts: 8942
Posted: Mon Sep 24, 2007 9:54 am
quote:Originally posted by QuantumET:It would seem to me that memory is one basic requirement for such a thing. To me, even primitive sentience would imply that the same action is not always taken in the presence of the same stimulus, and that requires memory. I completely agree. Every time I've tried to analyze this in the past few years, I've come up with the presence of some sort of memory capacity as a logical entailment of any sort of sentience. If a creature has the capacity to represent parts of it's world, and not merely react to them, the question arises, Why? To what end? The most plausible reason I can think of (admittedly, not a strong premise) is that the creature has representational capacity in order that it might react conditionally, rather than by rote. Well, conditional action implies some sort of decision procedure (lest it be necessary to emphasize, none of this is necessarily conscious in any full-blooded human sense), and any sort of decision procedure implies memory.Moreover, SFAIK the evidence is accumulating that sentience itself is shot through with memory: our visual, auditory, and other sense domains are filled in by memory-based prediction of what "should be there". (This is why it is that we can be "wide-eyed" in novel circumstances. In normal, familiar circumstances, our senses are largely on auto-pilot.) When some out-of-pattern unpredicted stimulus occurs, our attention is drawn in and our senses are heightened to full gain. As well, we can deliberately choose to focus on some phenomenon and scrutinize it anew. But either case is an exception to the norm.quote:Originally posted by ringobob: I think even "memory" is too high level a concept for this, since it implies higher level understanding about past/present time relationships.That's why I went for context, as it just means that the entity in question has some concept that the sensory input they are currently receiving is not all that there is. I appreciate your caution, but don't entirely share your concern. (This differing attitude may be an artifact of our differing circumstances. I'm an arm-chair philosopher; you, if I'm reading between the lines correctly, are involved in some fashion or another in robotics. For any notion of "memory" to play a role in your work, it has to be sufficiently defined so as to admit of implementation. I have the luxury of ignoring such a demanding requirement.)I'd like to think we can all agree on a casual use of "memory" though, where, depending on context, it means as little as whatever is involved in artificial mnemonic circuits, or as much as is involved in full-blooded human memory. I think both I and QuantumET were thinking of fairly primitive forms of memory which are closer the basic circuit end of the scale. Does that get us past this conversational hurdle?My main objection to adverting from "memory" to "context" is simply that, once we've filled in what you mean by "context", I think we end up with the same concept as memory, just dressed in non-standard language. A sort of lexical charade. Don't get me wrong, I respect your intentions and your reasons. I just think we can satisfy your concerns by bracketing the more complex forms of memory and agreeing that they are not what is being implied in present context.
ringobob
Ars Praefectus
Registered: Dec 8, 1999Posts: 3984
Posted: Mon Sep 24, 2007 10:30 am
quote:I appreciate your caution, but don't entirely share your concern. (This differing attitude may be an artifact of our differing circumstances. I'm an arm-chair philosopher; you, if I'm reading between the lines correctly, are involved in some fashion or another in robotics. For any notion of "memory" to play a role in your work, it has to be sufficiently defined so as to admit of implementation. I have the luxury of ignoring such a demanding requirement.) I'm more of a hobbiest in AI and robotics, but you're correct that when I'm talking about it I'm thinking in terms of how it would be implemented. quote: I'd like to think we can all agree on a casual use of "memory" though, where, depending on context, it means as little as whatever is involved in artificial mnemonic circuits, or as much as is involved in full-blooded human memory. Fair enough, I'll do my best to not get too caught up in the language.So, given that, yes, I agree with pretty much all of what you said in response to QuantumET with regards to memory.
parallel
Ars Tribunus Angusticlavius
Registered: Nov 28, 2000Posts: 6461
Posted: Tue Sep 25, 2007 6:20 pm
I know I should reference bits from the previous posters to show where we differ, but it would take too long. Forgive me.I think consciousness is the key. Just projecting a picture onto some sensor doesn’t begin to produce this anymore than a photograph does. What is needed is for the computer to make a real time model of the local world around it in RAM and where it’s sensors are located in that world. Add other senses to your taste.I suppose many of things that have been developed like pattern & speech recognition could be added to speed the learning process, and make it more efficient, as an alternative to teaching it like a baby. But words would be meaningless until associated with something. Finding the best way to group things and look for matches is a difficult problem to solve in a universal way.I visualize that the basic thought process could be done in a parallel 3D model, which could work for concrete things, but abstract ideas are harder to visualize and I don’t have a good solution for this.Then of course you need the driving force of insatiable curiosity: something that could be directed by the builder. Sooner or later you might want to add Asimov’s three laws too.I would be happy to take it further should anyone be interested and feedback would be welcomed.
Citrus538
Ars Legatus Legionis
Registered: Jul 21, 2000Posts: 12726
Posted: Tue Sep 25, 2007 7:43 pm
quote:Originally posted by parallel:I know I should reference bits from the previous posters to show where we differ, but it would take too long. Forgive me.I think consciousness is the key. Just projecting a picture onto some sensor doesn’t begin to produce this anymore than a photograph does. What is needed is for the computer to make a real time model of the local world around it in RAM and where it’s sensors are located in that world. Add other senses to your taste. Well. . .sounds like you're talking about the hard problem of consciousness. It's still very much a philosophical area.
201 posts • 123 ... 6 Next
Ars Technica > Forums > Ars Lykaion > The Observatory
Jump to:
Select a forum
------------------
Hardware & Tweaking
Audio/Visual Club
Case and Cooling Fetish
CPU & Motherboard Technologia
Mobile Computing Outpost
Networking Matrix
Other Hardware
Agora Classifieds
Operating Systems & Software
Battlefront
Microsoft OS & Software Colloquium
Linux Kung Fu
Windows Technical Mojo
Distributed Computing Arcana
Macintoshian Achaia
Programmer's Symposium
The Server Room
Ars Lykaion
Gaming, Extra Strength Caplets
The Lounge
The Soap Box
The Boardroom
The Observatory
Ars Help & Feedback
Ars Subscription Member Areas
Image Galleries
Contact Us | Ars Technica
© Ars Technica 1998-2014
Powered by phpBB and...
© 2014 Condé Nast. All rights reserved
Use of this Site constitutes acceptance of our User Agreement (effective 3/21/12) and Privacy Policy (effective 3/21/12), and Ars Technica Addendum (effective 5/17/2012)
Your California Privacy Rights
The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast.
Ad Choices

